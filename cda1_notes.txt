This is John's personal developer notes for the CDA1 project.  Probably pretty boring reading
for anyone not intently helping out.

Big picture of this project's goals:
	1.1 - train a single agent to drive the new track to suitable destinations with several bots attempting to do the same thing.
	1.2 - expand the training to include several instances of the agent (replacing some bots) as pseudo-multi-agent training (iterative).
	1.3 - true multi-agent training, with several untrained agents (same policy) learning simultaneously using Ray multi-agent facilities.
	1.4 - multi-agent training for two policies in a single vehicle, one for planning and one for control.


8/21/23

* Project setup & Github repo creation.
* Completed first version of rqmts spec.

8/22/23

* Got legacy cda0 code running after breaking up all the classes in the main environment file
  so they are now in their own files.

8/23/23

* Rewrote the Roadway class to represent the new road geometry.

8/24/23

* Fixed some things in inference prgm and got it to display the new roadway.

8/27/23

* Began revamping the environment model to be more generalized with roadway definition and vehicle population.

8/28-30/23

* Continued building new environment model.

8/31/23

* Built env model to the point that it executes one iteration in inference (prior to checkpoint loading).

9/1/23

* Tried running inference with the final cda0 checkpoint file. It couldn't load because it was looking for
  the old class structure (simple_highway_ramp_wrapper), which is apparently baked into the checkpoint.

* Tried running the tune program - had to make several tweaks & fixes.
	* Got it running in the tuner.  Now to put in some real logic to get the NN to start learning.

9/2/23

* Updated inference prgm to work with the new vehicle structure and with or without a checkpoint (moving
  neighbor vehicles only).

9/3/23

* Fixed problems contributing to graphical display of vehicle locations.
* Ran inference on scenarios 90-95 with Bot1a controller to confirm geometry & speed limits all work.
* Developed & tested Bot1b controller to give offset to speed limit. 
* Ran a sample training job - everything looks nominal.  Ready to build the real learning controller.

9/4/23

* Wrote the Bridgit vehicle model with its huge observation vector encoding. Still need to test it.

9/5/23

* Wrote testing program for BrigitModel and ran them test suite. Found & fixed several defects.
* Tried running a training session, but got exception - too tired to investigate.

9/6/23

* Debugged problems with training. Found & fixed 2 defects.

9/7/23

* Started a training run to verify training will work.
	* Training hangs after 0-2 iterations. Log advances every 5 sec with no changes to the
	  info shown, and no print stmts from any of my code (ran with debug = 1). No data is
	  going to the tensorboard.
	* Started a thread on Ray Discuss at https://discuss.ray.io/t/tune-hangs-soon-after-starting/12081
	  to get help.

9/8/23

* Found defect in environment reset() method that induced an infinite loop occasionally, when trying to 
  place all of the neighbor vehicles within a close space. Fix allows training to move forward.
* Fixed defect in obs collection when neighbor was barely inside the front edge of the grid.

* Training run d226a using SAC on 2 trials. Reward function is scaled by 0.1 from what was used in
  cda0, so the reward range is now -1.5 for a crash, but otherwise within [-1, 1].
	* No good. rmax stayed < -1 for 3.5M steps (12k iterations).

9/16/23

* Run ? training with SAC
	* To accommodate changed reward scale, I added initial_alpha as a tuning HP.
	* Increased range of noise magnitude to [0.1, 0.4]
	* Added noise scale timesteps as a tunable HP to stretch it out more.
	* Extended trial duration from 12k iterations to 30k since it will probably take a lot longer for the agent
	  to experience all this track has to offer.
	* First 2 trials didn't do anything; rmax <= -1.
		* I discovered a defect in reading configs, such that it was not ignoring neighbor crashes, thus 
		  even if the agent was performing well, the episode may end with a poor reward.

* Run 5e343 training with SAC
	* Fixed config defects for setting boolean flags, correctly setting ignore_neighbor_crashes = True.
	* Changed max_iters back to 18k, since 30k took > 8 hr for two trials.
	* Adjusted reward penalty for speed deviation - multiplier from 0.03 to 0.015, since one inference example
	  (of prev run) showed that agent successfully stayed behind a bot at 14 m/s and got severely punished for
	  that, worse than if it had crashed.
	* Added a staging area from which to execute the training sessions, and a train.sh script to set it up and
	  initiate each run. This allows code editing & testing in the source area while running, without need to
	  make a second copy of the repo.
	* Inference on early trial (1) shows
		* Lots of crashes right after starting - it seems vehicles are packed too tightly together given the
		  wide range of start speeds.
	* Found a defect in VehicleModel, where it needs to have previous time step's obs passed in so that historical
	  reference info can be saved.

* Run a2caf training with SAC
	* Fixed defect that omitted proper updates of common obs vector elements.
	* Increased safe separation for initial vehicle placement from 4 to 5 car lengths.
	* Increased initial speed assignments from [0, MAX_SPEED] to [5, MAX_SPEED].
	* Added _decide_num_vehicles() to allow fewer vehicles to be present, especially during early episodes.
	* Inference on trial 0 near its peak (rmax ~-0.05): never changes lanes, LC cmds stay very close to 0; it
	  settles into speeds ~23.6 m/s in open field; it is common to see same-lane crashes between ego and a bot
	  either before or after it, due to close spacing and widely differing initial speeds.
	* Mean episode lenghts max out ~70 (target is 80).
	* It seems that training for only a fragment of the route gives no motivation for a lane change, esp when the
	  agent gets punished for doing so. Found that episode is not getting a reward for completing the allotted
	  steps, because it is not considered "done".

9/17/23

* Run 9010f training with SAC
	* Changed step() to set both the done and truncated flags when it hits the max allowed steps. This will allow
	  the completion reward to be counted for completing an episode, even if it doesn't hit the target.
	* Limited the speed of a neighbor vehicle placed close behind the ego vehicle so that it won't immediately
	  rear-end ego.
	* Expanded safe distance from 5 to 6 car lengths for placing vehicles in same lane near other vehicles.
	* Adjusted noise tuning params.
	* After 4 trials completed, the run died of a segfault after ~10k iterations in trials 4 & 5.
	* Before it died, trial 5 achieved rmax ~0.94 and rmean noisy in [0.4, 0.6] at 3M steps. Inference on it shows
	  it likes a steady speed of 21.91 m/s. This is similar to several other trials, settling at a lower-than
	  limit speed. I wonder if this is due to training in a crowd, where it learns that these are the safe speeds
	  to avoid a crash.

* Mods ready for next run:
	* Fixed small defect in TargetDestination that was incorrectly initializing the lane ID, so targets weren't
	  recognized. Also added indicator in reward function to give a bonus if target is reached.
	* Adjusted ACC control in BotType1Ctrl to start activating farther away (from 8 to 12 car lenghts) in hopes
	  of reducing its forward collisions.
	* Made some small improvements to inference usability.

9/18/23

* Began designing the capability to do simple route planning for controllers.

9/21/23

* Implemented the tree construction code for lane feeder connectivity.

9/22/23

* Designed & implemented lane change control logic for the BotType1bCtrl. Still has some problems.

9/23/23

* Tested Bottype1Ctrl algorithm. Made several changes to minimize crashes. A big factor is two bots that are in
  adjacent lanes but far apart longitudinally, with greatly differing speeds, then the rear one changes lanes and
  ACC cannot slow it down fast enough to avoid a rear-end. Another is two vehicles 2 lanes apart that both want to
  move into the lane between, and sensors have no way of knowing this will happen because intentions aren't
  broadcast.
	* LC decision is based on a one-time snapshot that only looks at 9 sensor zones in the adjacent lane. Once
	  the decision is made to start moving, a neighbor in that lane can quickly move into the zones, and the
	  host vehicle can't react (by design, there is no emergency abort maneuver).
	* I increased the maneuverability of these bots by making max accel = 4.0 m/s^2 and LC duration only 2.4 s.
	* Had to beef up the discrimination in the _check_for_collision() method to handle the possibility of two
	  vehicles separated by 2 lanes simultaneously trying to move into the common lane between.

9/24/23

* Began implementing the route planning for the Bridgit controller.

9/25/23

* Completed coding for the Bridgit controller. Dealing with test problems.

9/26/23

* Completed testing of the Bridget controller route planning method. It now adds desired probabilities of lane
  change to left, center or right on each plan execution, and these are put into the obs vector so that the
  controller NN will see them.
* Enhanced reset() provide scenarios 1 & 2 for all bots in ego's lane and for no bots starting in ego's lane,
  respectively. Also, if scenario 0 is specified in a training run, then it will sometimes sets it to 1 or 2 instead.

* Began training run 5d5d4 with SAC on this major new upgrade. This uses code in Git training branch, commit 8b23668.
	* All trials ended quickly with errors due to divide by zero in plan_route().

* Run ce52f training with SAC
	* Put in a quick band-aid and some diagnostics for the previous run's errors.
	* 7 of 8 trials errored out quickly. They were all SIGSEGVs.
	* Trial 1 seems to have run to completion, but only did ~1.3M steps (18k iterations).
	* The whole thing ran for 6.7 hr.

9/27/23

* Fixed defect where I had forgotten to retain the planning output values of lane change desirability in the obs
  vector, in BridgitModel.
* Cleaned up several minor TODOs.

* Run 84bc8 training with SAC for 40k iterations on 4 trials
	* Trial 1 errored out very early, due to failure to place a vehicle during reset.
	* Trials 0 & 2 ran for ~29k iterations (close to 9M steps) before I had to shut them down due to traveling.
	  Both of them showed rmax > 0.9 steadily after ~3.5M steps. Trial 0 rmean bounced a lot in [0, 0.4] and
	  trial 2 rmean was a little steadier in [0.4, 0.6] after 3M steps.
	* Distribution of episode_vehicles looks good across the progression of the trial.
	* Didn't see many logs of scenario adjustment - seems to be only for trial 1. I don't understand this.
	* Inference shows a fatal flaw: no lane changes. Even though it now has obs inputs to indicate the desired
	  lane change activity, there is no reward signal telling it whether the choice was good!

9/29/23

* Added reward bonus for following the planner's lane recommendation; removed the constant part of the penalty for
  lane changes (still gets hit for frequent maneuvers, but not just for doing the maneuver itself); reset the
  completion reward for tgt reached to 1.0 so that it is congruent with any episode completion and doesn't bias
  the learning toward the specific route geometry; added log details for crash situations.

* Run b557e training with SAC for 40k iterations on 6 trials
	* Computer froze up when trials 2 & 3 were ~32k iterations.
	* Trials 0 & 1 errored out early where reset() couldn't place a neighbor vehicle in lane 5.

* Run 83dda training with SAC for 40k iterations on 6 trials
	* Modified reset() so that if it can't find a safe location for a neighbor vehicle, it deactivates it.
	* Changed reward multiplier on LC behavior from 0.003 to 0.004.
	* Fixed a defect with scenario resetting that forced all episodes to run scenario 2 after a short time.
	* Killed after ~32k iterations on trials 0 & 1 due to travel needs.
	* Inference on a recent checkpoint shows that the agent like to drive ~32 m/s without slowing to avoid a
	  rear-end crash. It also did not demonstrate a lane change in a couple quick episodes where it should
	  have. Hoping it just needs more experience.

9/30/23

* Added capability for the tune program to restart an in-progress job from the checkpoint dir. It doesn't work,
  with Ray throwing an assertion error. Stored the checkpoint in ray_results/cda1-no-restore for future
  investigation.

* Run a120d training with SAC for 40k iterations
	* Tweaked distro of scenarios 0, 1 & 2.
	* System locked up after running for 9:28, the first two trials achieving ~26k iterations (~8M steps),
	  providing productivity of 5500 iters/hr. This is with obs verification off.
	* Trial 0's reward plot was somewhat jagged, but by 8M steps, rmax ~1.2 and rmean in [0.7, 0.8]. It
	  had noise magnitude of 0.12 decaying over 8M steps. Trial 1's reward was much smoother but less
	  effective. Its rmax climbed quickly to 1.2 by 3M steps, then stayed there, while rmean hit 0.6 then
	  noisily drifted in [0.1, 0.7]. Both had rmin < -1.0 the entire time.
	* Inference on trial 0 shows agent quickly moves to a speed of 17.6 m/s and does not change lanes.
	* Inference on trial 1 shows agent quickly moves to 31.9 m/s and does not change lanes. Also, it
	  rear-ended a slightly slower bot without attempting to slow.
	* Found a defect in the reward bonus for LC planning.

10/3/23

* Spent a good deal of time debugging WARNINGs received occasionally from get_reward(), where it detects no
  desirable lanes specified by the Bridgit planner. I cannot determine what is causing this, as it only appears
  during training, not in inference (so far), and training with parallel workers produces non-sequential log
  content.

* Run 9ef56 training with SAC for 40k iterations
	* Changed noise scale_timesteps to choose either 8M or 12M steps (was 4M or 8M).
	* Narrowed the tuning range of noise stddev to [0.1, 0.4].
	* Fixed the LC plan-following reward bonus.
	* Increased the multiplier on the plan-following reward bonus (to 0.006), since inference showed that
	  it was overshadowed by the speed penalty.
	* Improved reward code to not penalize for speed deviation if ego is going slower than speed limit because
	  it is following a slow forward vehicle.
	* Trial 1 seems to have started well, reaching rmax ~1.3 by 4M steps and rmean reaching 0.8 in a chaotic
	  way by 6M steps.
	* Trial 0 errored after 3.5M steps, with an assertion error in the ACC logic.
	* The whole job stopped way early due to a segfault.
	* Reviewed the warnings from get_reward() about not having a desirable lane. I only saw occurrences where
	  it was a lost cause because there is no physical way to reach a target from the current location, so
	  this is okay. This was only in inference runs, however.
	* It is still pretty easy for the LC desired bonus to be swamped by a speed penalty, for even moderately
	  poor speeds.

10/4/23

* Run 7476e training with SAC for 40k iterations
	* Enhanced _verify_safe_location() to use a carefully calculated amount of space in the same lane, rather
	  than the previously used wild guess. The change is a lot more conservative.
	* Modified rewards so that: LC desired bonus mult increased from 0.006 to 0.008; lane change penalty mult
	  was decreased from 0.0005 to 0.0001; LC command penaly was eliminated (allowing frequent LCs); increased
	  speed penalty mult from 0.015 to 0.018.
	* All trials died before 600k steps, all with an unknown system error.
	* Still seeing plenty of warnings about all desirability lanes with 0 probability.

* Ran a tune run with a single worker thread in order to debug the training situation with desirability lanes.
	* Added debugging statements.
	* Changed Ray resources to no GPU and only 1 cpu to help find problems.
	* Found several cases where an illegal lane change zeros out the obs vector of ego, since the dynamics
	  method marks it as inactive. Added a guard for this in step().
	* It seems this fixes the problems. There are times when the desirability vector will be all zeros,
	  which is right after a reset, or if the vehicle is on an exit ramp with no hope of reaching one of
	  its targets. All other conditions now produce reasonable desirabilities.

* Run b96d8 training with SAC for 40k iterations. Corresponds to git commit 0bbd952 on the train branch.
	* Includes a few fixes for handling of lane desirabilities.
	* Program halted due to segfault when first two trials were ~32k iterations in (9M steps).
	* Trial 1 reached rmax ~1.4 steadily since 5M steps, and rmean peaked at 0.4 briefly before the end.
	  Inference on it shows that it loves a steady speed of 32.1 m/s, even though it resulted in rear-
	  ending another vehicle. It never demonstrated a lane change.
		* Its initial alpha = 0.2, noise stddev = 0.47 decaying over 12M steps.
	* Trial 0 rmax reached 1.4 a couple times, earlier than trial 1, but was very jagged. Its rmean was
	  equally chaotic, reaching 0.4 after 5.5M steps, but then fluctuating in [-1, 0.2] afterwards. I
	  ran inference on a checkpoint from the peak rmean, which performed about like trial 1, but
	  preferring a speed of 27.7 m/s. It ignored speed limits, and also refused to change lanes.
		* Its initial alpha = 0.002, noise stddev = 0.34 decaying over 12M steps.
	* I suspect that this consistent lack of ability to adapt to situations implies that the NN is too
	  simple.
	* I saved the checkpoints in the project's training dir for future reference, named 256-256.
	* Training performance on this run was 3550 iters/hr, with verify_obs turned on and 0 cpus_per_worker.

10/5/23

* Run 33541 training with SAC for 40k iterations - with larger NN. Corresponds to git commit a0eb2cb on training
  branch.
	* Increased NN structure to 3 layers, consisting of 600, 256, 128 neurons each (since there are
	  559 inputs) for both the policy and Q networks.
	* Removed tuning from the initial alpha, fixing it at 0.2.
	* Changed num_cpus_per_worker from 0 to 4 (I had used 2 for a long time) and num_cpus_for_local_worker
	  from 2 to 4.
	* Turned off verify_obs.
	* Set rollout_fragment_length = 80, since this is the episode length; set train_batch_size = 1040
	  to be a multiple of this.
	* All 6 trials died very quickly due to system fault. I suspect it was an OOM failure.

* Run a52ad training with SAC for 40k iterations, with larger NN. This is a re-do of previous run 33541, but...
	* Reduced num_cpus_per_worker to 2 and num_cpus_for_local_worker to 2.
	* All trials errored before 700k steps. All were unknown system faults (OOM?). I verified that the
	  resource configs were the same as on 10/3, before I started playing with them.

10/6/23

* Performed system update, per system notes, due to inaccessible GPU.

* Run ec5a7 training with SAC for 40k iterations, with larger NN...re-re-do.
	* Set num_rollout_workers = 1 (was previously unspecified), to help limit memory usage.
	* Trials 3, 4 & 5 all died due to system fault (possible OOM). 3 made it to 1M steps, but 4 & 5
	  stopped before 400k steps.
	* Trials 0 & 1 ran to completion, ~8M steps, which is the first time I've seen any trial do that.
	* Trial 0 (noise stddev 0.23 over 12M steps) reached rmax > 1 quickly, but occasionally dropped well
	  below that a few times. After 5M steps, rmean fluctuated noisily ~0.8 and stayed there. rmin never
	  consistently exceeded -1.3. Mean episode length was ~78 after 5M steps.
		* Inference shows agent prefers speed 18.2 m/s, although it stayed close to the ramp 0 speed
		  limit initially. It never changed lanes.
	* Trial 1 (noise stddev 0.45 over 12M steps) showed a steady climb in rmax from 0.5 to 1.4 over the
	  8M steps. rmean stayed low until 4M steps, then gradually curved upward, crossing 0 at 7.5M steps
	  and ending > 0.4 at 8.1M steps, with every indication that it would have continued to climb. rmin
	  stayed ~-1.4. Mean episode length gradually curved upward too, ending at 70.
		* Inference shows a preference for 22.9 m/s, but otherwise behaves same as trail 0. When it
		  runs on a target lane, it always succeeds, and gets a fat reward.
	* Trial 2 proceeded will to about 3.5M steps, reaching rmean = 0.8 by then, but the job died without
	  any error message at that time. It had noise stddev = 0.24 over 8M steps.
	* This run performed at approx 5400 iters/hr, running two trials simultaneously, but seems to have
	  had some memory problems.

10/7/23

* Run 17e90 training with SAC over 40k iterations on 1 trial
	* Set episode completion reward to 0 (was 1) to force agent to pay more attention to the lane change
	  opportunities and speed control opportunities.
	* Fixed the noise magnitude at 0.25 over 12M steps (not tunable).
	* Trial completed. rmax was very noisy, bouncing rapidly in [0.1, 1.2], while rmean slowly fluctuated
	  in [-0.3, -0.1] after 4M steps, but mean episode duration was ~77 after 5M steps.
		* Inference shows no consideration of lane change, and a desire for speed of 14.8 m/s.
	* Half-way through the run, performance was showing 2400 iters/hr.

10/8/23

* Run a5c6d training with SAC over 40k iterations on 1 trial
	* Changed reset() to spread out initial P location of ego vehicle closer to end of its lane (up to
	  150 m from the end in all case. For early episodes (< 10k) it also skews the choice of location toward
	  the lanes without targets, and to P coords near the end of whatever lane is chosen. Al this will help
	  expose it to failure scenarios.
	* Changed BridigitCtrl.plan_route() to scale outputs by max_prob instead of by sum_prob, thus magnifying
	  the difference between larger and smaller ones, because the largest value is always 1. Also changed
	  its calculation of desirable probability to be dependent only on the remaining distance in the ending
	  lane, not on its length relative to the target lane.
	* Changed num_gpus from 0.5 to 1 to give more compute to the single trial worker.
	* Training performance was 2710 iters/hr.
	* Judging by shape of reward curves, the first 10k episodes, which dictate more difficult initial conditions,
	  were completed after only ~300k steps (avg 30 steps/iter).
	* Inference was no good. It sought a constant speed, well below nominal speed limit (values 17-19 m/s,
	  depending on which checkpoint was used), and it never attempted to change lanes.

10/9/23

* Run 424f1 training with SAC on 40k iterations on 1 trial. Corresponds to train branch commit 80a32e7.
	* Changed the early episode threshold in reset() from 10k to 100k episodes, which should correspond to
	  closer to 4M steps.
	* Improved BridgitCtrl.plan_route() so that it won't recommend a lane change unless it is legal for all of
	  the following planning cycle (approx; it looks ahead 6 sensor zones).
	* Removed penalty for widely varying speed commands (just to keep it out of the way for now).
	* Increased reward speed_mult from 0.018 to 0.030, since this penalty was getting washed out by LC bonus.
	* Changed noise magnitude from 0.25 to 0.35.
	* Reward curves don't look as good as prev run. The "early episodes" lasted ~2.2M steps, after which rmax
	  dropped a lot. It then peaked at 0.5 ~6M steps, then dropped to 0 and only recoverd to 0.1 afterward.
	  rmean peaked at -0.2 @ 7M steps, then dropped.
	* Performance was 2790 iters/hr.
	* Inference at ~22k and 37k iterations shows it seeks steady speed of 16.4 m/s and does not change lanes.
		* I notice that LC desired bonus is being collected generously (full value) when it cannot change
		  lanes (e.g. early part of lane 0), so it is getting rewarded for non-action. Also, it is collecting
		  this full reward on the exit ramps in the suicide phase, which reinforces simply staying in the
		  same lane always.
		* I'm concerned that the number of obs inputs from the sensors is overwhelming the handful of inputs
		  that are most important (speed diff, lane change commands).

* Run fe196 training with SAC over 40k iterations on 1 trial
	* Change reward bonus for LC desirability to be awarded only if there is a choice other than current lane to
	  be made (possibly current lane is one of the choices).
	* Increased reward penalty speed_mult from 0.3 to 0.5 to further emphasize speed discrepancies.
	* Increased episode length from 80 to 160 steps to make it more difficult to complete without a lane change,
	  and slightly more chance of including a target destination.
	* Increased noise magnitude from 0.35 to 0.4.
	* Changed num_cpus_per_worker and num_cpus_for_local_worker from 2 to 4 to improve performance.
	* No good. After the first 2.2M steps (early episode treatment), rmax dropped to -1, then after 7M steps it
	  slowly fluctuated in [-1.2, -0.2] for the remainder of the 14M steps. Trial was stopped early (at iter
	  25863) due to poor performance. However, mean episode length did reach ~150 after 6M steps.
	* Performance was 2600 iters/hr.
	* Inference confirms the poor actions I've seen in previous runs.

10/10/23

* Several quick experiment runs with differing noise levels, looking at very early checkpoints. I added logging to show
  the input commands to every step() call. They seem to be pretty well distributed in [-1, 1], although somewhat skewed
  to negative values. It appears, however that the noise makes up virtually all of the magnitude of these commands.
  When running an early checkpoint (from iters 10 to 200) in inference mode, the NN output from every step is within a
  hair of 0, regardless of the situation. With almost no significant learning accomplished by this point, I would exped
  the outputs to look a lot more random.

* Run c4dd8 training with SAC over 40k iterasions - back to multiple trials
	* Switched to 4 trials, since I can train 2 in the same time I can train 1 by allocating resources differently.
	  Back to num_gpus = 0.5, num_cpus_per_worker = 2, num_cpus_for_local_worker = 2, and num_rollout_workers
	  undefined.
	* Increased episode length to 500 steps, which should allow most episodes to run the full track. Changed 
	  rollout_fragement_length to 500 also, to align with a full episode in each fragment.
	* Changed noise magnitude to 0.5, based on experiments earlier today, to help expose it to LC commands with
	  magnitude > 0.5 (a lane change).
	* No good. After the initial episode treatment (2M steps), rmax dropped to -1 and stayed close to that until
	  5M steps, where I killed the job.

* Run e8679 training with SAC over 40k iterations on multiple trials.
	* Turned off Gaussian noise, at Kevin's suggestion.
	* Added exception handling to step() and to BridgitModel.get_obs_vector() to better understand a strange error
	  tripped in the Roadway model.
	* Trial 0 rmean gradually dropped to -11, recovered a bit, then dropped to -12.
	* Trial 1 was stopped early because rmax fell dramatically after 4M steps.
	* Training performance 3200 iters/hr.

* Began studying RLlib code to figure out how to pre-populate a replay buffer (web site doesn't seem to have any info
  on how to do it).

10/11/23

* Run a58db training with SAC over 40k iterations on multiple trials
	* Turned on Gaussian noise at stddev = 0.25.
	* Added explicit replay buffer config to the tuner program. Specified MultiAgentPrioritizedReplayBuffer (which
	  seems to be the default, capacity of 1M (which was the default), and turned on prioritized_replay (which had
	  been off).



TODO:
- consider larger LRs?
- Scan log for "reset: bots that" couldn't be placed. May need to rethink this algo if too many are being left out.
- Consider training full route episodes to ensure it understand mission completion.
- Kevin suggestion: initialize NN weights with pytorch Xavier method.
- Kevin suggestion #1: pre-populate experience buffer w/steps that have random actions.
- Kevin suggestion #2: pre-train an encoder to abstract the sensor data lieu of a CNN, to reduce its size to the obs.
- Build BridgitCtrl to read a checkpoint & execute it (preferably without Ray)
- find a way to load just the NN weights in inference (or in a vehicle controller) so that it can be run
  only with torch, no ray involved.
- Try running inference with real checkpoint - modify the action loop to use BridgitCtrl instead of loading directly?

- Fix requirements.txt to either be pip compatible (to use pip install -r requirements.txt) or to be a conda import compatible.
- Use argparse to handle args in inference
- Investigate speed overshoot when speed limit changes (with bot1a controller)
- Add lane IDs to graphics
- Add off-road icon
- Add vehicle icons to graphics
- Review all TODOs
- Add ego sensor display to graphics showing pvmt type/speed colors, vehicle speeds
