This is John's personal developer notes for the CDA1 project.  Probably pretty boring reading
for anyone not intently helping out.

Big picture of this project's goals:
	1.1 - train a single agent to drive the new track to suitable destinations with several bots attempting to do the same thing.
	1.2 - expand the training to include several instances of the agent (replacing some bots) as pseudo-multi-agent training (iterative).
	1.3 - true multi-agent training, with several untrained agents (same policy) learning simultaneously using Ray multi-agent facilities.
	1.4 - multi-agent training for two policies in a single vehicle, one for planning and one for control.


8/21/23

* Project setup & Github repo creation.
* Completed first version of rqmts spec.

8/22/23

* Got legacy cda0 code running after breaking up all the classes in the main environment file
  so they are now in their own files.

8/23/23

* Rewrote the Roadway class to represent the new road geometry.

8/24/23

* Fixed some things in inference prgm and got it to display the new roadway.

8/27/23

* Began revamping the environment model to be more generalized with roadway definition and vehicle population.

8/28-30/23

* Continued building new environment model.

8/31/23

* Built env model to the point that it executes one iteration in inference (prior to checkpoint loading).

9/1/23

* Tried running inference with the final cda0 checkpoint file. It couldn't load because it was looking for
  the old class structure (simple_highway_ramp_wrapper), which is apparently baked into the checkpoint.

* Tried running the tune program - had to make several tweaks & fixes.
	* Got it running in the tuner.  Now to put in some real logic to get the NN to start learning.

9/2/23

* Updated inference prgm to work with the new vehicle structure and with or without a checkpoint (moving
  neighbor vehicles only).

9/3/23

* Fixed problems contributing to graphical display of vehicle locations.
* Ran inference on scenarios 90-95 with Bot1a controller to confirm geometry & speed limits all work.
* Developed & tested Bot1b controller to give offset to speed limit. 
* Ran a sample training job - everything looks nominal.  Ready to build the real learning controller.

9/4/23

* Wrote the Bridgit vehicle model with its huge observation vector encoding. Still need to test it.

9/5/23

* Wrote testing program for BrigitModel and ran them test suite. Found & fixed several defects.
* Tried running a training session, but got exception - too tired to investigate.

9/6/23

* Debugged problems with training. Found & fixed 2 defects.

9/7/23

* Started a training run to verify training will work.
	* Training hangs after 0-2 iterations. Log advances every 5 sec with no changes to the
	  info shown, and no print stmts from any of my code (ran with debug = 1). No data is
	  going to the tensorboard.
	* Started a thread on Ray Discuss at https://discuss.ray.io/t/tune-hangs-soon-after-starting/12081
	  to get help.

9/8/23

* Found defect in environment reset() method that induced an infinite loop occasionally, when trying to 
  place all of the neighbor vehicles within a close space. Fix allows training to move forward.
* Fixed defect in obs collection when neighbor was barely inside the front edge of the grid.

* Training run d226a using SAC on 2 trials. Reward function is scaled by 0.1 from what was used in
  cda0, so the reward range is now -1.5 for a crash, but otherwise within [-1, 1].
	* No good. rmax stayed < -1 for 3.5M steps (12k iterations).

9/16/23

* Run ? training with SAC
	* To accommodate changed reward scale, I added initial_alpha as a tuning HP.
	* Increased range of noise magnitude to [0.1, 0.4]
	* Added noise scale timesteps as a tunable HP to stretch it out more.
	* Extended trial duration from 12k iterations to 30k since it will probably take a lot longer for the agent
	  to experience all this track has to offer.
	* First 2 trials didn't do anything; rmax <= -1.
		* I discovered a defect in reading configs, such that it was not ignoring neighbor crashes, thus 
		  even if the agent was performing well, the episode may end with a poor reward.

* Run 5e343 training with SAC
	* Fixed config defects for setting boolean flags, correctly setting ignore_neighbor_crashes = True.
	* Changed max_iters back to 18k, since 30k took > 8 hr for two trials.
	* Adjusted reward penalty for speed deviation - multiplier from 0.03 to 0.015, since one inference example
	  (of prev run) showed that agent successfully stayed behind a bot at 14 m/s and got severely punished for
	  that, worse than if it had crashed.
	* Added a staging area from which to execute the training sessions, and a train.sh script to set it up and
	  initiate each run. This allows code editing & testing in the source area while running, without need to
	  make a second copy of the repo.
	* Inference on early trial (1) shows
		* Lots of crashes right after starting - it seems vehicles are packed too tightly together given the
		  wide range of start speeds.
	* Found a defect in VehicleModel, where it needs to have previous time step's obs passed in so that historical
	  reference info can be saved.

* Run a2caf training with SAC
	* Fixed defect that omitted proper updates of common obs vector elements.
	* Increased safe separation for initial vehicle placement from 4 to 5 car lengths.
	* Increased initial speed assignments from [0, MAX_SPEED] to [5, MAX_SPEED].
	* Added _decide_num_vehicles() to allow fewer vehicles to be present, especially during early episodes.
	* Inference on trial 0 near its peak (rmax ~-0.05): never changes lanes, LC cmds stay very close to 0; it
	  settles into speeds ~23.6 m/s in open field; it is common to see same-lane crashes between ego and a bot
	  either before or after it, due to close spacing and widely differing initial speeds.
	* Mean episode lenghts max out ~70 (target is 80).
	* It seems that training for only a fragment of the route gives no motivation for a lane change, esp when the
	  agent gets punished for doing so. Found that episode is not getting a reward for completing the allotted
	  steps, because it is not considered "done".

9/17/23

* Run 9010f training with SAC
	* Changed step() to set both the done and truncated flags when it hits the max allowed steps. This will allow
	  the completion reward to be counted for completing an episode, even if it doesn't hit the target.
	* Limited the speed of a neighbor vehicle placed close behind the ego vehicle so that it won't immediately
	  rear-end ego.
	* Expanded safe distance from 5 to 6 car lengths for placing vehicles in same lane near other vehicles.
	* Adjusted noise tuning params.
	* After 4 trials completed, the run died of a segfault after ~10k iterations in trials 4 & 5.
	* Before it died, trial 5 achieved rmax ~0.94 and rmean noisy in [0.4, 0.6] at 3M steps. Inference on it shows
	  it likes a steady speed of 21.91 m/s. This is similar to several other trials, settling at a lower-than
	  limit speed. I wonder if this is due to training in a crowd, where it learns that these are the safe speeds
	  to avoid a crash.

* Mods ready for next run:
	* Fixed small defect in TargetDestination that was incorrectly initializing the lane ID, so targets weren't
	  recognized. Also added indicator in reward function to give a bonus if target is reached.
	* Adjusted ACC control in BotType1Ctrl to start activating farther away (from 8 to 12 car lenghts) in hopes
	  of reducing its forward collisions.
	* Made some small improvements to inference usability.

9/18/23

* Began designing the capability to do simple route planning for controllers.

9/21/23

* Implemented the tree construction code for lane feeder connectivity.

9/22/23

* Designed & implemented lane change control logic for the BotType1bCtrl. Still has some problems.

9/23/23

* Tested Bottype1Ctrl algorithm. Made several changes to minimize crashes. A big factor is two bots that are in
  adjacent lanes but far apart longitudinally, with greatly differing speeds, then the rear one changes lanes and
  ACC cannot slow it down fast enough to avoid a rear-end. Another is two vehicles 2 lanes apart that both want to
  move into the lane between, and sensors have no way of knowing this will happen because intentions aren't
  broadcast.
	* LC decision is based on a one-time snapshot that only looks at 9 sensor zones in the adjacent lane. Once
	  the decision is made to start moving, a neighbor in that lane can quickly move into the zones, and the
	  host vehicle can't react (by design, there is no emergency abort maneuver).
	* I increased the maneuverability of these bots by making max accel = 4.0 m/s^2 and LC duration only 2.4 s.
	* Had to beef up the discrimination in the _check_for_collision() method to handle the possibility of two
	  vehicles separated by 2 lanes simultaneously trying to move into the common lane between.

9/24/23

* Began implementing the route planning for the Bridgit controller.

9/25/23

* Completed coding for the Bridgit controller. Dealing with test problems.

9/26/23

* Completed testing of the Bridget controller route planning method. It now adds desired probabilities of lane
  change to left, center or right on each plan execution, and these are put into the obs vector so that the
  controller NN will see them.
* Enhanced reset() provide scenarios 1 & 2 for all bots in ego's lane and for no bots starting in ego's lane,
  respectively. Also, if scenario 0 is specified in a training run, then it will sometimes sets it to 1 or 2 instead.

* Began training run 5d5d4 with SAC on this major new upgrade. This uses code in Git training branch, commit 8b23668.
	* All trials ended quickly with errors due to divide by zero in plan_route().

* Run ce52f training with SAC
	* Put in a quick band-aid and some diagnostics for the previous run's errors.
	* 7 of 8 trials errored out quickly. They were all SIGSEGVs.
	* Trial 1 seems to have run to completion, but only did ~1.3M steps (18k iterations).
	* The whole thing ran for 6.7 hr.

9/27/23

* Fixed defect where I had forgotten to retain the planning output values of lane change desirability in the obs
  vector, in BridgitModel.
* Cleaned up several minor TODOs.

* Run 84bc8 training with SAC for 40k iterations on 4 trials
	* Trial 1 errored out very early, due to failure to place a vehicle during reset.
	* Trials 0 & 2 ran for ~29k iterations (close to 9M steps) before I had to shut them down due to traveling.
	  Both of them showed rmax > 0.9 steadily after ~3.5M steps. Trial 0 rmean bounced a lot in [0, 0.4] and
	  trial 2 rmean was a little steadier in [0.4, 0.6] after 3M steps.
	* Distribution of episode_vehicles looks good across the progression of the trial.
	* Didn't see many logs of scenario adjustment - seems to be only for trial 1. I don't understand this.
	* Inference shows a fatal flaw: no lane changes. Even though it now has obs inputs to indicate the desired
	  lane change activity, there is no reward signal telling it whether the choice was good!

9/29/23

* Added reward bonus for following the planner's lane recommendation; removed the constant part of the penalty for
  lane changes (still gets hit for frequent maneuvers, but not just for doing the maneuver itself); reset the
  completion reward for tgt reached to 1.0 so that it is congruent with any episode completion and doesn't bias
  the learning toward the specific route geometry; added log details for crash situations.

* Run b557e training with SAC for 40k iterations on 6 trials
	* Computer froze up when trials 2 & 3 were ~32k iterations.
	* Trials 0 & 1 errored out early where reset() couldn't place a neighbor vehicle in lane 5.

* Run 83dda training with SAC for 40k iterations on 6 trials
	* Modified reset() so that if it can't find a safe location for a neighbor vehicle, it deactivates it.
	* Changed reward multiplier on LC behavior from 0.003 to 0.004.
	* Fixed a defect with scenario resetting that forced all episodes to run scenario 2 after a short time.
	* Killed after ~32k iterations on trials 0 & 1 due to travel needs.
	* Inference on a recent checkpoint shows that the agent like to drive ~32 m/s without slowing to avoid a
	  rear-end crash. It also did not demonstrate a lane change in a couple quick episodes where it should
	  have. Hoping it just needs more experience.

9/30/23

* Added capability for the tune program to restart an in-progress job from the checkpoint dir. It doesn't work,
  with Ray throwing an assertion error. Stored the checkpoint in ray_results/cda1-no-restore for future
  investigation.

* Run a120d training with SAC for 40k iterations
	* Tweaked distro of scenarios 0, 1 & 2.
	* System locked up after running for 9:28, the first two trials achieving ~26k iterations (~8M steps),
	  providing productivity of 5500 iters/hr. This is with obs verification off.
	* Trial 0's reward plot was somewhat jagged, but by 8M steps, rmax ~1.2 and rmean in [0.7, 0.8]. It
	  had noise magnitude of 0.12 decaying over 8M steps. Trial 1's reward was much smoother but less
	  effective. Its rmax climbed quickly to 1.2 by 3M steps, then stayed there, while rmean hit 0.6 then
	  noisily drifted in [0.1, 0.7]. Both had rmin < -1.0 the entire time.
	* Inference on trial 0 shows agent quickly moves to a speed of 17.6 m/s and does not change lanes.
	* Inference on trial 1 shows agent quickly moves to 31.9 m/s and does not change lanes. Also, it
	  rear-ended a slightly slower bot without attempting to slow.
	* Found a defect in the reward bonus for LC planning.

10/3/23

* Spent a good deal of time debugging WARNINGs received occasionally from get_reward(), where it detects no
  desirable lanes specified by the Bridgit planner. I cannot determine what is causing this, as it only appears
  during training, not in inference (so far), and training with parallel workers produces non-sequential log
  content.

* Run 9ef56 training with SAC for 40k iterations
	* Changed noise scale_timesteps to choose either 8M or 12M steps (was 4M or 8M).
	* Narrowed the tuning range of noise stddev to [0.1, 0.4].
	* Fixed the LC plan-following reward bonus.
	* Increased the multiplier on the plan-following reward bonus (to 0.006), since inference showed that
	  it was overshadowed by the speed penalty.
	* Improved reward code to not penalize for speed deviation if ego is going slower than speed limit because
	  it is following a slow forward vehicle.
	* Trial 1 seems to have started well, reaching rmax ~1.3 by 4M steps and rmean reaching 0.8 in a chaotic
	  way by 6M steps.
	* Trial 0 errored after 3.5M steps, with an assertion error in the ACC logic.
	* The whole job stopped way early due to a segfault.
	* Reviewed the warnings from get_reward() about not having a desirable lane. I only saw occurrences where
	  it was a lost cause because there is no physical way to reach a target from the current location, so
	  this is okay. This was only in inference runs, however.
	* It is still pretty easy for the LC desired bonus to be swamped by a speed penalty, for even moderately
	  poor speeds.

10/4/23

* Run 7476e training with SAC for 40k iterations
	* Enhanced _verify_safe_location() to use a carefully calculated amount of space in the same lane, rather
	  than the previously used wild guess. The change is a lot more conservative.
	* Modified rewards so that: LC desired bonus mult increased from 0.006 to 0.008; lane change penalty mult
	  was decreased from 0.0005 to 0.0001; LC command penaly was eliminated (allowing frequent LCs); increased
	  speed penalty mult from 0.015 to 0.018.
	* All trials died before 600k steps, all with an unknown system error.
	* Still seeing plenty of warnings about all desirability lanes with 0 probability.

* Ran a tune run with a single worker thread in order to debug the training situation with desirability lanes.
	* Added debugging statements.
	* Changed Ray resources to no GPU and only 1 cpu to help find problems.
	* Found several cases where an illegal lane change zeros out the obs vector of ego, since the dynamics
	  method marks it as inactive. Added a guard for this in step().
	* It seems this fixes the problems. There are times when the desirability vector will be all zeros,
	  which is right after a reset, or if the vehicle is on an exit ramp with no hope of reaching one of
	  its targets. All other conditions now produce reasonable desirabilities.

* Run b96d8 training with SAC for 40k iterations. Corresponds to git commit 0bbd952 on the train branch.
	* Includes a few fixes for handling of lane desirabilities.
	* Program halted due to segfault when first two trials were ~32k iterations in (9M steps).
	* Trial 1 reached rmax ~1.4 steadily since 5M steps, and rmean peaked at 0.4 briefly before the end.
	  Inference on it shows that it loves a steady speed of 32.1 m/s, even though it resulted in rear-
	  ending another vehicle. It never demonstrated a lane change.
		* Its initial alpha = 0.2, noise stddev = 0.47 decaying over 12M steps.
	* Trial 0 rmax reached 1.4 a couple times, earlier than trial 1, but was very jagged. Its rmean was
	  equally chaotic, reaching 0.4 after 5.5M steps, but then fluctuating in [-1, 0.2] afterwards. I
	  ran inference on a checkpoint from the peak rmean, which performed about like trial 1, but
	  preferring a speed of 27.7 m/s. It ignored speed limits, and also refused to change lanes.
		* Its initial alpha = 0.002, noise stddev = 0.34 decaying over 12M steps.
	* I suspect that this consistent lack of ability to adapt to situations implies that the NN is too
	  simple.
	* I saved the checkpoints in the project's training dir for future reference, named 256-256.
	* Training performance on this run was 3550 iters/hr, with verify_obs turned on and 0 cpus_per_worker.

10/5/23

* Run 33541 training with SAC for 40k iterations - with larger NN. Corresponds to git commit a0eb2cb on training
  branch.
	* Increased NN structure to 3 layers, consisting of 600, 256, 128 neurons each (since there are
	  559 inputs) for both the policy and Q networks.
	* Removed tuning from the initial alpha, fixing it at 0.2.
	* Changed num_cpus_per_worker from 0 to 4 (I had used 2 for a long time) and num_cpus_for_local_worker
	  from 2 to 4.
	* Turned off verify_obs.
	* Set rollout_fragment_length = 80, since this is the episode length; set train_batch_size = 1040
	  to be a multiple of this.
	* All 6 trials died very quickly due to system fault. I suspect it was an OOM failure.

* Run a52ad training with SAC for 40k iterations, with larger NN. This is a re-do of previous run 33541, but...
	* Reduced num_cpus_per_worker to 2 and num_cpus_for_local_worker to 2.
	* All trials errored before 700k steps. All were unknown system faults (OOM?). I verified that the
	  resource configs were the same as on 10/3, before I started playing with them.

10/6/23

* Performed system update, per system notes, due to inaccessible GPU.

* Run ec5a7 training with SAC for 40k iterations, with larger NN...re-re-do.
	* Set num_rollout_workers = 1 (was previously unspecified), to help limit memory usage.
	* Trials 3, 4 & 5 all died due to system fault (possible OOM). 3 made it to 1M steps, but 4 & 5
	  stopped before 400k steps.
	* Trials 0 & 1 ran to completion, ~8M steps, which is the first time I've seen any trial do that.
	* Trial 0 (noise stddev 0.23 over 12M steps) reached rmax > 1 quickly, but occasionally dropped well
	  below that a few times. After 5M steps, rmean fluctuated noisily ~0.8 and stayed there. rmin never
	  consistently exceeded -1.3. Mean episode length was ~78 after 5M steps.
		* Inference shows agent prefers speed 18.2 m/s, although it stayed close to the ramp 0 speed
		  limit initially. It never changed lanes.
	* Trial 1 (noise stddev 0.45 over 12M steps) showed a steady climb in rmax from 0.5 to 1.4 over the
	  8M steps. rmean stayed low until 4M steps, then gradually curved upward, crossing 0 at 7.5M steps
	  and ending > 0.4 at 8.1M steps, with every indication that it would have continued to climb. rmin
	  stayed ~-1.4. Mean episode length gradually curved upward too, ending at 70.
		* Inference shows a preference for 22.9 m/s, but otherwise behaves same as trail 0. When it
		  runs on a target lane, it always succeeds, and gets a fat reward.
	* Trial 2 proceeded will to about 3.5M steps, reaching rmean = 0.8 by then, but the job died without
	  any error message at that time. It had noise stddev = 0.24 over 8M steps.
	* This run performed at approx 5400 iters/hr, running two trials simultaneously, but seems to have
	  had some memory problems.

10/7/23

* Run 17e90 training with SAC over 40k iterations on 1 trial
	* Set episode completion reward to 0 (was 1) to force agent to pay more attention to the lane change
	  opportunities and speed control opportunities.
	* Fixed the noise magnitude at 0.25 over 12M steps (not tunable).
	* Trial completed. rmax was very noisy, bouncing rapidly in [0.1, 1.2], while rmean slowly fluctuated
	  in [-0.3, -0.1] after 4M steps, but mean episode duration was ~77 after 5M steps.
		* Inference shows no consideration of lane change, and a desire for speed of 14.8 m/s.
	* Half-way through the run, performance was showing 2400 iters/hr.

10/8/23

* Run a5c6d training with SAC over 40k iterations on 1 trial
	* Changed reset() to spread out initial P location of ego vehicle closer to end of its lane (up to
	  150 m from the end in all case. For early episodes (< 10k) it also skews the choice of location toward
	  the lanes without targets, and to P coords near the end of whatever lane is chosen. Al this will help
	  expose it to failure scenarios.
	* Changed BridigitCtrl.plan_route() to scale outputs by max_prob instead of by sum_prob, thus magnifying
	  the difference between larger and smaller ones, because the largest value is always 1. Also changed
	  its calculation of desirable probability to be dependent only on the remaining distance in the ending
	  lane, not on its length relative to the target lane.
	* Changed num_gpus from 0.5 to 1 to give more compute to the single trial worker.
	* Training performance was 2710 iters/hr.
	* Judging by shape of reward curves, the first 10k episodes, which dictate more difficult initial conditions,
	  were completed after only ~300k steps (avg 30 steps/iter).
	* Inference was no good. It sought a constant speed, well below nominal speed limit (values 17-19 m/s,
	  depending on which checkpoint was used), and it never attempted to change lanes.

10/9/23

* Run 424f1 training with SAC on 40k iterations on 1 trial. Corresponds to train branch commit 80a32e7.
	* Changed the early episode threshold in reset() from 10k to 100k episodes, which should correspond to
	  closer to 4M steps.
	* Improved BridgitCtrl.plan_route() so that it won't recommend a lane change unless it is legal for all of
	  the following planning cycle (approx; it looks ahead 6 sensor zones).
	* Removed penalty for widely varying speed commands (just to keep it out of the way for now).
	* Increased reward speed_mult from 0.018 to 0.030, since this penalty was getting washed out by LC bonus.
	* Changed noise magnitude from 0.25 to 0.35.
	* Reward curves don't look as good as prev run. The "early episodes" lasted ~2.2M steps, after which rmax
	  dropped a lot. It then peaked at 0.5 ~6M steps, then dropped to 0 and only recoverd to 0.1 afterward.
	  rmean peaked at -0.2 @ 7M steps, then dropped.
	* Performance was 2790 iters/hr.
	* Inference at ~22k and 37k iterations shows it seeks steady speed of 16.4 m/s and does not change lanes.
		* I notice that LC desired bonus is being collected generously (full value) when it cannot change
		  lanes (e.g. early part of lane 0), so it is getting rewarded for non-action. Also, it is collecting
		  this full reward on the exit ramps in the suicide phase, which reinforces simply staying in the
		  same lane always.
		* I'm concerned that the number of obs inputs from the sensors is overwhelming the handful of inputs
		  that are most important (speed diff, lane change commands).

* Run fe196 training with SAC over 40k iterations on 1 trial
	* Change reward bonus for LC desirability to be awarded only if there is a choice other than current lane to
	  be made (possibly current lane is one of the choices).
	* Increased reward penalty speed_mult from 0.3 to 0.5 to further emphasize speed discrepancies.
	* Increased episode length from 80 to 160 steps to make it more difficult to complete without a lane change,
	  and slightly more chance of including a target destination.
	* Increased noise magnitude from 0.35 to 0.4.
	* Changed num_cpus_per_worker and num_cpus_for_local_worker from 2 to 4 to improve performance.
	* No good. After the first 2.2M steps (early episode treatment), rmax dropped to -1, then after 7M steps it
	  slowly fluctuated in [-1.2, -0.2] for the remainder of the 14M steps. Trial was stopped early (at iter
	  25863) due to poor performance. However, mean episode length did reach ~150 after 6M steps.
	* Performance was 2600 iters/hr.
	* Inference confirms the poor actions I've seen in previous runs.

10/10/23

* Several quick experiment runs with differing noise levels, looking at very early checkpoints. I added logging to show
  the input commands to every step() call. They seem to be pretty well distributed in [-1, 1], although somewhat skewed
  to negative values. It appears, however that the noise makes up virtually all of the magnitude of these commands.
  When running an early checkpoint (from iters 10 to 200) in inference mode, the NN output from every step is within a
  hair of 0, regardless of the situation. With almost no significant learning accomplished by this point, I would exped
  the outputs to look a lot more random.

* Run c4dd8 training with SAC over 40k iterasions - back to multiple trials
	* Switched to 4 trials, since I can train 2 in the same time I can train 1 by allocating resources differently.
	  Back to num_gpus = 0.5, num_cpus_per_worker = 2, num_cpus_for_local_worker = 2, and num_rollout_workers
	  undefined.
	* Increased episode length to 500 steps, which should allow most episodes to run the full track. Changed 
	  rollout_fragement_length to 500 also, to align with a full episode in each fragment.
	* Changed noise magnitude to 0.5, based on experiments earlier today, to help expose it to LC commands with
	  magnitude > 0.5 (a lane change).
	* No good. After the initial episode treatment (2M steps), rmax dropped to -1 and stayed close to that until
	  5M steps, where I killed the job.

* Run e8679 training with SAC over 40k iterations on multiple trials.
	* Turned off Gaussian noise, at Kevin's suggestion.
	* Added exception handling to step() and to BridgitModel.get_obs_vector() to better understand a strange error
	  tripped in the Roadway model.
	* Trial 0 rmean gradually dropped to -11, recovered a bit, then dropped to -12.
	* Trial 1 was stopped early because rmax fell dramatically after 4M steps.
	* Training performance 3200 iters/hr.

* Began studying RLlib code to figure out how to pre-populate a replay buffer (web site doesn't seem to have any info
  on how to do it).

10/11/23

* Run a58db training with SAC over 40k iterations on multiple trials
	* Turned on Gaussian noise at stddev = 0.25.
	* Added explicit replay buffer config to the tuner program. Specified MultiAgentPrioritizedReplayBuffer (which
	  seems to be the default, capacity of 1M (which was the default), and turned on prioritized_replay (which had
	  been off).
	* Program died due to segfault with no explanation, other than a problem in the logger json encoder.
	* Training performance 2800 iters/hr.
	* Trial 0 looked pretty unstable, with rmax slowly fluctuating in [-1, 3] and rmean in [-4, -0.5].
	* Trial 1 was stopped early due to terrible rmax.
	* Trial 2 showed some promise, although rather unstable, with rmax ending ~4 and rmean ~-0.5 when it was cut
	  off.

10/12/23

* Run 95004 training with SAC over 40k iterations on multiple trials. Corresponds to train branch commit 02fdd2ab.
	* Added Xavier normal initialization of all model weights (tested first) in CdaCallbacks, per Kevin's suggestion.
	* Reduced num early episodes from 100k to 20k, since this large number seems to breed a lot of instability.
	* Reset the episode length back to 80 steps.
	* Modified reward bonus for LC desirability to only apply if a LC command was issued.
	* Changed step() to interpret cmd[1] differently:  the boundaries between STAY_IN_LANE and either of the lane
	  changes is now +/- 0.2 instead of 0.5. I hope this will encourage more exploration of lane change actions
	  without needing so much noise to push it into those regions.
	* All trials died very early, possible OOM error.

* Run 57115 training with SAC over 40k iterations on multiple trials.  Re-do of above (95004) after reboot.
	* However, changed num early episodes to 0, as it seems to be generating a lot of craziness.
	* Computer hung in the middle of the first 2 trials, which each completed ~23k iterations (12M steps).
	* Trial 0 seemed headed for an early stop due to poor rmax.
	* Trial 1 held a bit of promise, with rmax ~0.4 at the end, and rmin heading upward from -0.7. It had actor LR
	  = 4.0e-6, critic LR = 1.6e-5, entropy LR = 1.0e-6. Trial 0's LRs were larger on all 3.
	* Analyzed bot placement problems (where it couldn't place all selected). All cases occurred when the ego p value
	  was at the very beginning of its chosen lane. However, many cases should be easy to solve. It seems the algo
	  is too restrictive in the allowed radius.
	* Training performance was 3130 iters/hr.

10/13/23

* Playing with callback for initializing the replay buffer.
	* Found a major defect in my code - the env wrapper was not passing through most of the observations in scale_obs()!
	  Fixed it.
	* The on_postprocess_trajectory() callback is invoked after a trajectory has been completed, and includes all of the
	  (s, a, r, s', a') contents, plus the dones/terminateds, etc, for each step.  Therefore, this is not the place to
	  inject random actions, since it already represents the r, s', etc, of the actions that are included.
	* It seems the place to inject random actions is at the beginning of the environment step(), where it can then
	  compute the r, s', etc, from those actions. That won't work, however, as those new action values won't get
	  captured by RLlib anywhere.
	* Tried implementing the on_episode_step() callback, but it runs after the environment step() method returns.
	* More investigation...still not finding where in RLlib the training loop is that my code uses.

* Run 8b8ce training with SAC over 40k iterations.
	* Fixed tune program to set rollout_fragment_length = 80 to match the episode length, and train_batch_size = 1040.
	* Added a new CdaCallback method, on_postprocess_trajectory(). It's a no-op for now.
	* Modified reset() to relax the bot vehicle placement region a bit to minimize the number that have to be
	  deactivated.
	* Includes fix to HighwayEnvWrapper to scale all observations rather than zeroing out most of them!
	* First two trials ran to completion (7.5M steps), but trials 2 & 3 stopped very early with an unknow problem (OOM?).
	* Trial 0 reached rmax ~3.5 at 2.5M steps, then stayed close to that. rmean peaked ~0.7 at that time, then
	  slowly & noisily drifted down to 0.2. Mean episode length was only 35-40.
		* Inference on checkpoint 13280 (~2.8M steps) shows the agent starting in lane 1 frequently changing lanes
		  and speeds, fluctuating them back and forth, but completed the route! Speed was well below speed limit,
		  but it was making bank on LC cmd rewards, so didn't need to worry about speed penalties.
		* Several times it made illegal lane changes.
	* Trial 1 reached rmax of 3.5 in only 0.5M steps, then stayed close to that. Its rmean hit 0.6 at a similar time,
	  drifted noisily down to 0.3, then back up to 0.6. Both trials had rmin < -1.2, and dropped rapidly after 4M steps.
	  Mean episode length was similar to trial 0.
	* Training took 17.6 hr with a productivity of 4840 iters/hr, also 911 steps/hr, which is probably a more
	  transferrable measurement, since it doesn't depend on batch size.

10/14/23

* Run b24d6 training with SAC for 40k iterations. Corresponds to train branch commit 44edb60.
	* Changed reward bonus for LC des from 0.1 to 0.01.
	* Uncommented reward penalties for varying speed commands and varying LC commands.
	* Altered the top of step() to change the LC cmd threshold back to 0.5, as it had always been.
	* Run cut short by a segfault elsewhere in the system. Trials 0 & 1 had completed ~29k iters (5M steps), and were
	  both showing good promise, with still a long way to go. 
	* Both trials had rmax gradually climbed from -0.5 to -0.1 while rmean gradually climbed to -0.7 and rmin was
	  climbing steadily as well. Episode durations were > 70 at the end.
	* Inference on trial 1 (the better one at the end) shows improved desire to stay somewhat close to speed limit
	  with lower frequency speed changes. It also has much fewer lane changes. It appears on lanes 0 & 4 that it is
	  somewhat attempting to match the speed limits, although reaction is pretty slow.

10/15/23

* Run 5dac3 training with SAC for 50k iterations. Corresponds to train branch commit 62e3976.
	* Adjusted rewards so that lane command penalty is reduced from 0.01 to 0.002, since it was wiping out the LC
	  desired bonus being awarded for just a few time steps.
	* Fixed defect in obs for steps since lane change that wasn't getting initialized correctly. It will affect
	  penalties for the first lane change maneuver.
	* Increased multiplier for lane change penalty from 0.0001 to 0.0002.
	* Increased multiplier for speed command penalty from 0.04 to 0.1 to encourage more smoothness.
	* I killed it early, after ~27k iters, since I found some fixes/improvements that I felt were essential
	  (see description of the next run).
	* Trial 0 rmax was climbing slowly, reaching -0.3 after 5M steps. Trial 1 (with smaller LRs) was jaggedly
	  moving downward, with recent rmax varying slowly in [-1, -0.5].

* Added Argparse to the inference program and got it ready to take in filename for a NN weights file for the Bridgit
  controller.

* Run 13930 training with SAC for 50k iterations.
	* Fixed defect in BridgitModel counting steps since lane change, which led to too much penalty for a lane
	  change.
	* Increased reward bonus for lane change desirability from 0.01 to 0.02.
	* Narrowed the tuning range for LR on critic and entropy from [1e-6, 1e-4] to [5e-6, 1e-4].
	* Run ended early with segfault. Seems to be in the json encoder (saving plots?). Both trials 0 & 1 were
	  near iter 44k (8M steps).
	* Both trials performed similarly, with rmax climing slowly but pretty steadily to -0.1 at the end. rmean
	  was on a steady climb, ending at -0.90. rmin was quite noisy, but still climbing also. All 3 look like
	  they would have kept climbing. Mean episode length hit upper 70s after ~5M steps.
	* Inference on trial 1 shows it performs lane changes occasionally, but often it seems random, not for a
	  beneficial purpose. This might be corrected with more training. Also, speed control is still pretty
	  jittery and earns quite a bit of penalty for speeds too slow. It doesn't seem to understand speed limit
	  changes.
	* Performance was 5030 iters/hr, 890k steps/hr.

10/16/23

* Attempted to test the necessary package installation for this project with the requirements.txt file, but could
  not get pip to work. Then I couldn't get conda to create a new environment (to start a fresh install of python
  & pip). Giving up - too frustrated.

* Run ac2f2 training with SAC over 80k iterations.
	* Increased max iterations from 50k to 80k.
	* Increased reward bonus from 0.02 to 0.04 for LC desirability matching, and added a negative bonus if it
	  chooses a LC direction associated with a very small desirability.
	* Run died due to segfault when trials 0 & 1 had completed ~48k iters.
	* Trial 1 had rmax climbing steadily throughout, reaching 1.5. Its rmean climbed steadily too, until 
	  ~7.5M steps, then leveled out at ~0. rmin also climbed steadily until ~6M steps, and leveled at ~-1.7.
	  This trial had actor LR = 6.8e-5, critic LR = 1.6e-5, entropy LR = 3.2e-5.
		* Inference on lane 0 shows agent has much better speed control than previous runs, but still
		  fluctuates like a sine wave. It also changed lanes frequently, went to lane 1 before the exit
		  ramp split off, then selected lane 2/1/2 a couple times. Avoided some close call crashes!
		* Starting in lanes 1-3 looked similar, with lots of lane changes, but speed profile attempted to
		  match the speed limit change near the end.
		* It appears that it can't decide between lanes 1 & 2 (both have targets), so it frequently
		  goes back and forth between them. Needs more incentive to stay in one lane if it works.
		* Tested every starting lane. At no point did the agent run off road or crash into a neighbor.
	* Trial 0 looks less good, with rmax peaking at 0.6, then reducing to ~0. rmean climbed steadily, then
	  slowed after 4M steps, but continued climbing, ending at -0.4. Its rmin ended slightly better than
	  that for trial 1.
	* Training performance was 5090 iters/hr or 891k time steps/hr (took 19 hr).

10/17/23

* Run 35b28 training with SAC for 80k iterations. Corresponds to train branch commit 0f3ad47.
	* Added logic to reward bonus for LC desirability to only award the bonus for the first 2 time steps
	  that the command is given, at the beginning of a maneuver, which prevents the agent from scooping up
	  extra reward with no effect (leaving the command on for the whole maneuver). Since the bonus will be
	  awarded much less now, its value is increased from 0.04 to 0.1. Also changed the bonus decision to
	  be only if the cmd desirability > current lane desirability (it was >=).
	* Increased LC cmd penalty (for cmd frequency) from 0.0002 to 0.001.
	* Doubled the penalty for varying speed commands (multiplier from 0.1 to 0.2).
	* Changed tuning range for learning rates so that all 3 are now in [1e-5, 1e-4].
	* Trial 1 ended early due to code defect that allowed an illegal round-off error P value to be used.
	* Run ended early on a SIGSEGV that again appears to be in the checkpoint storing code of Ray. At
	  end, trial 0 completed 54k iters and trial 2 completed 25k iters.
	* Trial 0 rmax crossed 0 at 7M steps and almost leveled out, reaching ~0.05 before dying just after
	  9M steps. rmean climbed pretty steadily, although slowed after 7M steps, but ended at its peak of
	  -0.4, while rmin mostly leveled after 7M at -1.0.
		* Inference: did not change lanes out of lane 0 or 5. Overall, lane changes are way down; only
		  a couple extraneous one in several runs. Saw one off-road. Speed profile is still somewhat
		  jagged, but follows speed limit fairly well. Made one poor LC choice from 4 to 5. Overall,
		  it seems it needs more reward for desired lane changes, and generally a bit more training.

10/18/23

* More investigation into how to pre-populate the experience buffer.

* Run 07eb6 training with SAC on 80k iterations. Corresponds to train branch commit 5355739.
	* Changed tuner to store checkpoints only every 50 iters (was 10), and not storing checkpoint at end.
	  This should minimize the chances of a SIGSEGV killing everything.
	* Fixed RoadwayB.param_to_map_frame() to ensure slightly small P values are not going to raise an
	  exception.
	* Reward penalty for deviating from speed limit increased multiplier from 0.05 to 0.08.
	* Reward bonus for desirable lane changes changed from 0.1 to 0.4, since they are so rare.
	* Run stopped prematurely due to segfault (SIGSEGV apparently in JSON logger). Both trials had
	  reached ~51k iterations (9M steps).
	* Trial 0 rmax climbed a little unsteadily to ~1.2, rmean climbed fairly steadily past -0.2, and
	  looks like it would continue. rmin climbed unsteadily to ~-2.0. Mean episode length came close to
	  80 at 6M steps, then backed off a bit to ~75 for the duration.
	* Trial 1 rmax was a steady, slow climb to ~1.2. rmean climbed more aggressively, then slowed a bit
	  but contined to climb to 0, and looks like it would continue. rmin climbed a bit noisily to -1.5
	  at 6M steps, then almost leveled off, ending ~-1.3. Its mean episode length hit ~57 at 3M steps,
	  then continued to struggle for more gains, but finally leveled off ~69.
		* Inference: speed profiles a bit smoother, but the speed command penalties are very small,
		  and probably not having a noticeable impact. Several extraneous lane changes, but fewer
		  than seen in previous runs, and most were done safely. Start in lane 2 ended with legal
		  lane change to 1 into a bot vehicle. Lane 3 start did off-road immediately (one time).
		* I think it is trying to follow speed limits pretty well, but can't avoid speed noise.
		  Speed penalties are killing it. Probably need more penalty for speed fluctuations and
		  less on absolute speed deviations.
		* Refuses to change lanes out of 0, but easily merges from lanes 4 and 5.

10/19/23

* Run 6a61d training with SAC for 80k iterations. Corresponds to train branch commit 3b0b4e5.
	* LC desirability bonus - doubled this from 0.4 to 0.8, and made the poor-choice penalty -0.8.
	* Doubled the penalty for lane change (based on time since previous). Mult is now 0.002.
	* Increased speed command penalty mult from 0.2 to 1.0 to help smooth out the profile.
	* Reduced penalty for speed deviation - multi from 0.08 to 0.04.
	* Run died unexpectedly after 43k iterations (8M steps) due to GCS rpc timeout. gcs_server.out
	  does not seem to contain any info that would explain the problem.
	* Both trials had rmax climbing steadily, ending > 3. rmean similary, ending ~0.5 (trial 1 was
	  a little higher). rmin also steady climb, ending at -2. Oddly, mean episode lengths took a
	  dive. Trial 0 peaked at 70 ~4M steps, then steadily declined to 45. Trial 1 peaked ~55 at
	  2M steps, then dropped quickly to ~40 at 4M steps, then stayed there.
		* Trial 0 had actor LR = 5.8e-5, critic LR = 4.9e-5, entropy LR = 2.8e-5. Trial 1
		  values were slightly higher on all.
		* Trial 0 inference using its latest checkpoint (poor episode length). Lots of
		  extraneous lane changes, and wildly varying speeds (almost to the extremes several
		  times). Trial 0 inference on its earliest checkpoint (88% training complete, so
		  still on the downward slope of mean episode length): performed similarly, but speed
		  variations weren't quite as pronounced. It seems it experienced some catastrophic
		  forgetting along the way.
		* Trial 1 inference on its latest checkpoint showed similar behavior to trial 0, but
		  this one had much more fondness for extremely slow speeds.
	* Training performance
		* I am changing the way I look at this, looking at the average for a single trial, so
		  it will be easier to compare to different numbers of active trials. Usually, only
		  one trial will be useful anyway.
		* Avg 2490 iter/hr/trial over a 17.4 hr run (447 steps/hr).

10/20/23

* It has gotten to the point that I feel the need to reboot the computer just before starting every
  run to limit the chances of some sort of a crash. It seems many of these could be OS problems, or
  Ray memory mgmt problems, which build a history in the OS between runs.

* Run ce230 training with SAC for 80k iterations. Corresponds to train commit 7a01a05.
	* Increased speed deviation penalty somewhat, from 0.04 to 0.06 to modulate this penalty from
	  previous run.
	* Dialed back the speed command penalty mult from 1.0 to 0.5 to modulate from previous run.
	* Increased lane change penalty by adding a constant 0.002 regardless of time elapsed since
	  the previous maneuver. It seems most maneuvers have been waiting 60 steps or more, but still
	  doing them for no particular reason. This should slow that down.
	* I killed it after 35k iters, as the mean episode length was going way down again, on both
	  trials (one was < 40 steps). It is apparent the agent prefers suicide over a long, painful
	  episode.
	* A few quick inference runs on one of the trials (5M steps) shows wildly fluctuating speeds,
	  a preference for slow speeds, and several irrational lane changes. Some of this may be due
	  to insufficient training, however.

10/21/23

* Run 582ee training with SAC for 80k iterations. Corresponds to train branch commit b4d2e92.
	* Added episode completion bonus of 1.0 to encourage longevity.
	* Realized that the extraneous lane changes were its way of setting up an unnecessary number
	  of extra rewarding lane changes later (since the poor ones often don't get a penalty).
	  Added penalty for any lane change that goes against the desirability factor. Also, dialed
	  back the magnitude of this bonus/penalty so that it doesn't overwhelm the completion bonus,
	  and allows speed penalties to be more visible.
	* Died from SIGSEGV, again apparently during checkpointing ops, after ~14 hr.
	* Trial 0 had all reward curves moving steadily upward, and looking like they would go further
	  with enough training. rmax reached 0.8, rmean reached -0.1 and rmin ~-1.5. Mean episode
	  length ~80.
		* Inference: it performs most necessary lane changes, but not always (esp from lane 0)
		  and occasionally does an unnecessary one, but that is rare. The big problem is
		  speed likes to stay down around 23 m/s, regardless of the local speed limit. It
		  seems it is not seeing this in the obs.
	* Trial 1 showed slightly higher reward curves, but its episode length was more jittery and
	  hadn't yet exceeded 70.
	* Training performance: 2440 iter/hr/trial, 491 steps/hr.

10/29/23

* Tested observations for proper representation of speed limit in each forward sensor zone. It is
  getting all of these correct.
	* It seems maybe the best bet now is to train for longer. Need to get lucky with pickling
	  errors, however.

* Run 45914 training with SAC for 80k iterations. No changes since previous run on 10/21.
	* Run died after ~8k iters due to a checkpointing error I had not seen before. It was a
	  writing problem to a temp file (.tmp_expermient_state....), but I later found that I had
	  problems writing to other files, so it seems to be a widespread OS problem.

10/30/23

* Created new Git branch "ray_train" off of "train" to handle an experiment to convert from Ray
  Tune to Ray Train, since I'm having so much trouble running Tune.
	* Created a train.py program to replace my heretofore used tune.py and got it to basically
	  run.  Let it go for 80k iterations just to see what's possible. However, it does not save
	  any checkpoints, so it is not useful just now. It also isn't able to write to the correct
	  Tensorboard data directory.
		* Since there is no tuning, I selected all three LRs to be 5e-5, which is right in
		  the middle of the ranges that have been used in the recent past.
		* It ran for 75k iters before I killed it, so seems to be sidestepping whatever
		  problems Tune was experiencing in long runs.
		* Performance measured over the latest ~1 hr of its execution shows 3070 iter/hr.
		  It was hardly using any of the GPU and only ~1 cpu.

10/31/23

* Run a single Ray.Train run with SAC for 80k iterations (still on the ray_train branch). Corresponds
  to ray_train commit 10ab0c32.
	* Set num_gpus = 1, num_cpus_for_local_worker = 4, num_cpus_per_worker = 4.
	* Added checkpoint storage (pickle format for now) every 500 iters. I have set their
	  destination to the ~/ray_results/cda1 dir. Checkpoints are taking 33 MB apice!
	* Tensorboard data is still going to an autogen dir under ~/ray_results, but that's okay
	  for now.
	* Played with resource allocations some to figure out how to fully use what's available.
	  I cannot get it to use more than a small fraction of the GPU without a bunch of cuda
	  warnings.  Need to investigate that later.
	* Reward curves pretty much leveled off after 10-12M steps, and the episode mean length
	  leveled off ~80 after ~6M steps. rmax climbed a bit between 30M and 40M steps to ~1.3,
	  but rmean stayed about level through this period; rmin actually dropped after 20M steps
	  by ~20%. Also, episode mean len dropped from 80 to ~70 after 25M steps. So it appears the
	  best checkpoint may be from ~25M steps, or iter 50k.
	* Checkpoints are readable by the inference program.
	* Training ran to completion (~40M steps)! Performance was 3130 iter/hr.
	* Inference on checkpoint 42001 (training ~half done) shows that speed control is getting
	  fairly decent. It definitely responds to speed limits, but still is kinda noisy. Penalties
	  for speed command could stand to be a little larger.  However, lane changes are not very
	  good. It doesn't like to do them, and always overlooks the need from lanes 0, 4 & 5. But it
	  regularly does the required LC from 3 to 2. I only saw one extraneous LC in ~10 runs.
	* Inference on checkpoint 50001 shows similar behavior to that at 42k iters.

11/2/23

* Investigated the cuda error "Could not parse CUBLAS_WORKSPACE_CONFIG, using default workspace size"
  when num_gpus_per_worker > 0. It is not a Ray problem, it is apparently defective cuda software that
  performs an invalid regex test on the indicated env variable. Therefore, setting the variable will
  not fix the problem. The code handles the error by assigning a default value, so it is harmless. 
  Therefore, I found a way to eliminate these warnings from my run log (in the train.sh script with
  grep commands). Doing so makes the stdout rather slow & jerky, apparently due to buffering, but it
  seems to work fine. It does slow down the training performance a bit (~3%).

* Starting new nomenclature for these Train runs, since Ray doesn't automatically number them. This
  first series will start with 'A' to indicate we are still in part 1 of the cda1 project.

* Run A1, Ray.Train with SAC for 10k iterations, still on the ray_train branch.
	* Tuned the resource usage to get more steps/hr. Can now get the above 40M steps in only 8k
	  iters using 8 environments/worker. This should finish in substantially less time also.
	* Eliminated the lane change command penalty (for widely differing commands).
	* Doubled the lane change desirability reward/penalty from 0.2 to 0.4.
	* Doubled the speed variation penalty mult from 0.5 to 1.0.
	* Died after 600 iterations due to a segfault in concatenating sample batches.

* Run A2, Ray.Train with SAC for 10k iterations on the ray_train branch. This is a complete duplicate
  of the above run, after a reboot.
	* Inference on an early checkpoint (4401) shows consistent desire to do left-hand LC a little too early,
	  running off-road. Speed profile is very smooth, but accepts low speeds & big penalties as a result.

***
* Attempted to create a msgpack checkpoint converter, but got stuck.
	* It requires installation of msgpack_numpy, which can be added with `pip install msgpack msgpack_numpy`.
	* I tried to create new conda env cda1 from importing cda0.yaml. Conda/pip barfed due to some python
	  version incompatibilities on an unidentified package. Also unable to install two packages compatible
	  with torch 2.0.1+cu118. This is identical errors I found on 10/16 installation attempt (documented
	  above).
	* Probably need to create a brand new conda env from scratch and install everything needed using pip,
	  rather than mixing conda installer & pip installer commands. This might then solve the requirements.txt
	  problem as well.

* Merged ray_train branch back into the train branch, accepting that this is a better way to go than Tune.

* Run A3, Ray.Train job with SAC for 10k iterations.  Back on the train branch.
	* Reduced lane change desirability bonus multiplier from 0.4 to 0.3.
	* Reduced the speed variation penalty mult from 1.0 to 0.7 to allow it to explore a little more.
	* Ran to completion in ~5 hr (42M steps at 8900 steps/hr).
	* Reward curves start to look unstable and generally drop after 30M steps. Mean episode length also
	  starts to drop around 33M steps. Therefore, last best checkpoint is probably 7201.
	* Inference on checkpoint 7201: lane 0 start never tries to change lanes
		* Speed profile is a little rougher than before, and always well below the speed limit. It feels
		  as if speed is being misinterpreted somewhere so it is trying to reach a target that is slow.
		* It understands the need to change lanes out of 3, 4 and 5 and tries to do that as early as
		  possible. This usually results in a) running off road because the attempt is prior to the next
		  lane becoming available, or b) crashing into a bot next to it. In either case, it doesn't seem
		  to be paying attention to its sensors (or they are not working properly).

11/3/23

* Debugged the speed command vs actual speed data in actions, observations and vehicle object internal data. All
  seems to be consistent and correct, and reward penalties are calculated plausibly from those values. Over the
  course of an episode (80 steps), the penalties it is typically accumulating due to speed errors is roughly the
  same magnitude as the episode completion bonus, so it seems this penalty is sized reasonably.

* Run A4, Ray.Train job with SAC for 8k iterations, starting from scratch.
	* Reset the EARLY_EPISODES constant from 0 to 10k to force reset() to place the ego vehicle in the ramp
	  lanes most of the time in those first episodes. Hoping this will give it enough experience to do a
	  right lane change from lane 0.
	* Turned off the Gaussian noise.
	* Added capability to the train program to begin from a baseline checkpoint. Not using a baseline on
	  this run, but it is now capable.
	* After 2300 iters, it is clear that there is no learning occurring, so I killed it.

* Run A5, Ray.Train job with SAC for 8k iterations, starting from scratch
	* Turned on Gaussian noise again, and changed its params so that it decays over 30M steps (was 12M).
	* At completion (30M steps), all 3 reward curves were still climbing, but rmax & rmin were fairly
	  noisy. rmax ended ~1.5, rmean at ~0.1 and rmin ~-1.6. Mean episode length climbed slowly over first
	  17M steps to only 60, then suddenly jumped to ~80, where it stayed for the duration.
	* Performance was 8.9M steps/hr.
	* Inference: lane 0 start shows speeds well below limit, and no inclination to change lanes. Starts in
	  lanes 3 & 4 performed better, with speeds still low, but not as bad, and appropriate lane changes.
	  Lane 5 start was similar. Its final LC from 2 to 1 (unnecessary) waited a long time for the two
	  neighbors that were blocking it to move out of the way before it did the maneuver. Not sure why it
	  insists on going from 2 to 1 in the first place, but it was a safe maneuver.
	* Maybe this is going in the right direction and just needs more training time.

* Run A6 training with SAC from the A5 checkpoint 80000 for an additional 20k iterations.
	* Note that this is starting over on the noise level. I wonder if a continuation like this would be
	  better to set the noise to its minimum level?
	* Reward plots are quite ragged, but all 3 are trending upward. rmax moved from 1.0 to ~1.4. rmean
	  started ~-0.7, which is concerning given its baseline performance, but worked its way up to ~0.3.
	  rmin went from -2.6 to ~-1.4 (very noisy). Mean episode length started ~60 and stayed there (noisy)
	  for the first 20M steps, then jumped up to 78 and stayed above that for the remainder. This jump
	  corresponds to a similar jump in rmean and a smaller one in rmin at the same time.
	* Over this run plus the baseline run, 110M steps were collected. Performance for this run was 8874
	  steps/hr.
		* Since this run started at step ~30M, it isn't clear to me how much noise it was injecting.
		  The noise model is specified to degrade over 30M steps, but I don't know if it starts from
		  the beginning of a run, or continues from the baseline step count.
	* Inference on checkpoint 28000 (the final one; IDs are cumulative iterations from baseline):
	  speed control looks a little better than I've seen before, but not great. It definitely seems to
	  recognize the various speed limits. Eager to change lanes to the left, esp starting in lanes 4 & 5.
	  It even moves from 4/5 all the way to 0, which is a bad choice. It never changes right away from
	  lane 0 (or any other lane).
	* Stored this checkpoint for future reference.

11/4/23

* Run A7 training with SAC from scratch for 30k iterations. Corresponds to train branch commit e29ac28.
	* Enlarged the NN structure from [600, 256, 128] to [1024, 256, 128] for both the policy & Q networks.
	* Reduced LR for actor & critic from 5e-5 to 3e-5.
	* System crashed after iteration 13100. Rebooted and then restarted this training from that checkpoint.
		* Restarting doesn't bring back the entire state. This is obvious because the reward curves and
		  the mean episode length curve start in way different places than where they were left in the
		  original run.
	* Ran to completion, but the system started experiencing segfaults very near the end of the run.
	* Reward curves were somewhat noisy, but moving all over the place - not steady. Best cases, they
	  were no better than what I'd seen before.

11/11/23

***** Rebuilt Tensorbook laptop - see Notes.txt for details. Now using conda env "cda", which has been
	redefined as well.
	* The "cda" env was created with Ray 2.5.1, due to previous bad experience with 2.6. But Ray 2.8.0 is
	  now available, and docs looks like it may have fixed my previous complaints.
	* Ran a trial of the cda1 train program, and it is working.
		* It died after 9k iters (42M steps) due to an error in the replay buffer (SIGABRT).

11/12/23

*** Tentatively completed investigation of how to force random actions at every time step so that I can
  pre-populate the experience buffer with random action/obs pairs. The hope is that this kind of initial data
  will give the agent the broad experience base it needs in order to start learning. See RLlib_notes.txt for
  details, but the gist is that it seems the Gaussian noise class provides this capability already.

* Run A8 using SAC for 30k iterations. Corresponds to train branch commit da74fee.
	* Changed Gaussian noise variable random_timesteps from 10k to 1M. This seems to be what I need to
	  inject episodes based on fully random action values into the experience buffer at the beginning of
	  the training.
	* Completed 30k iterations, 142M steps. rmax mostly stayed in [0.8, 1.6], with a couple short dips 
	  down to 0.4. rmean climbed rapidly to ~0.25 at 30M steps then fluctuated until ~65M steps, when it
	  dropped to -0.7, then noisily climbed slowly back up to -0.2. rmin reached noisy -1.6 up through
	  ~65M steps, then dropped back to < -2.0 for the remainder. Mean episode length hit ~78 at 20M steps
	  then got more noisy & maybe dropped slightly after 60M steps.
	* Inference on checkpoint 13500 (64M steps): lane 0 starts refused to change lanes, as needed. Lane 1
	  starts worked okay, but speed is always below speed limit, and drops dramatically near the merge
	  zone (as previous runs did). Lane 3 & 4 starts attempt illegal lane changes to the left just prior
	  to the beginning of the merge zones. In short, similar performance to anything I'd seen earlier.
	* Inference on checkpoint 30000 (142M steps): speed control is definitely better, keeping closer to
	  the speed limits, but still a bit noisy. LC behavior may be slightly better, but basically the
	  inconsistent performance reported above. When in lane 0, the raw LC cmd action never even gets
	  close to the threshold, so it isn't even thinking about changing lanes.

11/13/23

* Run A9 training with SAC for 30k iterations from scratch.
	* Changed episode length from 80 to 100 steps, thinking this may allow the agent to see bigger
	  cumulative effects of small penalties.
	* Increased noise duration from 30M to 80M steps, and increased initial random timesteps from 1M to
	  4M.
	* Increased replay buffer capacity from 1M to 5M steps (already was prioritized). Didn't work - 
	  see below.
	* Two consecutive runs died between iters 300 and 400 (after 1.5M steps) with a segfault. I suspect
	  this is a symptom of an overly large replay buffer, so reduced it to a capacity of 2M.
	* This attempt also had a segfault at roughly the same time.
	* Adjusted buffer capacity back to 1M, and random_timesteps to 1M also, since there is no point in
	  generating more random steps than can be held in the buffer.
	* Third attempt crashed with tensor size mismatch in Ray's sac_torch_model after 8M steps. Considering
	  that this may be simply due to a need for reboot after previous memory corruptions.
	* Completed all 30k iters, ~96M steps. rmax peaked at 1.3 ~39M steps, then got real noisy in [0.7, 1.2].
	  rmean reached -0.2 at 40M steps then lived in [-0.2, 0] until 70M steps, then got a lot more erratic.
	* Inference on checkpoint 30000 (from 96M steps): similar inadequate speed performance, but lane
	  changes were worse. From lanes 3, 4, 5 it did illegal LC to the right (which it has never done before).
	  Did not LC from lane 0.
	* Inference on checkpoint 12000 (from 41M steps): similar speed behavior. LCs were at least in the correct
	  direction, but tended to crash sideways into a neighbor (from lane 5). No LC from lane 0.

11/15/23

* Merged the "train" git branch into develop, indicating the end of that effort. Created a new branch off of
  develop, named "vector-embed", where I am beginning to build a vector embedding solution for the sensor data.

* Began building the code to train the vector embedding.

11/18/23

* Completed writing & testing the embedding data capture program.

* Ran a data capture for 1000 episodes. Run time was ~3 min. This corresponds to vector-embed branch commit 41020523.

* Rearranged the obs vector to make a regular raster map of the whole zone grid. The center column now has no special
  treatment - it looks just like the other columns, and holds data in the ego zone. Center lane's boundary indicators
  have been moved to the end of the sensor area by themselves.

* I also added an up-front obs element reflecting the local speed limit at ego's location. This is redundant with
  info available in the sensor zone data, but I'm afraid that is getting lost in all the noise (and even moreso when
  it gets compressed), so wanted to pull it out separately to make it stand out more.

* Started a run of the embed_collect program for 20k episodes or 2M steps. This corresponds to commit 5af04473 on
  the vector-embed branch. It completed 20k episodes and collected 1,675,049 steps in a couple hours. The data file
  is observations_231118.csv, and is 8.6 GB.
	* Realized that the final step of an episode that ends in off-roading shows a vector of all zeros. Crashes
	  don't have this problem.

11/21/23

* Fixed BridgitModel and EmbedModel so that they don't bail out if the vehicle is inactive; they will still collect
  observations. This should prevent the all-zeros rows seen earlier.
	* Removed the previous observation capture, since it had many episodes affected by this problem.

* Redefined the obs vector elements for host's lane boundary data. Rather than have one element for each side of
  each zone, I reduced it to only 3 "regions" along the length of that lane, so only 6 data elements. These are
  now pulled out of the "sensor grid" and stored separately, so won't be captured for the autoencoder to look at.
	* In the process I found & fixed a defect where the forward-most element of the sensor grid was being
	  populating incorrectly and overwriting some of the host lane boundary cells, which may have hurt the
	  prior training.

* Ran embed collection program again. This time each step's vector is 500 elements long.
	* Dataset contains 1,834,984 time steps (20k episodes), and is 8.7 GB. It is stored in the training dir.
	* Created & tested a custom dataset class, and found that it expects a header row on the CSV, so I
	  added one.
	* Wrote a splitter script and split this file into 85% training data and 15% test data.

11/22/23

* Completed a first version of the embed_train program that trains the autoencoder, from which I will extract
  its embedding layer. Stored in vector-embed branch commit 95f3372.

* Run E1 of embed training for 50 epochs. Embedding dim = 200 neurons, batch size = 256, LR = 0.0005.
	* Used 4 workers
	* Unable to get tensorboard to plot the results, but didn't try very hard.
	* Training loss started at 0.09299, and steadily dropped to 0.08894.
	* Test loss started at 0.08954, and dropped somewhat jerkily to 0.08909.
	* Both losses were still dropping at the end, indicating more epochs would be useful.
	* Weights stored as embedding_200_231122-1830.pt

* Run E2 of embed training for 80 epochs. embedding dim = 200, batch = 32, LR = 0.0005.
	* used 4 workers again. This uses ~30% of GPU.
	* Training loss started at 0.08985, then settled ~0.08895 after 26 epochs.
	* Test loss started at 0.08918 and dropped to 0.08911 at epoch 33. After that min, no epoch reached
	  more than 0.2% above the min value.
	* The whole thing took ~1 hr.
	* Weights stored as embedding_200_231122-2047.pt
	* Losses here are almost identical to those in run E1 (slightly higher), so it seems batch size
	  is not a big factor, but larger may be preferred.

* Run E3 of embed training for 50 epochs. embedding dim = 200, batch = 256, LR = 0.0001.
	* Weights stored as embedding_200_231122-2205.pt
	* Both training & test losses monotonically dropped through all 50 epochs.  Need to run again with
	  longer duration.

* Run E4 - same as E3, but for 200 epochs.  Same output weight file also.
	* Computer crashed.

* Wrote the embed_eval program to help visualize a side-by-side comparison of the input sensor tensor
  and the embedding output tensor. Ideally, these will be identical, and this will help to understand
  where the challenges may be.
	* Began testing it & confirmed it is working okay. Commit 0d0a616 on the vector-embed branch.

11/23/23

* Running evaluation program on the 231122-1830 model. Ran it on several randomly chosen records from
  the test dataset.
	* The pavement layer correctly reproduces all of the mainline surfaces, but for all the non-
	  pavements it shows them as exit ramps (0 instead of -1).
	* Speed limits are all very close.
	* Vehicle occupancy is very close to correct.
	* Normally, vehicle speeds are all zeros, as if it just ignored this layer completely, at least
	  in the tiny dataset. It looks like all neighbor vehicles in the single episode represented by
	  this dataset are all moving at negative relative speeds to the host vehicle.
	* On record 178372 it reflected the speeds of the single vehicle accurately.
	* On record 40971 all neighbor speeds were accurate, and the varying speed limits were good.
	* On record 190964 it performed well on all 4 neighbors. Only non-pavement is wrong.
**	* I wonder if the activation function in the autoencoder is preventing it from generating 
	  large negative numbers, e.g. for the pavement layer. Need to find some counter examples in
	  the neighbor speed layer. Yes! I realize the output layer on the autoencoder is using a
	  sigmoid activation, which definitely prevents any negative values in the output.
**	* Good test cases for future reference:
		* 35923 - host is in lane 4 with 2 neighbors at greatly varying speeds.
		* 65532 - host is very close to end of exit ramp (apparently lane 4) and no neighbors.
		* 40971 - includes new lane starting and another lane dropping, differing speed limits
			  across lanes and along lane length, plus 4 neighbors.
		* 254992 - includes pavement & speed limit change at front edge of grid, plus 4
			  neighbors with negative relative speeds.
		* 260976 - 3 neighbors, one with rel speed ~0.0; also speed limit changes in 2 different
			  lanes
		* 182945 - 5 neighbors with large negative relative speeds.
		* 267115 - 2 neighbors with very large positive rel speeds.
		* 42709 - 6 neighbors, one with rel speed ~0, two bumper-to-bumper, varying negative
			   rel speeds.

* Run E5 training the embedding for 200 epochs. batch = 256, LR = 0.0001, embedding dim = 200. Run with
  commit 54abfba on the vector-embed branch.
	* Changed the Autoencoder layer 2 to have tanh activation.
	* Output file is embedding_200_231123-2236.pt
	* Training loss started at 0.01025 and dropped fairly steadily to 0.00024 in epoch 199.
	* Test loss started at 0.00228 and dropped fairly steadily to 0.00024 in epoch 197. These are
	  much lower losses than seen in runs E1 or E2.
	* Eval shows that the negative values generally work well now. Pavement types are all reasonable
	  but there is a little more uncertainty in the values than before (+/- 0.04 maybe), and a bit
	  more smoothing in the longitudinal transitions.
	* Ditto comment for the speed limits.
	* Occupation layer is quite accurate, but the neighbor speed layer is a bit mushy, with 
	  non-trivial speeds spread out over several zones longitudinally, but their values definitely
	  peak at the proper location with approximately correct values, and drop off sharply outside of
	  those zones. Also, rel speeds with small magnitudes tend to be less accurately represented, or
	  overlooked completely (|val| < 0.08 or so).

11/24/23

* Run E6 training the embedding for 200 epochs with embedding dim = 100, batch = 256, LR = 0.0001.
	* Output file is embedding_100_231124-0008.pt
	* Training loss started at 0.01573 and dropped to 0.00133.
	* Test loss started at 0.00537 and dropped to 0.00131 at epoch 198. Several epochs reversed
	  the trend, getting up to 0.5% above the current min, but then a new min was set after that.
	* Eval shows a lot more fluctuation in the outputs. 
	  Speed limit errors are not bad, and shouldn't be a concern now. But actual neighbor speeds
	  are hit or miss on their accuracy. When they appear, they are spread out over several cells
	  with an error up to 0.08 or 0.10 at times. 
	* Since I will only be using the encoder layer, it is not possible to look at these results
	  and consider procedural filters on the output.

* Run E7 training the embedding for 400 epochs with embedding dim = 100, batch = 256, LR = 0.00005.
	* Slowing it down a little and allowing it to run longer may help.
	* Output file is embedding_100_231124-1031.pt
	* Training loss similar to run E6 through first 200 epochs, then stayed pretty steady at
	  0.001306.
	* Test loss similar to run E6 through first 200, but dropped a little more to 0.001284 at
	  epoch 398.
	* Eval shows this performed slightly worse than E6, in that rel speeds were often way off,
	  and several times it completely overlooked negative values.

* Run E8 training the embedding for 300 epochs with embedding dim = 100, batch = 256, LR = 0.00005.
	* Changed the Autoencoder's first layer activation from relu to tanh in hopes that might
	  generate more accuracy for the negative speeds.
	* Output file is embedding_100_231124-1307.pt
	* Training loss ended at 0.00126 and test loss ended at 0.00124, which was the min.
	* All 4 of the negative rel speeds in test case 182945 were overlooked again.
	* In general, it seems to represent rel speeds a lot closer to zero than they should be,
	  on either size of zero. In one case, it even showed two ghost neighbors that weren't
	  there.

* I don't think that building a custom loss function is practical, as the loss Tensor has a
  built-in loss function that is related to the loss class being used.

* Run E9 training the embedding for 400 epochs with embedding dim = 64, batch = 256, LR = 0.00005.
	* Output is embedding_64_231124-1916.pt.
	* Training loss ended at 0.00290, test loss hit min of 0.00284 on ep 398.
	* As expected, it did not perform well at all on the rel speed layer; in a couple cases,
	  even the speed limit layer was kinda far off.

11/25/23

* Reworked the embed training program to only handle two "layers" of data at a time (of the 4
  elements in each sensor zone). Because of the unfortunate flat structure of the original obs
  vector, doing this required a lot of cpu-intensive reshaping to pull out individual layers.
	* This layer separation slows things down a lot.  Performance tests:
		* with 1, 2, 4 or 8 workers, batch = 16, I get ~27 sec/1000 batches, or 1.69e-3 sec/record.
		* with 0 workers, batch = 16, gives 26.3 sec/1000 batches, or 1.65e-3 sec/record.
		* with 4 or 8 workers, batch = 64, I get 102 sec/1000 batches, or 1.59e-3 sec/record.
		* with 0 workers, batch = 128, gives 215 sec/1000 batches, or 1.68e-3 sec/record

11/26/23

* Run E10 training the embedding for layers 0-1 (pavement) for 200 epochs with embedding dim = 64,
  batch = 256, LR = 0.0001. Corresponds to vector-embed commit 7dd6b7d.
	* Fixed a defect in handling the final partial batch.
	* Sped up the batch reshaping code a bit, but found that it is still 100x of the time that the
	  entire rest of the loop is taking.  Searched for ways to speed it up more, but I think the only
	  way is to refactor the env to spit out a multi-dim tensor, and even that is challenging because
	  there are 25 scalar elements that are not part of the sensor grid. So I'll just put up with the long
	  training time for now.
	* Output file is embedding_64p_231126-0910.pt. The 'p' after dim value indicates that it is trained
	  on pavement data.
	* Job completed after 190 epochs due to early stopping criterion; test loss jumped by 7.8% above min.
	  Unfortunately, it saved the model at the very end, not the one with the lowest loss.
	* Training loss upon stopping was 0.000120, test loss was 0.000129 (previous min was 0.000119).
	* Evaluation on my above test cases all looked very good. There is a bit of smoothing where values
	  change longitudinally, but minimal.

* Run E11 training the embedding for layers 2-3 (vehicles) for 200 epochs with embedding dim = 64,
  batch = 256, LR = 0.0001.
	* Fixed a defect in reshaping for the higher layers giving a layer index out-of-bounds.
	* Output file is embedding_64v_231126-1300.pt.
	* Training ran the full 200 epochs. Final training loss = 0.00331, test loss = 0.00324. There was
	  a segfault apparently after the pt file was written. It looks like the same size as the previous
	  one, so maybe it's okay.
	* Eval shows performance on par with the pre-splitting of layers, which is not satisfactory. It
	  consistently overlooks rel speeds that are far from 0. It does a lot of smoothing on occupancy,
	  which may be acceptable to some extent. But in one case it registered a vehicle's presence with
	  a value of 0.12, where any others show at least 0.72.

* Run E12 training the embedding for vehicle layers again. Want to see if it's possible to keep the
  embeding dim = 64, so trying with batch = 256, LR = 1e-5, 500 epochs.
	* Output file is embedding_64v_231126-1823.pt.
	* Test loss hit its min of 0.003324 on epoch 496. At epoch 200 it was 0.00335, so basically didn't
	  learn anything useful beyond that point. Training loss ended at 0.003394.
	* Eval runs showed expected performance as poor as earlier 64 models. Not acceptable.

11/27/23

* Run E13 training the embedding for vehicle layers with embedding dim = 100, batch = 256, LR = 1e-4 for
  200 epochs.
	* Output file is embedding_100v_231127-2012.pt.
	* Test loss reached 0.001157 at epoch 197, which is only slightly better than its 0.001166 at
	  epoch 100. Train loss ended at 0.001170.
	* Evaluation of a few test cases shows this didn't significantly improve performance over the 64
	  neuron model. It still routinely ignores large magnitude rel speeds, or even represents them
	  with a small value of the opposite sign.

11/28/23

* Run E14 training the embedding for vehicle layers with dim = 150, batch = 256, LR = 1e-4 for 140 epochs.
	* Output file is embedding_150v_231128-0921.pt
	* Test loss ended at 0.000306 and train loss at 0.000310, which is ~4x better than E13.
	* Eval shows much expected improvement over previous models. Neighbor locations are spot on. Speeds
	  are usually pretty close, but all are estimated a little closer to zero than reality. There is
	  also occasional noise values up to magnitude 0.07 where no vehicles exist.
	* Overall, this is acceptable for now. I don't think I'd want to reduce its size any more and take
	  any worse performance.

* Run E15 training the embedding for pavement layers with dim = 30, batch = 256, LR = 1e-4 for 100 epochs.
  Building on success in run E10 on 11/26, trying to squeeze the size down as far as possible while still
  performing decently.
	* Output file is embedding_30p_231128-1655.pt.
	* Test loss ended at 0.000399 with training loss = 0.000399.
	* Evaluation shows that this reproduces the input data quite well, with a minimal amount of
	  smoothing.
	* It might be possible to squeeze it down to 25 neurons or so, but that doesn't feel like much of
	  a gain in the big picture, so I'll say this is acceptable as it to go forward with.

***
* The embedding models that will be going forward in the full Bridgit model will be:
	* Pavement: embedding_30p_231128-1655_99.pt
	* Vehicles: embedding_150v_231128-0921_139.pt


11/29/23

* Began building code to integrate the embeddings into the Bridgit NN model.

12/2/23

* Created new branch "reshape" off of branch "vector-embed". This will house work to reshape the
  ObsVec sensor data structure into layers, which is critical for performance of the new
  algorithm.

12/7/23

* Completed refactoring the code for the sensor data reshaping. Passed the updated test_bridgit.

* Made a new run of embed_collect for 10k episodes or 2M steps.

NEXT: 
	Finish writing BridgitNN





TODO:
- consider rescaling obs element so that they all use the full range of [-1, 1] (e.g. speed limit).
- Convert Train checkpoints to msgpack format (see RLlib_notes.txt)
- Convert Stopper class to a Checkpointing class to manage the number of checkpoints, and also the reporting of results based on running averages.
- Make bot ACC more aggressive.
- Build BridgitCtrl to read a checkpoint & execute it (preferably without Ray)
- find a way to load just the NN weights in inference (or in a vehicle controller) so that it can be run
  only with torch, no ray involved.
- Try running inference with real checkpoint - modify the action loop to use BridgitCtrl instead of loading directly?

- Investigate speed overshoot when speed limit changes (with bot1a controller)
- Add lane IDs to graphics
- Add vehicle icons to graphics
- Review all TODOs
- Add ego sensor display to graphics showing pvmt type/speed colors, vehicle speeds

- For adding messaging, look into RLlib's nested_action_spaces example.
- For adding messaging, look at https://arxiv.org/pdf/2008.02616.pdf and https://github.com/proroklab/adversarial_comms
  about adversarial MARL for cooperative & adversarial comms among agents.
