This is John's personal developer notes for the CDA1 project.  Probably pretty boring reading
for anyone not intently helping out.

Big picture of this project's goals:
	1.1 - train a single agent to drive the new track to suitable destinations with several bots attempting to do the same thing.
	1.2 - expand the training to include several instances of the agent (replacing some bots) as pseudo-multi-agent training (iterative).
	1.3 - true multi-agent training, with several untrained agents (same policy) learning simultaneously using Ray multi-agent facilities.
	1.4 - multi-agent training for two policies in a single vehicle, one for planning and one for control.


8/21/23

* Project setup & Github repo creation.
* Completed first version of rqmts spec.

8/22/23

* Got legacy cda0 code running after breaking up all the classes in the main environment file
  so they are now in their own files.

8/23/23

* Rewrote the Roadway class to represent the new road geometry.

8/24/23

* Fixed some things in inference prgm and got it to display the new roadway.

8/27/23

* Began revamping the environment model to be more generalized with roadway definition and vehicle population.

8/28-30/23

* Continued building new environment model.

8/31/23

* Built env model to the point that it executes one iteration in inference (prior to checkpoint loading).

9/1/23

* Tried running inference with the final cda0 checkpoint file. It couldn't load because it was looking for
  the old class structure (simple_highway_ramp_wrapper), which is apparently baked into the checkpoint.

* Tried running the tune program - had to make several tweaks & fixes.
	* Got it running in the tuner.  Now to put in some real logic to get the NN to start learning.

9/2/23

* Updated inference prgm to work with the new vehicle structure and with or without a checkpoint (moving
  neighbor vehicles only).

9/3/23

* Fixed problems contributing to graphical display of vehicle locations.
* Ran inference on scenarios 90-95 with Bot1a controller to confirm geometry & speed limits all work.
* Developed & tested Bot1b controller to give offset to speed limit. 
* Ran a sample training job - everything looks nominal.  Ready to build the real learning controller.

9/4/23

* Wrote the Bridgit vehicle model with its huge observation vector encoding. Still need to test it.

9/5/23

* Wrote testing program for BrigitModel and ran them test suite. Found & fixed several defects.
* Tried running a training session, but got exception - too tired to investigate.

9/6/23

* Debugged problems with training. Found & fixed 2 defects.

9/7/23

* Started a training run to verify training will work.
	* Training hangs after 0-2 iterations. Log advances every 5 sec with no changes to the
	  info shown, and no print stmts from any of my code (ran with debug = 1). No data is
	  going to the tensorboard.
	* Started a thread on Ray Discuss at https://discuss.ray.io/t/tune-hangs-soon-after-starting/12081
	  to get help.

9/8/23

* Found defect in environment reset() method that induced an infinite loop occasionally, when trying to 
  place all of the neighbor vehicles within a close space. Fix allows training to move forward.
* Fixed defect in obs collection when neighbor was barely inside the front edge of the grid.

* Training run d226a using SAC on 2 trials. Reward function is scaled by 0.1 from what was used in
  cda0, so the reward range is now -1.5 for a crash, but otherwise within [-1, 1].
	* No good. rmax stayed < -1 for 3.5M steps (12k iterations).

9/16/23

* Run ? training with SAC
	* To accommodate changed reward scale, I added initial_alpha as a tuning HP.
	* Increased range of noise magnitude to [0.1, 0.4]
	* Added noise scale timesteps as a tunable HP to stretch it out more.
	* Extended trial duration from 12k iterations to 30k since it will probably take a lot longer for the agent
	  to experience all this track has to offer.
	* First 2 trials didn't do anything; rmax <= -1.
		* I discovered a defect in reading configs, such that it was not ignoring neighbor crashes, thus 
		  even if the agent was performing well, the episode may end with a poor reward.

* Run 5e343 training with SAC
	* Fixed config defects for setting boolean flags, correctly setting ignore_neighbor_crashes = True.
	* Changed max_iters back to 18k, since 30k took > 8 hr for two trials.
	* Adjusted reward penalty for speed deviation - multiplier from 0.03 to 0.015, since one inference example
	  (of prev run) showed that agent successfully stayed behind a bot at 14 m/s and got severely punished for
	  that, worse than if it had crashed.
	* Added a staging area from which to execute the training sessions, and a train.sh script to set it up and
	  initiate each run. This allows code editing & testing in the source area while running, without need to
	  make a second copy of the repo.
	* Inference on early trial (1) shows
		* Lots of crashes right after starting - it seems vehicles are packed too tightly together given the
		  wide range of start speeds.
	* Found a defect in VehicleModel, where it needs to have previous time step's obs passed in so that historical
	  reference info can be saved.

* Run a2caf training with SAC
	* Fixed defect that omitted proper updates of common obs vector elements.
	* Increased safe separation for initial vehicle placement from 4 to 5 car lengths.
	* Increased initial speed assignments from [0, MAX_SPEED] to [5, MAX_SPEED].
	* Added _decide_num_vehicles() to allow fewer vehicles to be present, especially during early episodes.
	* Inference on trial 0 near its peak (rmax ~-0.05): never changes lanes, LC cmds stay very close to 0; it
	  settles into speeds ~23.6 m/s in open field; it is common to see same-lane crashes between ego and a bot
	  either before or after it, due to close spacing and widely differing initial speeds.
	* Mean episode lenghts max out ~70 (target is 80).
	* It seems that training for only a fragment of the route gives no motivation for a lane change, esp when the
	  agent gets punished for doing so. Found that episode is not getting a reward for completing the allotted
	  steps, because it is not considered "done".

9/17/23

* Run 9010f training with SAC
	* Changed step() to set both the done and truncated flags when it hits the max allowed steps. This will allow
	  the completion reward to be counted for completing an episode, even if it doesn't hit the target.
	* Limited the speed of a neighbor vehicle placed close behind the ego vehicle so that it won't immediately
	  rear-end ego.
	* Expanded safe distance from 5 to 6 car lengths for placing vehicles in same lane near other vehicles.
	* Adjusted noise tuning params.
	* After 4 trials completed, the run died of a segfault after ~10k iterations in trials 4 & 5.
	* Before it died, trial 5 achieved rmax ~0.94 and rmean noisy in [0.4, 0.6] at 3M steps. Inference on it shows
	  it likes a steady speed of 21.91 m/s. This is similar to several other trials, settling at a lower-than
	  limit speed. I wonder if this is due to training in a crowd, where it learns that these are the safe speeds
	  to avoid a crash.

* Mods ready for next run:
	* Fixed small defect in TargetDestination that was incorrectly initializing the lane ID, so targets weren't
	  recognized. Also added indicator in reward function to give a bonus if target is reached.
	* Adjusted ACC control in BotType1Ctrl to start activating farther away (from 8 to 12 car lenghts) in hopes
	  of reducing its forward collisions.
	* Made some small improvements to inference usability.

9/18/23

* Began designing the capability to do simple route planning for controllers.

9/21/23

* Implemented the tree construction code for lane feeder connectivity.

9/22/23

* Designed & implemented lane change control logic for the BotType1bCtrl. Still has some problems.

9/23/23

* Tested Bottype1Ctrl algorithm. Made several changes to minimize crashes. A big factor is two bots that are in
  adjacent lanes but far apart longitudinally, with greatly differing speeds, then the rear one changes lanes and
  ACC cannot slow it down fast enough to avoid a rear-end. Another is two vehicles 2 lanes apart that both want to
  move into the lane between, and sensors have no way of knowing this will happen because intentions aren't
  broadcast.
	* LC decision is based on a one-time snapshot that only looks at 9 sensor zones in the adjacent lane. Once
	  the decision is made to start moving, a neighbor in that lane can quickly move into the zones, and the
	  host vehicle can't react (by design, there is no emergency abort maneuver).
	* I increased the maneuverability of these bots by making max accel = 4.0 m/s^2 and LC duration only 2.4 s.
	* Had to beef up the discrimination in the _check_for_collision() method to handle the possibility of two
	  vehicles separated by 2 lanes simultaneously trying to move into the common lane between.

9/24/23

* Began implementing the route planning for the Bridgit controller.

9/25/23

* Completed coding for the Bridgit controller. Dealing with test problems.

9/26/23

* Completed testing of the Bridget controller route planning method. It now adds desired probabilities of lane
  change to left, center or right on each plan execution, and these are put into the obs vector so that the
  controller NN will see them.
* Enhanced reset() provide scenarios 1 & 2 for all bots in ego's lane and for no bots starting in ego's lane,
  respectively. Also, if scenario 0 is specified in a training run, then it will sometimes sets it to 1 or 2 instead.

* Began training run 5d5d4 with SAC on this major new upgrade. This uses code in Git training branch, commit 8b23668.
	* All trials ended quickly with errors due to divide by zero in plan_route().

* Run ce52f training with SAC
	* Put in a quick band-aid and some diagnostics for the previous run's errors.
	* 7 of 8 trials errored out quickly. They were all SIGSEGVs.
	* Trial 1 seems to have run to completion, but only did ~1.3M steps (18k iterations).
	* The whole thing ran for 6.7 hr.

9/27/23

* Fixed defect where I had forgotten to retain the planning output values of lane change desirability in the obs
  vector, in BridgitModel.
* Cleaned up several minor TODOs.

* Run 84bc8 training with SAC for 40k iterations on 4 trials
	* Trial 1 errored out very early, due to failure to place a vehicle during reset.
	* Trials 0 & 2 ran for ~29k iterations (close to 9M steps) before I had to shut them down due to traveling.
	  Both of them showed rmax > 0.9 steadily after ~3.5M steps. Trial 0 rmean bounced a lot in [0, 0.4] and
	  trial 2 rmean was a little steadier in [0.4, 0.6] after 3M steps.
	* Distribution of episode_vehicles looks good across the progression of the trial.
	* Didn't see many logs of scenario adjustment - seems to be only for trial 1. I don't understand this.
	* Inference shows a fatal flaw: no lane changes. Even though it now has obs inputs to indicate the desired
	  lane change activity, there is no reward signal telling it whether the choice was good!

9/29/23

* Added reward bonus for following the planner's lane recommendation; removed the constant part of the penalty for
  lane changes (still gets hit for frequent maneuvers, but not just for doing the maneuver itself); reset the
  completion reward for tgt reached to 1.0 so that it is congruent with any episode completion and doesn't bias
  the learning toward the specific route geometry; added log details for crash situations.

* Run b557e training with SAC for 40k iterations on 6 trials
	* Computer froze up when trials 2 & 3 were ~32k iterations.
	* Trials 0 & 1 errored out early where reset() couldn't place a neighbor vehicle in lane 5.

* Run 83dda training with SAC for 40k iterations on 6 trials
	* Modified reset() so that if it can't find a safe location for a neighbor vehicle, it deactivates it.
	* Changed reward multiplier on LC behavior from 0.003 to 0.004.
	* Fixed a defect with scenario resetting that forced all episodes to run scenario 2 after a short time.
	* Killed after ~32k iterations on trials 0 & 1 due to travel needs.
	* Inference on a recent checkpoint shows that the agent like to drive ~32 m/s without slowing to avoid a
	  rear-end crash. It also did not demonstrate a lane change in a couple quick episodes where it should
	  have. Hoping it just needs more experience.

9/30/23

* Added capability for the tune program to restart an in-progress job from the checkpoint dir. It doesn't work,
  with Ray throwing an assertion error. Stored the checkpoint in ray_results/cda1-no-restore for future
  investigation.

* Run a120d training with SAC for 40k iterations
	* Tweaked distro of scenarios 0, 1 & 2.
	* System locked up after running for 9:28, the first two trials achieving ~26k iterations (~8M steps),
	  providing productivity of 5500 iters/hr. This is with obs verification off.
	* Trial 0's reward plot was somewhat jagged, but by 8M steps, rmax ~1.2 and rmean in [0.7, 0.8]. It
	  had noise magnitude of 0.12 decaying over 8M steps. Trial 1's reward was much smoother but less
	  effective. Its rmax climbed quickly to 1.2 by 3M steps, then stayed there, while rmean hit 0.6 then
	  noisily drifted in [0.1, 0.7]. Both had rmin < -1.0 the entire time.
	* Inference on trial 0 shows agent quickly moves to a speed of 17.6 m/s and does not change lanes.
	* Inference on trial 1 shows agent quickly moves to 31.9 m/s and does not change lanes. Also, it
	  rear-ended a slightly slower bot without attempting to slow.
	* Found a defect in the reward bonus for LC planning.

10/3/23

* Spent a good deal of time debugging WARNINGs received occasionally from get_reward(), where it detects no
  desirable lanes specified by the Bridgit planner. I cannot determine what is causing this, as it only appears
  during training, not in inference (so far), and training with parallel workers produces non-sequential log
  content.

* Run 9ef56 training with SAC for 40k iterations
	* Changed noise scale_timesteps to choose either 8M or 12M steps (was 4M or 8M).
	* Narrowed the tuning range of noise stddev to [0.1, 0.4].
	* Fixed the LC plan-following reward bonus.
	* Increased the multiplier on the plan-following reward bonus (to 0.006), since inference showed that
	  it was overshadowed by the speed penalty.
	* Improved reward code to not penalize for speed deviation if ego is going slower than speed limit because
	  it is following a slow forward vehicle.
	* Trial 1 seems to have started well, reaching rmax ~1.3 by 4M steps and rmean reaching 0.8 in a chaotic
	  way by 6M steps.
	* Trial 0 errored after 3.5M steps, with an assertion error in the ACC logic.
	* The whole job stopped way early due to a segfault.
	* Reviewed the warnings from get_reward() about not having a desirable lane. I only saw occurrences where
	  it was a lost cause because there is no physical way to reach a target from the current location, so
	  this is okay. This was only in inference runs, however.
	* It is still pretty easy for the LC desired bonus to be swamped by a speed penalty, for even moderately
	  poor speeds.

10/4/23

* Run 7476e training with SAC for 40k iterations
	* Enhanced _verify_safe_location() to use a carefully calculated amount of space in the same lane, rather
	  than the previously used wild guess. The change is a lot more conservative.
	* Modified rewards so that: LC desired bonus mult increased from 0.006 to 0.008; lane change penalty mult
	  was decreased from 0.0005 to 0.0001; LC command penaly was eliminated (allowing frequent LCs); increased
	  speed penalty mult from 0.015 to 0.018.
	* All trials died before 600k steps, all with an unknown system error.
	* Still seeing plenty of warnings about all desirability lanes with 0 probability.

* Ran a tune run with a single worker thread in order to debug the training situation with desirability lanes.
	* Added debugging statements.
	* Changed Ray resources to no GPU and only 1 cpu to help find problems.
	* Found several cases where an illegal lane change zeros out the obs vector of ego, since the dynamics
	  method marks it as inactive. Added a guard for this in step().
	* It seems this fixes the problems. There are times when the desirability vector will be all zeros,
	  which is right after a reset, or if the vehicle is on an exit ramp with no hope of reaching one of
	  its targets. All other conditions now produce reasonable desirabilities.

* Run b96d8 training with SAC for 40k iterations. Corresponds to git commit 0bbd952 on the train branch.
	* Includes a few fixes for handling of lane desirabilities.
	* Program halted due to segfault when first two trials were ~32k iterations in (9M steps).
	* Trial 1 reached rmax ~1.4 steadily since 5M steps, and rmean peaked at 0.4 briefly before the end.
	  Inference on it shows that it loves a steady speed of 32.1 m/s, even though it resulted in rear-
	  ending another vehicle. It never demonstrated a lane change.
		* Its initial alpha = 0.2, noise stddev = 0.47 decaying over 12M steps.
	* Trial 0 rmax reached 1.4 a couple times, earlier than trial 1, but was very jagged. Its rmean was
	  equally chaotic, reaching 0.4 after 5.5M steps, but then fluctuating in [-1, 0.2] afterwards. I
	  ran inference on a checkpoint from the peak rmean, which performed about like trial 1, but
	  preferring a speed of 27.7 m/s. It ignored speed limits, and also refused to change lanes.
		* Its initial alpha = 0.002, noise stddev = 0.34 decaying over 12M steps.
	* I suspect that this consistent lack of ability to adapt to situations implies that the NN is too
	  simple.
	* I saved the checkpoints in the project's training dir for future reference, named 256-256.
	* Training performance on this run was 3550 iters/hr, with verify_obs turned on and 0 cpus_per_worker.

10/5/23

* Run 33541 training with SAC for 40k iterations - with larger NN. Corresponds to git commit a0eb2cb on training
  branch.
	* Increased NN structure to 3 layers, consisting of 600, 256, 128 neurons each (since there are
	  559 inputs) for both the policy and Q networks.
	* Removed tuning from the initial alpha, fixing it at 0.2.
	* Changed num_cpus_per_worker from 0 to 4 (I had used 2 for a long time) and num_cpus_for_local_worker
	  from 2 to 4.
	* Turned off verify_obs.
	* Set rollout_fragment_length = 80, since this is the episode length; set train_batch_size = 1040
	  to be a multiple of this.
	* All 6 trials died very quickly due to system fault. I suspect it was an OOM failure.

* Run a52ad training with SAC for 40k iterations, with larger NN. This is a re-do of previous run 33541, but...
	* Reduced num_cpus_per_worker to 2 and num_cpus_for_local_worker to 2.
	* All trials errored before 700k steps. All were unknown system faults (OOM?). I verified that the
	  resource configs were the same as on 10/3, before I started playing with them.

10/6/23

* Performed system update, per system notes, due to inaccessible GPU.

* Run ec5a7 training with SAC for 40k iterations, with larger NN...re-re-do.
	* Set num_rollout_workers = 1 (was previously unspecified), to help limit memory usage.
	* Trials 3, 4 & 5 all died due to system fault (possible OOM). 3 made it to 1M steps, but 4 & 5
	  stopped before 400k steps.
	* Trials 0 & 1 ran to completion, ~8M steps, which is the first time I've seen any trial do that.
	* Trial 0 (noise stddev 0.23 over 12M steps) reached rmax > 1 quickly, but occasionally dropped well
	  below that a few times. After 5M steps, rmean fluctuated noisily ~0.8 and stayed there. rmin never
	  consistently exceeded -1.3. Mean episode length was ~78 after 5M steps.
		* Inference shows agent prefers speed 18.2 m/s, although it stayed close to the ramp 0 speed
		  limit initially. It never changed lanes.
	* Trial 1 (noise stddev 0.45 over 12M steps) showed a steady climb in rmax from 0.5 to 1.4 over the
	  8M steps. rmean stayed low until 4M steps, then gradually curved upward, crossing 0 at 7.5M steps
	  and ending > 0.4 at 8.1M steps, with every indication that it would have continued to climb. rmin
	  stayed ~-1.4. Mean episode length gradually curved upward too, ending at 70.
		* Inference shows a preference for 22.9 m/s, but otherwise behaves same as trail 0. When it
		  runs on a target lane, it always succeeds, and gets a fat reward.
	* Trial 2 proceeded will to about 3.5M steps, reaching rmean = 0.8 by then, but the job died without
	  any error message at that time. It had noise stddev = 0.24 over 8M steps.
	* This run performed at approx 5400 iters/hr, running two trials simultaneously, but seems to have
	  had some memory problems.

10/7/23

* Run 17e90 training with SAC over 40k iterations on 1 trial
	* Set episode completion reward to 0 (was 1) to force agent to pay more attention to the lane change
	  opportunities and speed control opportunities.
	* Fixed the noise magnitude at 0.25 over 12M steps (not tunable).
	* Trial completed. rmax was very noisy, bouncing rapidly in [0.1, 1.2], while rmean slowly fluctuated
	  in [-0.3, -0.1] after 4M steps, but mean episode duration was ~77 after 5M steps.
		* Inference shows no consideration of lane change, and a desire for speed of 14.8 m/s.
	* Half-way through the run, performance was showing 2400 iters/hr.

10/8/23

* Run a5c6d training with SAC over 40k iterations on 1 trial
	* Changed reset() to spread out initial P location of ego vehicle closer to end of its lane (up to
	  150 m from the end in all case. For early episodes (< 10k) it also skews the choice of location toward
	  the lanes without targets, and to P coords near the end of whatever lane is chosen. Al this will help
	  expose it to failure scenarios.
	* Changed BridigitCtrl.plan_route() to scale outputs by max_prob instead of by sum_prob, thus magnifying
	  the difference between larger and smaller ones, because the largest value is always 1. Also changed
	  its calculation of desirable probability to be dependent only on the remaining distance in the ending
	  lane, not on its length relative to the target lane.
	* Changed num_gpus from 0.5 to 1 to give more compute to the single trial worker.
	* Training performance was 2710 iters/hr.
	* Judging by shape of reward curves, the first 10k episodes, which dictate more difficult initial conditions,
	  were completed after only ~300k steps (avg 30 steps/iter).
	* Inference was no good. It sought a constant speed, well below nominal speed limit (values 17-19 m/s,
	  depending on which checkpoint was used), and it never attempted to change lanes.

10/9/23

* Run 424f1 training with SAC on 40k iterations on 1 trial. Corresponds to train branch commit 80a32e7.
	* Changed the early episode threshold in reset() from 10k to 100k episodes, which should correspond to
	  closer to 4M steps.
	* Improved BridgitCtrl.plan_route() so that it won't recommend a lane change unless it is legal for all of
	  the following planning cycle (approx; it looks ahead 6 sensor zones).
	* Removed penalty for widely varying speed commands (just to keep it out of the way for now).
	* Increased reward speed_mult from 0.018 to 0.030, since this penalty was getting washed out by LC bonus.
	* Changed noise magnitude from 0.25 to 0.35.
	* Reward curves don't look as good as prev run. The "early episodes" lasted ~2.2M steps, after which rmax
	  dropped a lot. It then peaked at 0.5 ~6M steps, then dropped to 0 and only recoverd to 0.1 afterward.
	  rmean peaked at -0.2 @ 7M steps, then dropped.
	* Performance was 2790 iters/hr.
	* Inference at ~22k and 37k iterations shows it seeks steady speed of 16.4 m/s and does not change lanes.
		* I notice that LC desired bonus is being collected generously (full value) when it cannot change
		  lanes (e.g. early part of lane 0), so it is getting rewarded for non-action. Also, it is collecting
		  this full reward on the exit ramps in the suicide phase, which reinforces simply staying in the
		  same lane always.
		* I'm concerned that the number of obs inputs from the sensors is overwhelming the handful of inputs
		  that are most important (speed diff, lane change commands).

* Run fe196 training with SAC over 40k iterations on 1 trial
	* Change reward bonus for LC desirability to be awarded only if there is a choice other than current lane to
	  be made (possibly current lane is one of the choices).
	* Increased reward penalty speed_mult from 0.3 to 0.5 to further emphasize speed discrepancies.
	* Increased episode length from 80 to 160 steps to make it more difficult to complete without a lane change,
	  and slightly more chance of including a target destination.
	* Increased noise magnitude from 0.35 to 0.4.
	* Changed num_cpus_per_worker and num_cpus_for_local_worker from 2 to 4 to improve performance.
	* No good. After the first 2.2M steps (early episode treatment), rmax dropped to -1, then after 7M steps it
	  slowly fluctuated in [-1.2, -0.2] for the remainder of the 14M steps. Trial was stopped early (at iter
	  25863) due to poor performance. However, mean episode length did reach ~150 after 6M steps.
	* Performance was 2600 iters/hr.
	* Inference confirms the poor actions I've seen in previous runs.

10/10/23

* Several quick experiment runs with differing noise levels, looking at very early checkpoints. I added logging to show
  the input commands to every step() call. They seem to be pretty well distributed in [-1, 1], although somewhat skewed
  to negative values. It appears, however that the noise makes up virtually all of the magnitude of these commands.
  When running an early checkpoint (from iters 10 to 200) in inference mode, the NN output from every step is within a
  hair of 0, regardless of the situation. With almost no significant learning accomplished by this point, I would exped
  the outputs to look a lot more random.

* Run c4dd8 training with SAC over 40k iterasions - back to multiple trials
	* Switched to 4 trials, since I can train 2 in the same time I can train 1 by allocating resources differently.
	  Back to num_gpus = 0.5, num_cpus_per_worker = 2, num_cpus_for_local_worker = 2, and num_rollout_workers
	  undefined.
	* Increased episode length to 500 steps, which should allow most episodes to run the full track. Changed 
	  rollout_fragement_length to 500 also, to align with a full episode in each fragment.
	* Changed noise magnitude to 0.5, based on experiments earlier today, to help expose it to LC commands with
	  magnitude > 0.5 (a lane change).
	* No good. After the initial episode treatment (2M steps), rmax dropped to -1 and stayed close to that until
	  5M steps, where I killed the job.

* Run e8679 training with SAC over 40k iterations on multiple trials.
	* Turned off Gaussian noise, at Kevin's suggestion.
	* Added exception handling to step() and to BridgitModel.get_obs_vector() to better understand a strange error
	  tripped in the Roadway model.
	* Trial 0 rmean gradually dropped to -11, recovered a bit, then dropped to -12.
	* Trial 1 was stopped early because rmax fell dramatically after 4M steps.
	* Training performance 3200 iters/hr.

* Began studying RLlib code to figure out how to pre-populate a replay buffer (web site doesn't seem to have any info
  on how to do it).

10/11/23

* Run a58db training with SAC over 40k iterations on multiple trials
	* Turned on Gaussian noise at stddev = 0.25.
	* Added explicit replay buffer config to the tuner program. Specified MultiAgentPrioritizedReplayBuffer (which
	  seems to be the default, capacity of 1M (which was the default), and turned on prioritized_replay (which had
	  been off).
	* Program died due to segfault with no explanation, other than a problem in the logger json encoder.
	* Training performance 2800 iters/hr.
	* Trial 0 looked pretty unstable, with rmax slowly fluctuating in [-1, 3] and rmean in [-4, -0.5].
	* Trial 1 was stopped early due to terrible rmax.
	* Trial 2 showed some promise, although rather unstable, with rmax ending ~4 and rmean ~-0.5 when it was cut
	  off.

10/12/23

* Run 95004 training with SAC over 40k iterations on multiple trials. Corresponds to train branch commit 02fdd2ab.
	* Added Xavier normal initialization of all model weights (tested first) in CdaCallbacks, per Kevin's suggestion.
	* Reduced num early episodes from 100k to 20k, since this large number seems to breed a lot of instability.
	* Reset the episode length back to 80 steps.
	* Modified reward bonus for LC desirability to only apply if a LC command was issued.
	* Changed step() to interpret cmd[1] differently:  the boundaries between STAY_IN_LANE and either of the lane
	  changes is now +/- 0.2 instead of 0.5. I hope this will encourage more exploration of lane change actions
	  without needing so much noise to push it into those regions.
	* All trials died very early, possible OOM error.

* Run 57115 training with SAC over 40k iterations on multiple trials.  Re-do of above (95004) after reboot.
	* However, changed num early episodes to 0, as it seems to be generating a lot of craziness.
	* Computer hung in the middle of the first 2 trials, which each completed ~23k iterations (12M steps).
	* Trial 0 seemed headed for an early stop due to poor rmax.
	* Trial 1 held a bit of promise, with rmax ~0.4 at the end, and rmin heading upward from -0.7. It had actor LR
	  = 4.0e-6, critic LR = 1.6e-5, entropy LR = 1.0e-6. Trial 0's LRs were larger on all 3.
	* Analyzed bot placement problems (where it couldn't place all selected). All cases occurred when the ego p value
	  was at the very beginning of its chosen lane. However, many cases should be easy to solve. It seems the algo
	  is too restrictive in the allowed radius.
	* Training performance was 3130 iters/hr.

10/13/23

* Playing with callback for initializing the replay buffer.
	* Found a major defect in my code - the env wrapper was not passing through most of the observations in scale_obs()!
	  Fixed it.
	* The on_postprocess_trajectory() callback is invoked after a trajectory has been completed, and includes all of the
	  (s, a, r, s', a') contents, plus the dones/terminateds, etc, for each step.  Therefore, this is not the place to
	  inject random actions, since it already represents the r, s', etc, of the actions that are included.
	* It seems the place to inject random actions is at the beginning of the environment step(), where it can then
	  compute the r, s', etc, from those actions. That won't work, however, as those new action values won't get
	  captured by RLlib anywhere.
	* Tried implementing the on_episode_step() callback, but it runs after the environment step() method returns.
	* More investigation...still not finding where in RLlib the training loop is that my code uses.

* Run 8b8ce training with SAC over 40k iterations.
	* Fixed tune program to set rollout_fragment_length = 80 to match the episode length, and train_batch_size = 1040.
	* Added a new CdaCallback method, on_postprocess_trajectory(). It's a no-op for now.
	* Modified reset() to relax the bot vehicle placement region a bit to minimize the number that have to be
	  deactivated.
	* Includes fix to HighwayEnvWrapper to scale all observations rather than zeroing out most of them!
	* First two trials ran to completion (7.5M steps), but trials 2 & 3 stopped very early with an unknow problem (OOM?).
	* Trial 0 reached rmax ~3.5 at 2.5M steps, then stayed close to that. rmean peaked ~0.7 at that time, then
	  slowly & noisily drifted down to 0.2. Mean episode length was only 35-40.
		* Inference on checkpoint 13280 (~2.8M steps) shows the agent starting in lane 1 frequently changing lanes
		  and speeds, fluctuating them back and forth, but completed the route! Speed was well below speed limit,
		  but it was making bank on LC cmd rewards, so didn't need to worry about speed penalties.
		* Several times it made illegal lane changes.
	* Trial 1 reached rmax of 3.5 in only 0.5M steps, then stayed close to that. Its rmean hit 0.6 at a similar time,
	  drifted noisily down to 0.3, then back up to 0.6. Both trials had rmin < -1.2, and dropped rapidly after 4M steps.
	  Mean episode length was similar to trial 0.
	* Training took 17.6 hr with a productivity of 4840 iters/hr, also 911 steps/hr, which is probably a more
	  transferrable measurement, since it doesn't depend on batch size.

10/14/23

* Run b24d6 training with SAC for 40k iterations. Corresponds to train branch commit 44edb60.
	* Changed reward bonus for LC des from 0.1 to 0.01.
	* Uncommented reward penalties for varying speed commands and varying LC commands.
	* Altered the top of step() to change the LC cmd threshold back to 0.5, as it had always been.
	* Run cut short by a segfault elsewhere in the system. Trials 0 & 1 had completed ~29k iters (5M steps), and were
	  both showing good promise, with still a long way to go. 
	* Both trials had rmax gradually climbed from -0.5 to -0.1 while rmean gradually climbed to -0.7 and rmin was
	  climbing steadily as well. Episode durations were > 70 at the end.
	* Inference on trial 1 (the better one at the end) shows improved desire to stay somewhat close to speed limit
	  with lower frequency speed changes. It also has much fewer lane changes. It appears on lanes 0 & 4 that it is
	  somewhat attempting to match the speed limits, although reaction is pretty slow.

10/15/23

* Run 5dac3 training with SAC for 50k iterations. Corresponds to train branch commit 62e3976.
	* Adjusted rewards so that lane command penalty is reduced from 0.01 to 0.002, since it was wiping out the LC
	  desired bonus being awarded for just a few time steps.
	* Fixed defect in obs for steps since lane change that wasn't getting initialized correctly. It will affect
	  penalties for the first lane change maneuver.
	* Increased multiplier for lane change penalty from 0.0001 to 0.0002.
	* Increased multiplier for speed command penalty from 0.04 to 0.1 to encourage more smoothness.
	* I killed it early, after ~27k iters, since I found some fixes/improvements that I felt were essential
	  (see description of the next run).
	* Trial 0 rmax was climbing slowly, reaching -0.3 after 5M steps. Trial 1 (with smaller LRs) was jaggedly
	  moving downward, with recent rmax varying slowly in [-1, -0.5].

* Added Argparse to the inference program and got it ready to take in filename for a NN weights file for the Bridgit
  controller.

* Run 13930 training with SAC for 50k iterations.
	* Fixed defect in BridgitModel counting steps since lane change, which led to too much penalty for a lane
	  change.
	* Increased reward bonus for lane change desirability from 0.01 to 0.02.
	* Narrowed the tuning range for LR on critic and entropy from [1e-6, 1e-4] to [5e-6, 1e-4].
	* Run ended early with segfault. Seems to be in the json encoder (saving plots?). Both trials 0 & 1 were
	  near iter 44k (8M steps).
	* Both trials performed similarly, with rmax climing slowly but pretty steadily to -0.1 at the end. rmean
	  was on a steady climb, ending at -0.90. rmin was quite noisy, but still climbing also. All 3 look like
	  they would have kept climbing. Mean episode length hit upper 70s after ~5M steps.
	* Inference on trial 1 shows it performs lane changes occasionally, but often it seems random, not for a
	  beneficial purpose. This might be corrected with more training. Also, speed control is still pretty
	  jittery and earns quite a bit of penalty for speeds too slow. It doesn't seem to understand speed limit
	  changes.
	* Performance was 5030 iters/hr, 890k steps/hr.

10/16/23

* Attempted to test the necessary package installation for this project with the requirements.txt file, but could
  not get pip to work. Then I couldn't get conda to create a new environment (to start a fresh install of python
  & pip). Giving up - too frustrated.

* Run ac2f2 training with SAC over 80k iterations.
	* Increased max iterations from 50k to 80k.
	* Increased reward bonus from 0.02 to 0.04 for LC desirability matching, and added a negative bonus if it
	  chooses a LC direction associated with a very small desirability.
	* Run died due to segfault when trials 0 & 1 had completed ~48k iters.
	* Trial 1 had rmax climbing steadily throughout, reaching 1.5. Its rmean climbed steadily too, until 
	  ~7.5M steps, then leveled out at ~0. rmin also climbed steadily until ~6M steps, and leveled at ~-1.7.
	  This trial had actor LR = 6.8e-5, critic LR = 1.6e-5, entropy LR = 3.2e-5.
		* Inference on lane 0 shows agent has much better speed control than previous runs, but still
		  fluctuates like a sine wave. It also changed lanes frequently, went to lane 1 before the exit
		  ramp split off, then selected lane 2/1/2 a couple times. Avoided some close call crashes!
		* Starting in lanes 1-3 looked similar, with lots of lane changes, but speed profile attempted to
		  match the speed limit change near the end.
		* It appears that it can't decide between lanes 1 & 2 (both have targets), so it frequently
		  goes back and forth between them. Needs more incentive to stay in one lane if it works.
		* Tested every starting lane. At no point did the agent run off road or crash into a neighbor.
	* Trial 0 looks less good, with rmax peaking at 0.6, then reducing to ~0. rmean climbed steadily, then
	  slowed after 4M steps, but continued climbing, ending at -0.4. Its rmin ended slightly better than
	  that for trial 1.
	* Training performance was 5090 iters/hr or 891k time steps/hr (took 19 hr).

10/17/23

* Run 35b28 training with SAC for 80k iterations. Corresponds to train branch commit 0f3ad47.
	* Added logic to reward bonus for LC desirability to only award the bonus for the first 2 time steps
	  that the command is given, at the beginning of a maneuver, which prevents the agent from scooping up
	  extra reward with no effect (leaving the command on for the whole maneuver). Since the bonus will be
	  awarded much less now, its value is increased from 0.04 to 0.1. Also changed the bonus decision to
	  be only if the cmd desirability > current lane desirability (it was >=).
	* Increased LC cmd penalty (for cmd frequency) from 0.0002 to 0.001.
	* Doubled the penalty for varying speed commands (multiplier from 0.1 to 0.2).
	* Changed tuning range for learning rates so that all 3 are now in [1e-5, 1e-4].
	* Trial 1 ended early due to code defect that allowed an illegal round-off error P value to be used.
	* Run ended early on a SIGSEGV that again appears to be in the checkpoint storing code of Ray. At
	  end, trial 0 completed 54k iters and trial 2 completed 25k iters.
	* Trial 0 rmax crossed 0 at 7M steps and almost leveled out, reaching ~0.05 before dying just after
	  9M steps. rmean climbed pretty steadily, although slowed after 7M steps, but ended at its peak of
	  -0.4, while rmin mostly leveled after 7M at -1.0.
		* Inference: did not change lanes out of lane 0 or 5. Overall, lane changes are way down; only
		  a couple extraneous one in several runs. Saw one off-road. Speed profile is still somewhat
		  jagged, but follows speed limit fairly well. Made one poor LC choice from 4 to 5. Overall,
		  it seems it needs more reward for desired lane changes, and generally a bit more training.

10/18/23

* More investigation into how to pre-populate the experience buffer.

* Run 07eb6 training with SAC on 80k iterations. Corresponds to train branch commit 5355739.
	* Changed tuner to store checkpoints only every 50 iters (was 10), and not storing checkpoint at end.
	  This should minimize the chances of a SIGSEGV killing everything.
	* Fixed RoadwayB.param_to_map_frame() to ensure slightly small P values are not going to raise an
	  exception.
	* Reward penalty for deviating from speed limit increased multiplier from 0.05 to 0.08.
	* Reward bonus for desirable lane changes changed from 0.1 to 0.4, since they are so rare.
	* Run stopped prematurely due to segfault (SIGSEGV apparently in JSON logger). Both trials had
	  reached ~51k iterations (9M steps).
	* Trial 0 rmax climbed a little unsteadily to ~1.2, rmean climbed fairly steadily past -0.2, and
	  looks like it would continue. rmin climbed unsteadily to ~-2.0. Mean episode length came close to
	  80 at 6M steps, then backed off a bit to ~75 for the duration.
	* Trial 1 rmax was a steady, slow climb to ~1.2. rmean climbed more aggressively, then slowed a bit
	  but contined to climb to 0, and looks like it would continue. rmin climbed a bit noisily to -1.5
	  at 6M steps, then almost leveled off, ending ~-1.3. Its mean episode length hit ~57 at 3M steps,
	  then continued to struggle for more gains, but finally leveled off ~69.
		* Inference: speed profiles a bit smoother, but the speed command penalties are very small,
		  and probably not having a noticeable impact. Several extraneous lane changes, but fewer
		  than seen in previous runs, and most were done safely. Start in lane 2 ended with legal
		  lane change to 1 into a bot vehicle. Lane 3 start did off-road immediately (one time).
		* I think it is trying to follow speed limits pretty well, but can't avoid speed noise.
		  Speed penalties are killing it. Probably need more penalty for speed fluctuations and
		  less on absolute speed deviations.
		* Refuses to change lanes out of 0, but easily merges from lanes 4 and 5.

10/19/23

* Run 6a61d training with SAC for 80k iterations. Corresponds to train branch commit 3b0b4e5.
	* LC desirability bonus - doubled this from 0.4 to 0.8, and made the poor-choice penalty -0.8.
	* Doubled the penalty for lane change (based on time since previous). Mult is now 0.002.
	* Increased speed command penalty mult from 0.2 to 1.0 to help smooth out the profile.
	* Reduced penalty for speed deviation - multi from 0.08 to 0.04.
	* Run died unexpectedly after 43k iterations (8M steps) due to GCS rpc timeout. gcs_server.out
	  does not seem to contain any info that would explain the problem.
	* Both trials had rmax climbing steadily, ending > 3. rmean similary, ending ~0.5 (trial 1 was
	  a little higher). rmin also steady climb, ending at -2. Oddly, mean episode lengths took a
	  dive. Trial 0 peaked at 70 ~4M steps, then steadily declined to 45. Trial 1 peaked ~55 at
	  2M steps, then dropped quickly to ~40 at 4M steps, then stayed there.
		* Trial 0 had actor LR = 5.8e-5, critic LR = 4.9e-5, entropy LR = 2.8e-5. Trial 1
		  values were slightly higher on all.
		* Trial 0 inference using its latest checkpoint (poor episode length). Lots of
		  extraneous lane changes, and wildly varying speeds (almost to the extremes several
		  times). Trial 0 inference on its earliest checkpoint (88% training complete, so
		  still on the downward slope of mean episode length): performed similarly, but speed
		  variations weren't quite as pronounced. It seems it experienced some catastrophic
		  forgetting along the way.
		* Trial 1 inference on its latest checkpoint showed similar behavior to trial 0, but
		  this one had much more fondness for extremely slow speeds.
	* Training performance
		* I am changing the way I look at this, looking at the average for a single trial, so
		  it will be easier to compare to different numbers of active trials. Usually, only
		  one trial will be useful anyway.
		* Avg 2490 iter/hr/trial over a 17.4 hr run (447 steps/hr).

10/20/23

* It has gotten to the point that I feel the need to reboot the computer just before starting every
  run to limit the chances of some sort of a crash. It seems many of these could be OS problems, or
  Ray memory mgmt problems, which build a history in the OS between runs.

* Run ce230 training with SAC for 80k iterations. Corresponds to train commit 7a01a05.
	* Increased speed deviation penalty somewhat, from 0.04 to 0.06 to modulate this penalty from
	  previous run.
	* Dialed back the speed command penalty mult from 1.0 to 0.5 to modulate from previous run.
	* Increased lane change penalty by adding a constant 0.002 regardless of time elapsed since
	  the previous maneuver. It seems most maneuvers have been waiting 60 steps or more, but still
	  doing them for no particular reason. This should slow that down.
	* I killed it after 35k iters, as the mean episode length was going way down again, on both
	  trials (one was < 40 steps). It is apparent the agent prefers suicide over a long, painful
	  episode.
	* A few quick inference runs on one of the trials (5M steps) shows wildly fluctuating speeds,
	  a preference for slow speeds, and several irrational lane changes. Some of this may be due
	  to insufficient training, however.

10/21/23

* Run 582ee training with SAC for 80k iterations. Corresponds to train branch commit b4d2e92.
	* Added episode completion bonus of 1.0 to encourage longevity.
	* Realized that the extraneous lane changes were its way of setting up an unnecessary number
	  of extra rewarding lane changes later (since the poor ones often don't get a penalty).
	  Added penalty for any lane change that goes against the desirability factor. Also, dialed
	  back the magnitude of this bonus/penalty so that it doesn't overwhelm the completion bonus,
	  and allows speed penalties to be more visible.
	* Died from SIGSEGV, again apparently during checkpointing ops, after ~14 hr.
	* Trial 0 had all reward curves moving steadily upward, and looking like they would go further
	  with enough training. rmax reached 0.8, rmean reached -0.1 and rmin ~-1.5. Mean episode
	  length ~80.
		* Inference: it performs most necessary lane changes, but not always (esp from lane 0)
		  and occasionally does an unnecessary one, but that is rare. The big problem is
		  speed likes to stay down around 23 m/s, regardless of the local speed limit. It
		  seems it is not seeing this in the obs.
	* Trial 1 showed slightly higher reward curves, but its episode length was more jittery and
	  hadn't yet exceeded 70.
	* Training performance: 2440 iter/hr/trial, 491 steps/hr.

10/29/23

* Tested observations for proper representation of speed limit in each forward sensor zone. It is
  getting all of these correct.
	* It seems maybe the best bet now is to train for longer. Need to get lucky with pickling
	  errors, however.


BEFORE NEXT RUN:
- test installing & running (in the tmp/cda1_dup dir)
- Investigate ways to pre-populate experience buffer




TODO:
- Make bot ACC more aggressive.
- Upon part 1 success, try it again with: 256, 256 NN, LC cmd threshold at 0.5, no Gaussian noise.
- Kevin suggestion #1: pre-populate experience buffer w/steps that have random actions.
- Kevin suggestion #2: pre-train an encoder to abstract the sensor data lieu of a CNN, to reduce its size to the obs.
- Build BridgitCtrl to read a checkpoint & execute it (preferably without Ray)
- find a way to load just the NN weights in inference (or in a vehicle controller) so that it can be run
  only with torch, no ray involved.
- Try running inference with real checkpoint - modify the action loop to use BridgitCtrl instead of loading directly?

- Fix requirements.txt to either be pip compatible (to use pip install -r requirements.txt) or to be a conda import compatible.
- Investigate speed overshoot when speed limit changes (with bot1a controller)
- Add lane IDs to graphics
- Add vehicle icons to graphics
- Review all TODOs
- Add ego sensor display to graphics showing pvmt type/speed colors, vehicle speeds

- For adding messaging, look into RLlib's nested_action_spaces example.
- For adding messaging, look at https://arxiv.org/pdf/2008.02616.pdf and https://github.com/proroklab/adversarial_comms
  about adversarial MARL for cooperative & adversarial comms among agents.
