This is John's personal developer notes for the CDA1 project.  Probably pretty boring reading
for anyone not intently helping out.

8/21/23

* Project setup & Github repo creation.
* Completed first version of rqmts spec.

8/22/23

* Got legacy cda0 code running after breaking up all the classes in the main environment file
  so they are now in their own files.

8/23/23

* Rewrote the Roadway class to represent the new road geometry.

8/24/23

* Fixed some things in inference prgm and got it to display the new roadway.

8/27/23

* Began revamping the environment model to be more generalized with roadway definition and vehicle population.

8/28-30/23

* Continued building new environment model.

8/31/23

* Built env model to the point that it executes one iteration in inference (prior to checkpoint loading).

9/1/23

* Tried running inference with the final cda0 checkpoint file. It couldn't load because it was looking for
  the old class structure (simple_highway_ramp_wrapper), which is apparently baked into the checkpoint.

* Tried running the tune program - had to make several tweaks & fixes.
	* Got it running in the tuner.  Now to put in some real logic to get the NN to start learning.

9/2/23

* Updated inference prgm to work with the new vehicle structure and with or without a checkpoint (moving
  neighbor vehicles only).

9/3/23

* Fixed problems contributing to graphical display of vehicle locations.
* Ran inference on scenarios 90-95 with Bot1a controller to confirm geometry & speed limits all work.
* Developed & tested Bot1b controller to give offset to speed limit. 
* Ran a sample training job - everything looks nominal.  Ready to build the real learning controller.

9/4/23

* Wrote the Bridgit vehicle model with its huge observation vector encoding. Still need to test it.

9/5/23

* Wrote testing program for BrigitModel and ran them test suite. Found & fixed several defects.
* Tried running a training session, but got exception - too tired to investigate.

9/6/23

* Debugged problems with training. Found & fixed 2 defects.

9/7/23

* Started a training run to verify training will work.
	* Training hangs after 0-2 iterations. Log advances every 5 sec with no changes to the
	  info shown, and no print stmts from any of my code (ran with debug = 1). No data is
	  going to the tensorboard.
	* Started a thread on Ray Discuss at https://discuss.ray.io/t/tune-hangs-soon-after-starting/12081
	  to get help.

9/8/23

* Found defect in environment reset() method that induced an infinite loop occasionally, when trying to 
  place all of the neighbor vehicles within a close space. Fix allows training to move forward.
* Fixed defect in obs collection when neighbor was barely inside the front edge of the grid.

* Training run d226a using SAC on 2 trials. Reward function is scaled by 0.1 from what was used in
  cda0, so the reward range is now -1.5 for a crash, but otherwise within [-1, 1].
	* No good. rmax stayed < -1 for 3.5M steps (12k iterations).

9/16/23

* Run ? training with SAC
	* To accommodate changed reward scale, I added initial_alpha as a tuning HP.
	* Increased range of noise magnitude to [0.1, 0.4]
	* Added noise scale timesteps as a tunable HP to stretch it out more.
	* Extended trial duration from 12k iterations to 30k since it will probably take a lot longer for the agent
	  to experience all this track has to offer.
	* First 2 trials didn't do anything; rmax <= -1.
		* I discovered a defect in reading configs, such that it was not ignoring neighbor crashes, thus 
		  even if the agent was performing well, the episode may end with a poor reward.

* Run 5e343 training with SAC
	* Fixed config defects for setting boolean flags, correctly setting ignore_neighbor_crashes = True.
	* Changed max_iters back to 18k, since 30k took > 8 hr for two trials.
	* Adjusted reward penalty for speed deviation - multiplier from 0.03 to 0.015, since one inference example
	  (of prev run) showed that agent successfully stayed behind a bot at 14 m/s and got severely punished for
	  that, worse than if it had crashed.
	* Added a staging area from which to execute the training sessions, and a train.sh script to set it up and
	  initiate each run. This allows code editing & testing in the source area while running, without need to
	  make a second copy of the repo.
	* Inference on early trial (1) shows
		* Lots of crashes right after starting - it seems vehicles are packed too tightly together given the
		  wide range of start speeds.
	* Found a defect in VehicleModel, where it needs to have previous time step's obs passed in so that historical
	  reference info can be saved.

* Run a2caf training with SAC
	* Fixed defect that omitted proper updates of common obs vector elements.
	* Increased safe separation for initial vehicle placement from 4 to 5 car lengths.
	* Increased initial speed assignments from [0, MAX_SPEED] to [5, MAX_SPEED].
	* Added _decide_num_vehicles() to allow fewer vehicles to be present, especially during early episodes.
	* Inference on trial 0 near its peak (rmax ~-0.05): never changes lanes, LC cmds stay very close to 0; it
	  settles into speeds ~23.6 m/s in open field; it is common to see same-lane crashes between ego and a bot
	  either before or after it, due to close spacing and widely differing initial speeds.
	* Mean episode lenghts max out ~70 (target is 80).
	* It seems that training for only a fragment of the route gives no motivation for a lane change, esp when the
	  agent gets punished for doing so. Found that episode is not getting a reward for completing the allotted
	  steps, because it is not considered "done".

9/17/23

* Run 9010f training with SAC
	* Changed step() to set both the done and truncated flags when it hits the max allowed steps. This will allow
	  the completion reward to be counted for completing an episode, even if it doesn't hit the target.
	* Limited the speed of a neighbor vehicle placed close behind the ego vehicle so that it won't immediately
	  rear-end ego.
	* Expanded safe distance from 5 to 6 car lengths for placing vehicles in same lane near other vehicles.
	* Adjusted noise tuning params.
	* After 4 trials completed, the run died of a segfault after ~10k iterations in trials 4 & 5.
	* Before it died, trial 5 achieved rmax ~0.94 and rmean noisy in [0.4, 0.6] at 3M steps. Inference on it shows
	  it likes a steady speed of 21.91 m/s. This is similar to several other trials, settling at a lower-than
	  limit speed. I wonder if this is due to training in a crowd, where it learns that these are the safe speeds
	  to avoid a crash.

* Mods ready for next run:
	* Fixed small defect in TargetDestination that was incorrectly initializing the lane ID, so targets weren't
	  recognized. Also added indicator in reward function to give a bonus if target is reached.
	* Adjusted ACC control in BotType1Ctrl to start activating farther away (from 8 to 12 car lenghts) in hopes
	  of reducing its forward collisions.
	* Made some small improvements to inference usability.

9/18/23

* Began designing the capability to do simple route planning for controllers.

9/21/23

* Implemented the tree coonstruction code for lane feeder connectivity.


TODO:
- Create a route planning capability for controllers.
	- Debug 2 items shown in previous inference run:
		- which lane can't be reached and why?
		- need to append lists, not nest them. Should they be ordered?

- Add targets for the bot vehicles as well.
- Scan log for "num vehicles" to see how training patterns change
- Create a scenario with only neighbors in same lane and one with vehicles only in adjacent lanes.
- Consider limiting reward penalty for going slow behind a slow forward vehicle.
- Build BridgitCtrl to read a checkpoint & execute it (preferably without Ray)
- Try running inference with real checkpoint - modify the action loop to use BridgitCtrl instead of loading directly?
- Investigate speed overshoot when speed limit changes (with bot1a controller)
- Review all TODOs
- Fix requirements.txt to either be pip compatible (to use pip install -r requirements.txt) or to be a conda import compatible.
- add bot1 lane change capability when on a ramp.
- find a way to load just the NN weights in inference (or in a vehicle controller) so that it can be run
  only with torch, no ray involved.

- Add lane IDs to graphics
- Add vehicle icons to graphics
- Allow env ctor to be called with an initialization code. 
	- value lane_exercise -> cycles reset() through 6 episodes, and each episode places a single vehicle at the
	  beginning of each lane, in sequence, and lets it run the entire lane. Use this to verify the basic functions
	  of the env, roadway, and graphics all work correctly.
- Add ego sensor display to graphics showing pvmt type/speed colors, vehicle speeds
