This is John's personal developer notes for the CDA1 project.  Probably pretty boring reading
for anyone not intently helping out.

Big picture of this project's goals:
	1.1 - train a single agent to drive the new track to suitable destinations with several bots attempting to do the same thing.
	1.2 - expand the training to include several instances of the agent (replacing some bots) as pseudo-multi-agent training (iterative).
	1.3 - true multi-agent training, with several untrained agents (same policy) learning simultaneously using Ray multi-agent facilities.
	1.4 - multi-agent training for two policies in a single vehicle, one for planning and one for control.


8/21/23

* Project setup & Github repo creation.
* Completed first version of rqmts spec.

8/22/23

* Got legacy cda0 code running after breaking up all the classes in the main environment file
  so they are now in their own files.

8/23/23

* Rewrote the Roadway class to represent the new road geometry.

8/24/23

* Fixed some things in inference prgm and got it to display the new roadway.

8/27/23

* Began revamping the environment model to be more generalized with roadway definition and vehicle population.

8/28-30/23

* Continued building new environment model.

8/31/23

* Built env model to the point that it executes one iteration in inference (prior to checkpoint loading).

9/1/23

* Tried running inference with the final cda0 checkpoint file. It couldn't load because it was looking for
  the old class structure (simple_highway_ramp_wrapper), which is apparently baked into the checkpoint.

* Tried running the tune program - had to make several tweaks & fixes.
	* Got it running in the tuner.  Now to put in some real logic to get the NN to start learning.

9/2/23

* Updated inference prgm to work with the new vehicle structure and with or without a checkpoint (moving
  neighbor vehicles only).

9/3/23

* Fixed problems contributing to graphical display of vehicle locations.
* Ran inference on scenarios 90-95 with Bot1a controller to confirm geometry & speed limits all work.
* Developed & tested Bot1b controller to give offset to speed limit. 
* Ran a sample training job - everything looks nominal.  Ready to build the real learning controller.

9/4/23

* Wrote the Bridgit vehicle model with its huge observation vector encoding. Still need to test it.

9/5/23

* Wrote testing program for BrigitModel and ran them test suite. Found & fixed several defects.
* Tried running a training session, but got exception - too tired to investigate.

9/6/23

* Debugged problems with training. Found & fixed 2 defects.

9/7/23

* Started a training run to verify training will work.
	* Training hangs after 0-2 iterations. Log advances every 5 sec with no changes to the
	  info shown, and no print stmts from any of my code (ran with debug = 1). No data is
	  going to the tensorboard.
	* Started a thread on Ray Discuss at https://discuss.ray.io/t/tune-hangs-soon-after-starting/12081
	  to get help.

9/8/23

* Found defect in environment reset() method that induced an infinite loop occasionally, when trying to 
  place all of the neighbor vehicles within a close space. Fix allows training to move forward.
* Fixed defect in obs collection when neighbor was barely inside the front edge of the grid.

* Training run d226a using SAC on 2 trials. Reward function is scaled by 0.1 from what was used in
  cda0, so the reward range is now -1.5 for a crash, but otherwise within [-1, 1].
	* No good. rmax stayed < -1 for 3.5M steps (12k iterations).

9/16/23

* Run ? training with SAC
	* To accommodate changed reward scale, I added initial_alpha as a tuning HP.
	* Increased range of noise magnitude to [0.1, 0.4]
	* Added noise scale timesteps as a tunable HP to stretch it out more.
	* Extended trial duration from 12k iterations to 30k since it will probably take a lot longer for the agent
	  to experience all this track has to offer.
	* First 2 trials didn't do anything; rmax <= -1.
		* I discovered a defect in reading configs, such that it was not ignoring neighbor crashes, thus 
		  even if the agent was performing well, the episode may end with a poor reward.

* Run 5e343 training with SAC
	* Fixed config defects for setting boolean flags, correctly setting ignore_neighbor_crashes = True.
	* Changed max_iters back to 18k, since 30k took > 8 hr for two trials.
	* Adjusted reward penalty for speed deviation - multiplier from 0.03 to 0.015, since one inference example
	  (of prev run) showed that agent successfully stayed behind a bot at 14 m/s and got severely punished for
	  that, worse than if it had crashed.
	* Added a staging area from which to execute the training sessions, and a train.sh script to set it up and
	  initiate each run. This allows code editing & testing in the source area while running, without need to
	  make a second copy of the repo.
	* Inference on early trial (1) shows
		* Lots of crashes right after starting - it seems vehicles are packed too tightly together given the
		  wide range of start speeds.
	* Found a defect in VehicleModel, where it needs to have previous time step's obs passed in so that historical
	  reference info can be saved.

* Run a2caf training with SAC
	* Fixed defect that omitted proper updates of common obs vector elements.
	* Increased safe separation for initial vehicle placement from 4 to 5 car lengths.
	* Increased initial speed assignments from [0, MAX_SPEED] to [5, MAX_SPEED].
	* Added _decide_num_vehicles() to allow fewer vehicles to be present, especially during early episodes.
	* Inference on trial 0 near its peak (rmax ~-0.05): never changes lanes, LC cmds stay very close to 0; it
	  settles into speeds ~23.6 m/s in open field; it is common to see same-lane crashes between ego and a bot
	  either before or after it, due to close spacing and widely differing initial speeds.
	* Mean episode lenghts max out ~70 (target is 80).
	* It seems that training for only a fragment of the route gives no motivation for a lane change, esp when the
	  agent gets punished for doing so. Found that episode is not getting a reward for completing the allotted
	  steps, because it is not considered "done".

9/17/23

* Run 9010f training with SAC
	* Changed step() to set both the done and truncated flags when it hits the max allowed steps. This will allow
	  the completion reward to be counted for completing an episode, even if it doesn't hit the target.
	* Limited the speed of a neighbor vehicle placed close behind the ego vehicle so that it won't immediately
	  rear-end ego.
	* Expanded safe distance from 5 to 6 car lengths for placing vehicles in same lane near other vehicles.
	* Adjusted noise tuning params.
	* After 4 trials completed, the run died of a segfault after ~10k iterations in trials 4 & 5.
	* Before it died, trial 5 achieved rmax ~0.94 and rmean noisy in [0.4, 0.6] at 3M steps. Inference on it shows
	  it likes a steady speed of 21.91 m/s. This is similar to several other trials, settling at a lower-than
	  limit speed. I wonder if this is due to training in a crowd, where it learns that these are the safe speeds
	  to avoid a crash.

* Mods ready for next run:
	* Fixed small defect in TargetDestination that was incorrectly initializing the lane ID, so targets weren't
	  recognized. Also added indicator in reward function to give a bonus if target is reached.
	* Adjusted ACC control in BotType1Ctrl to start activating farther away (from 8 to 12 car lenghts) in hopes
	  of reducing its forward collisions.
	* Made some small improvements to inference usability.

9/18/23

* Began designing the capability to do simple route planning for controllers.

9/21/23

* Implemented the tree construction code for lane feeder connectivity.

9/22/23

* Designed & implemented lane change control logic for the BotType1bCtrl. Still has some problems.

9/23/23

* Tested Bottype1Ctrl algorithm. Made several changes to minimize crashes. A big factor is two bots that are in
  adjacent lanes but far apart longitudinally, with greatly differing speeds, then the rear one changes lanes and
  ACC cannot slow it down fast enough to avoid a rear-end. Another is two vehicles 2 lanes apart that both want to
  move into the lane between, and sensors have no way of knowing this will happen because intentions aren't
  broadcast.
	* LC decision is based on a one-time snapshot that only looks at 9 sensor zones in the adjacent lane. Once
	  the decision is made to start moving, a neighbor in that lane can quickly move into the zones, and the
	  host vehicle can't react (by design, there is no emergency abort maneuver).
	* I increased the maneuverability of these bots by making max accel = 4.0 m/s^2 and LC duration only 2.4 s.
	* Had to beef up the discrimination in the _check_for_collision() method to handle the possibility of two
	  vehicles separated by 2 lanes simultaneously trying to move into the common lane between.

9/24/23

* Began implementing the route planning for the Bridgit controller.

9/25/23

* Completed coding for the Bridgit controller. Dealing with test problems.

9/26/23

* Completed testing of the Bridget controller route planning method. It now adds desired probabilities of lane
  change to left, center or right on each plan execution, and these are put into the obs vector so that the
  controller NN will see them.
* Enhanced reset() provide scenarios 1 & 2 for all bots in ego's lane and for no bots starting in ego's lane,
  respectively. Also, if scenario 0 is specified in a training run, then it will sometimes sets it to 1 or 2 instead.

* Began training run 5d5d4 with SAC on this major new upgrade. This uses code in Git training branch, commit 8b23668.
	* All trials ended quickly with errors due to divide by zero in plan_route().

* Run ce52f training with SAC
	* Put in a quick band-aid and some diagnostics for the previous run's errors.
	* 7 of 8 trials errored out quickly. They were all SIGSEGVs.
	* Trial 1 seems to have run to completion, but only did ~1.3M steps (18k iterations).
	* The whole thing ran for 6.7 hr.

9/27/23

* Fixed defect where I had forgotten to retain the planning output values of lane change desirability in the obs
  vector, in BridgitModel.
* Cleaned up several minor TODOs.

* Run 84bc8 training with SAC for 40k iterations on 4 trials
	* Trial 1 errored out very early, due to failure to place a vehicle during reset.
	* Trials 0 & 2 ran for ~29k iterations (close to 9M steps) before I had to shut them down due to traveling.
	  Both of them showed rmax > 0.9 steadily after ~3.5M steps. Trial 0 rmean bounced a lot in [0, 0.4] and
	  trial 2 rmean was a little steadier in [0.4, 0.6] after 3M steps.
	* Distribution of episode_vehicles looks good across the progression of the trial.
	* Didn't see many logs of scenario adjustment - seems to be only for trial 1. I don't understand this.
	* Inference shows a fatal flaw: no lane changes. Even though it now has obs inputs to indicate the desired
	  lane change activity, there is no reward signal telling it whether the choice was good!

9/29/23

* Added reward bonus for following the planner's lane recommendation; removed the constant part of the penalty for
  lane changes (still gets hit for frequent maneuvers, but not just for doing the maneuver itself); reset the
  completion reward for tgt reached to 1.0 so that it is congruent with any episode completion and doesn't bias
  the learning toward the specific route geometry; added log details for crash situations.

* Run b557e training with SAC for 40k iterations on 6 trials
	* Computer froze up when trials 2 & 3 were ~32k iterations.
	* Trials 0 & 1 errored out early where reset() couldn't place a neighbor vehicle in lane 5.

* Run 83dda training with SAC for 40k iterations on 6 trials
	* Modified reset() so that if it can't find a safe location for a neighbor vehicle, it deactivates it.
	* Changed reward multiplier on LC behavior from 0.003 to 0.004.
	* Fixed a defect with scenario resetting that forced all episodes to run scenario 2 after a short time.
	* Killed after ~32k iterations on trials 0 & 1 due to travel needs.
	* Inference on a recent checkpoint shows that the agent like to drive ~32 m/s without slowing to avoid a
	  rear-end crash. It also did not demonstrate a lane change in a couple quick episodes where it should
	  have. Hoping it just needs more experience.

9/30/23

* Added capability for the tune program to restart an in-progress job from the checkpoint dir. It doesn't work,
  with Ray throwing an assertion error. Stored the checkpoint in ray_results/cda1-no-restore for future
  investigation.

* Run a120d training with SAC for 40k iterations
	* Tweaked distro of scenarios 0, 1 & 2.
	* System locked up after running for 9:28, the first two trials achieving ~26k iterations (~8M steps),
	  providing productivity of 5500 iters/hr. This is with obs verification off.
	* Trial 0's reward plot was somewhat jagged, but by 8M steps, rmax ~1.2 and rmean in [0.7, 0.8]. It
	  had noise magnitude of 0.12 decaying over 8M steps. Trial 1's reward was much smoother but less
	  effective. Its rmax climbed quickly to 1.2 by 3M steps, then stayed there, while rmean hit 0.6 then
	  noisily drifted in [0.1, 0.7]. Both had rmin < -1.0 the entire time.
	* Inference on trial 0 shows agent quickly moves to a speed of 17.6 m/s and does not change lanes.
	* Inference on trial 1 shows agent quickly moves to 31.9 m/s and does not change lanes. Also, it
	  rear-ended a slightly slower bot without attempting to slow.
	* Found a defect in the reward bonus for LC planning.

10/3/23

* Spent a good deal of time debugging WARNINGs received occasionally from get_reward(), where it detects no
  desirable lanes specified by the Bridgit planner. I cannot determine what is causing this, as it only appears
  during training, not in inference (so far), and training with parallel workers produces non-sequential log
  content.

* Run 9ef56 training with SAC for 40k iterations
	* Changed noise scale_timesteps to choose either 8M or 12M steps (was 4M or 8M).
	* Narrowed the tuning range of noise stddev to [0.1, 0.4].
	* Fixed the LC plan-following reward bonus.
	* Increased the multiplier on the plan-following reward bonus (to 0.006), since inference showed that
	  it was overshadowed by the speed penalty.
	* Improved reward code to not penalize for speed deviation if ego is going slower than speed limit because
	  it is following a slow forward vehicle.
	* Trial 1 seems to have started well, reaching rmax ~1.3 by 4M steps and rmean reaching 0.8 in a chaotic
	  way by 6M steps.
	* Trial 0 errored after 3.5M steps, with an assertion error in the ACC logic.
	* The whole job stopped way early due to a segfault.
	* Reviewed the warnings from get_reward() about not having a desirable lane. I only saw occurrences where
	  it was a lost cause because there is no physical way to reach a target from the current location, so
	  this is okay. This was only in inference runs, however.
	* It is still pretty easy for the LC desired bonus to be swamped by a speed penalty, for even moderately
	  poor speeds.

10/4/23

* Run 7476e training with SAC for 40k iterations
	* Enhanced _verify_safe_location() to use a carefully calculated amount of space in the same lane, rather
	  than the previously used wild guess. The change is a lot more conservative.
	* Modified rewards so that: LC desired bonus mult increased from 0.006 to 0.008; lane change penalty mult
	  was decreased from 0.0005 to 0.0001; LC command penaly was eliminated (allowing frequent LCs); increased
	  speed penalty mult from 0.015 to 0.018.
	* All trials died before 600k steps, all with an unknown system error.
	* Still seeing plenty of warnings about all desirability lanes with 0 probability.

* Run caa62 training with SAC for 40k iterations
	* Added debugging statements.



TODO:
- Build BridgitCtrl to read a checkpoint & execute it (preferably without Ray)
- Try running inference with real checkpoint - modify the action loop to use BridgitCtrl instead of loading directly?
- Investigate speed overshoot when speed limit changes (with bot1a controller)
- Fix requirements.txt to either be pip compatible (to use pip install -r requirements.txt) or to be a conda import compatible.
- find a way to load just the NN weights in inference (or in a vehicle controller) so that it can be run
  only with torch, no ray involved.

- Add lane IDs to graphics
- Add off-road icon
- Add vehicle icons to graphics
- Review all TODOs
- Add ego sensor display to graphics showing pvmt type/speed colors, vehicle speeds
