REQUIREMENTS FOR THE CDA1 SOFTWARE
----------------------------------

The software will train a single RL agent to drive a simple highway scenario in variable
amounts of traffic. This agent then can be loaded into any number of the simulated
vehicles such that each instance must cooperate with other instances of the same policy
and possibly with instances of one other policy (the non-RL "policy N", which will be a
very simplistic model of a human driving an ACC-equipped vehicle. Since this is developed
by a one-person team, the documentation can be very informal.


Training
--------
* Use the Ray platform, as that provides the most promising pathway to future RL work
  with more complex situations and more demanding needs for training resources.
* Train a policy that can drive the test track successfully from any random initial
  conditions, with variable amounts of traffic, at least 90% of the time.
	* An episode is considered successful for a given RL agent if it either reaches
	  a designated course end point or completes the specified number of time steps
	  without:
		* Crashing into any other vehicle
		* Driving off the designated roadway
		* Coming to a stop in the roadway
	* For the 90% success threshold, we measure this on a per-agent basis. So if
	  there are 5 instances of the agent driving in the same episode and four of
	  them complete successfully, that is an 80% success rate. However, an
	  evaulation will be done over many episodes, to better capture the policy's
	  capabilities across a range of conditions.
* Since real-life driving is a non-episodic task, training is to be done over arbitrary
  segments of highway, not end-to-end on the test track. Each segment (shorter than the
  test track) will define the limits of what will be seen as an "episode" by the
  training algo. This segmenting will allow future use of multiple training tracks, if
  desired.
	* Each training episode will be N time steps unless the agent reaches a
	  designated end point earlier.


Environment Model
-----------------
* Roadway
	* The roadway will include multiple lanes with a variety of connectivities and
	  speed limits. Any given lane may have a different speed limit than the lane
	  next to it (e.g. such as a restricted HOV lane), and any lane may change speed
	  limits at any longitudinal location. It is assumed that speed limit info is
	  communicated to the vehicles by some unnamed perfect communication mechanism.
	* The roadway will have two types of endpoints:
		* Type S - success endpoints for RL agents in training as well as policy
		  N agents.
		* Type N - possible targets for policy N agents that represent
		  background traffic. If an RL agent reaches one of these (e.g. the end
		  of an exit ramp), it is the same as running off the road, and
		  considered a failure.
* Vehicle "sensor" observations
	* The RL agent will be able to sense the physical characteristics of the nearby
	  roadway via a grid of observation zones surrounding the vehicle. This will be
	  a larger and more granular grid than used in CDA0.
	* Each sensor zone will indicate:
		* Existence of drivable pavement in that zone (it is fully covered by
		  a paved lane)
		* Whether it is occupied (by a crash-able object, which can only be a
		  vehicle in this project, but could be extended further to anything)
		* The velocity of the object in that zone, if one exists
		* For zones in the ego vehicle's current lane only, there will be
		  indicators of lane edge type (solid, permeable).
		* Posted speed limit
	* The agent will have self-knowledge, understanding its own speed and
	  acceleration and any other intrisic attributes needed.
	* Since we are abstracting out lateral control details, it can be assumed that
	  the vehicle somehow know about its current lane change status (i.e. if a
	  lane change is in progress).
	* It may be valuable to also store one or more historical values of any of the
	  observations.
	* It may also be valuable to store timers since a previous event.
* The observation space may also include configuration params, which could act like
  vehicle operator preferences, such as dialing in certain types of comfort or
  aggressiveness.
* RL agents don't have access to a map or other understanding of the overall roadway
  structure. The only things they know about the roadway are what is visible in its
  observation zones.
* Note that there is no mechanism for vehicles to use turn signals or communicate with
  each other in any way.

* Action space - we assume that there exists a low-level controller that can translate
  these commands into physical actuation that makes the vehicle obey them within its
  operating limits
	* Speed command (desired speed)
	* Lane change command (left, right, no change)

* Vehicle dynamics - these rules apply to all vehicle models, not just RL agents
	* Each vehicle model must specify realistic physical limits on longitudinal
	  acceleration capability and jerk (which probably represents a passenger
	  comfort level more than a hardware limitation). Not all vehicle models need
	  to be the same. Acceleration limits are the same for forward and rearward
	  directions.
	* Lane change maneuvers will always occur over a period of time, nominally
	  3 sec.

* World dynamics - everything outside of the vehicle
	* The world is flat. There is no elevation change.
	* There is perfect traction and the pavement type is uniform. There is no
	  noticeable wheel slip, skidding, or lateral acceleration limits (other than
	  a simulated comfort level by taking 3 sec for a lane change).
	* Weather is not a factor - sensors always work perfectly, and vehicle handling
	  is never a consideration.

* Simulation will run at a frequency of 5 Hz. While this should be configurable, there
  is a strong desire to keep it at least this high, to support future extensions.

* Behavior consequences
	* While there is minimal required reward structure for the agent in training,
	  there are physical consequences for certain behaviors.
	* Each vehicle model has a physical upper limit on speed of 36 m/s.
	* If a vehicle comes to a stop (or nearly so) in the roadway, it will be
	  terminated.
	* If a vehicle crashes into another vehicle, then both involved vehicles will
	  be terminated.
	* If a vehicle drives off the pavement, by going straight on a lane that is
	  ending, or by making an illegal lane change (to the grass or into a jersey
	  wall), then the vehicle will be terminated.
	* It is expected that all vehicles will make a "reasonable attempt" to drive
	  close to the speed limit. While this can't be physically enforced by the
	  environment (we don't have police pulling vehicles over for violations), the
	  environment model needs a reward structure that discourages driving either
	  slower or faster than the local speed limit at the vehicle's current
	  location.


Inference
---------
* An inference program will allow configuration and running of a single episode with
  all of the scenario variability used in training (e.g. number of vehicles, assignment
  of agent type to each vehicle, random starting locations, etc).
* The inference program will provide a simple graphical display of the track geometry
  and the vehicles as the drive on it. The type of vehicle will be visually obvious, by
  color, shape or other indicator.
* An inference episode will run until all vehicles reach a designated course end point.
  If one or more vehicles is terminated early (e.g. a crash), then the remaining
  vehicles will continue to drive until they reach an endpoint or are terminated.


Multi-agent Performance Evaluation
----------------------------------
* Episodes will be scored for success based on the following metrics, which are applied
  to the whole scenario, not to an individual participant.
	* Any crash or off-road event terminates the episode with a score of 0. For
	  simplicity here, both will be referred to as a crash.
	* If there is no crash, then each vehicle contributes to the episode score.
	  We capture the integral of its speed_disadvantage, which is target_speed
	  minus actual speed (limited to no less than 0, so that going faster can't
	  offset a slow spell). Since episodes will have differing numbers of vehicles,
	  and each vehicle will have a different path length to travel, the speed
	  disadvantage must be normallized. So the final episode score will be
		score = 1 - F*sum[ SDi/Li ]/N
	  where SDi is the speed disadvantage of vehicle i, Li is the length of
	  vehicle i's path in that episode, and N is the number of vehicles in the
	  episode. F is a constant multiplier to be determined, that scales the
	  results readily into [0.5, 1]. Therefore, an ideal episode would score 1
	  and the lower the score, the worse the traffic performance. We target F
	  so that an episode without a crash will very unlikely score worse than 0.5.
	  In this way, the negative impact of a crash is highlighted.
* In addition to episode scoring rules, we need a basis for making significant
  statistical comparisons over a wide variety of situations. Total randomization will
  not be appropriate, since one would expect a scenario with only one or two vehicles
  to demonstrate no congestion and no crashes, we can't learn much from them. The more
  vehicles, the more chance for problems to occur, and episode scores to go down.
  Therefore, to compare models, they must be on an equal footing, as such:
	* Scores will be compiled over a series of 120 episodes, looking at the mean
	  and standard deviation of scores across the suite, or within subsets.
	* Scenarios will be defined to offer some repeatability to the evaluation, by
	  defining the starting location of the ego vehicle (a BridgitGuidance model),
	  the roadway, the active target locations, and the numbers of each type of
	  neighbor vehicle. However, the exact placement of those neighbors will be
	  randomized (accepting the possibility that occasionally they won't all fit
	  so one or more may be left out of the episode).
	* Ego staring location will be specified by lane ID, with longitudinal location
	  exactly 100 m downtrack of the start of that lane. This allows room for
	  at least one neighbor to start behind it, and provides a uniform experience
	  for runs where the speed limit changes early in the lane.
	* Ego starting speed will be set to the speed limit at its starting location.
	* During evaluation, all Bridgit vehicles must be running the same guidance
	  model version, including the ego vehicle, which is always a Bridgit.
	* Roadway B episodes with targets 1, 2 active:

			      Numbers of neighbors
     Roadway   Targets Lane  Bridgit  Bot1a   Bot1b Episodes Scenario
     -------   ------- ----  -------  -----   ----- -------- --------
	B	1, 2	0	8	3	5	10	50
			1				10	51
			2				10	52
			3				10	53
			4				10	54
			5				10	55

	B	1, 2	0	10	5	10	3	56
			3				3	57
			5				3	58

	B	1, 2	0	16	0	0	3	59
			3				3	60
			5				3	61

	B	1, 2	0	4	1	1	3	62
			3				3	63
			5				3	64

	B	0	1	8	3	5	6	65
			5				6	66

	C	3	0	8	3	5	3	67
			3				3	68
		0	2				3	69
			5				3	70

	D	2	1	8	3	5	3	71
			3				3	72
			5				3	73
						    -------
						      120

* It will be acceptable for bots to set target speeds either above or below the
  posted speed limits, since we are comparing their performance against that target
  speed, and not against the speed limit, per se. Bridgit vehicles, however, will be
  judged on ability to match the speed limit, as that is their design goal.


Scenario Definitions
--------------------
* A variety of scenarios may be defined, as needed, that specify the initial setup
  of the vehicles, and possibly some limitations on their behavior. These scenarios
  apply to training as well as to inference runs, unless otherwise noted. The scenario
  IDs recognized are described here:

0 - Random initializations of all vehicles, within some ranges params. Occasionally
    chooses to run scenario 1 or 2, with its peculiar restrictions.

1 - Ego vehicle randomly initialized; all bot vehicles randomly placed in ego's lane.

2 - Ego vehicle randomly initialized; bot vehicles randomly placed in lanes other than
    ego's lane.

10-18 - Ego vehicle randomly initialized in the lane indicated by the last digit. Bot
	vehicles randomly initialized around it.

20-28 - Inference only. Like ego training scenarios 10-15, but vehicle 0 is not training.
	These were created for building the sensor embeddings, giving a very broad range
	of initial conditions, and intended to use the Bridgit model (observations) and
	the Embed guidance. However, with a different vehicle config file, these
	scenarios can easily perform inference on any combination of vehicle models.
	Vehicle 0 is initialized to the lane indicated by the last digit, but is otherwise
	treated like all of the other vehicles.

29    - embedding run like 20-25, but where vehicle 0's location is fully randomized.

=====================================================================================
=== The following are highly constrained special situations for testing purposes. ===

50-79 - Various conditions for traffic flow evaluations; see previous section.

80    - RoadwayC, ego starting in lane 2 at p = 550 m. Bots are randomly distributed.

81    - RoadwayD, ego starting in lane 2 at p = 2200 m and speed = 22.5 m/s.

90-98 - Runs a single bot vehicle (vehicle index 1) starting in the lane identified by
	the last digit. No other vehicles will be present. The bot will drive the given
	lane at its speed limit.
