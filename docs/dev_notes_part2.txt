John's notes on development of the CDA1 project for the Part 2 goal: multiple agents using the same policy
----------------------------------------------------------------------------------------------------------

12/30/23

* Rearranged the project directory to make it more navigable.

1/1/24

* Performed routine system upgrade, including patches to the OS and an update of Ray from 2.5.1 to 2.9.0.
	* Ray changed their checkpoint API in v2.7, so now my saved checkpoints cannot be restored for
	  inference. Also, training doesn't save new checkpoints the same way, so I had to investigate
	  the new approach.

1/4/24

* Continued searching for a workaround to problems using the new checkpoint API. It seems to force me to
  use Ray's TorchTrainer, and its .fit() method, similar to Tune. This then requires resource allocation
  (num workers cannot be left to a default).
	* Specifying num_workers > 1 forces it to request more hardware than is available. It seems to 
	  want to assume 1 CPU and 1 GPU for each worker, but it's not clear where that is being
	  specified. My legacy resource specs in the AlgorithmConfig seem to be partially recognized.

* Run C1 training single agent BridgetNN with SAC from scratch for 30k iterations using 16 bots.
	* This should be identical config to run B5, which was run on 12/27/23 in part 1 of the project,
	  and resulted in the publication of the part 1 resulting model (however, that model cannot
	  currently be run because of Ray's problems reading the checkpoint). So this run should provide
	  a substitute model in case recovery is not possible.
	* The training still uses Algorithm checkpoints (although they seem a bit altered, as nowhere is
	  the checkpoint version ID stored), so hopefully restarting training from one of these will pick
	  up the optimizer state, etc, and allow smooth continuation of training.
	* Training avg performance was ~2070 steps/hr, utilizing 1 cpu and ~5% of the GPU. Final rewards
	  were rmin ~-2, rmean ~-0.4, rmax ~1.6.
	* Unable to run inference due to missing checkpoint content. It is looking for a file named
	  algorithm_state.pkl or checkpoint-[0-9]+, and none exists. The only file in the checkpoint dir
	  is the model*.pt. So it appears I am unable to work with Algorithm checkpoints from here on out.

* While C1 was running I cleaned up a bunch of TODOs in the source code, all except the graphics.py.

1/5/24

* Testing reading of the new checkpoints
	* Inference cannot read them.
	* Training program also cannot read them for the same reason: key files are missing. Tried the
	  .pt file itself as well as the two levels of directories above that.
	* Tried using the structure suggested in the error message: `trainer = TorchTrainer.restore(ckpt).
	  It loaded the checkpoint okay (top level dir), but assumed that the job had errored out, and
	  tried to pick it up from where it started. Since it had completed its specified number of iters,
	  it would not go any further.

	* Reverted back to the baseline (part2) method, not using the Train API.
		* All checkpoints are getting written on top of each other (same name).
		* Added logic to train.py to name the top level dir according to timestamp and put
		  multiple checkpoints underneath, all named according to iteration #.
		* This works for writing new checkpoints, for starting a new training session from one,
		  and for running inference.
	* Verified that inference on the saved part 1 checkpoint still doesn't work. It complains about
	  'is-atari' property in SACConfig.
		* The easily visible diff between these checkpoints is that the part 1 dir includes two
		  hidden files, .is_checkpoint (0 length) and .tune_metadata in the top level dir.
		* I removed these two files, but the error still occurs. Therefore, it is probably due to
		  some content in one of the .pkl files.
		* Still waiting on someone to respond to my post on Ray Discuss about this problem, so I'll
		  leave it alone for now and try to regenerate the model with a new run.

NEXT: 	merge in TODO cleanups from the other branch, then start a training run.
	try to improve resource usage



TODO:
- Review TODOs in graphics.py
- consider adding dropouts to BridgitNN
- consider using imitation learning. Create a bot (1c) that changes lanes to pass a slow forward vehicle. Then
  either:
	a) Pre-train the agent with supervised learning. Requires episode capture (like embed_collect) to capture a db
	   with full obs vector, actions and rewards. Then run training (like embedding) on this db comparing either full
	   episodes, short trajectory segments or single time steps.
	b) Intersperse imitation episodes with normal episodes in SAC RL training. ~10% of episodes would use the 1c bot
	   to produce the expert actions for a given time step, then adjust the full ego state to that result and repeat.
- consider rescaling obs element so that they all use the full range of [-1, 1] (e.g. speed limit).
- consider adding coreographed scenarios to routine training that force certain close call situations.
- reduce prob of lane change in bots so that they don't jump across so quickly
- make Roadway an abstract interface, and allow multiple realizations of it to be chosen during training; add a 5-lane highwaay.
- Convert Train checkpoints to msgpack format (see RLlib_notes.txt)
- Convert Stopper class to a Checkpointing class to manage the number of checkpoints, and also the reporting of results based on running averages.
- Build BridgitCtrl to read a checkpoint & execute it (preferably without Ray)
- find a way to load just the NN weights in inference (or in a vehicle controller) so that it can be run
  only with torch, no ray involved.
- Try running inference with real checkpoint - modify the action loop to use BridgitCtrl instead of loading directly?

- Investigate speed overshoot when speed limit changes (with bot1a controller)
- Add lane IDs to graphics
- Add vehicle icons to graphics
- Review all TODOs
- Add ego sensor display to graphics showing pvmt type/speed colors, vehicle speeds
