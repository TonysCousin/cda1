John's notes on development of the CDA1 project for the Part 2 goal: multiple agents using the same policy
----------------------------------------------------------------------------------------------------------

12/30/23

* Rearranged the project directory to make it more navigable.

1/1/24

* Performed routine system upgrade, including patches to the OS and an update of Ray from 2.5.1 to 2.9.0.
	* Ray changed their checkpoint API in v2.7, so now my saved checkpoints cannot be restored for
	  inference. Also, training doesn't save new checkpoints the same way, so I had to investigate
	  the new approach.

1/4/24

* Continued searching for a workaround to problems using the new checkpoint API. It seems to force me to
  use Ray's TorchTrainer, and its .fit() method, similar to Tune. This then requires resource allocation
  (num workers cannot be left to a default).
	* Specifying num_workers > 1 forces it to request more hardware than is available. It seems to 
	  want to assume 1 CPU and 1 GPU for each worker, but it's not clear where that is being
	  specified. My legacy resource specs in the AlgorithmConfig seem to be partially recognized.

* Run C1 training single agent BridgetNN with SAC from scratch for 30k iterations using 16 bots.
	* This should be identical config to run B5, which was run on 12/27/23 in part 1 of the project,
	  and resulted in the publication of the part 1 resulting model (however, that model cannot
	  currently be run because of Ray's problems reading the checkpoint). So this run should provide
	  a substitute model in case recovery is not possible.
	* The training still uses Algorithm checkpoints (although they seem a bit altered, as nowhere is
	  the checkpoint version ID stored), so hopefully restarting training from one of these will pick
	  up the optimizer state, etc, and allow smooth continuation of training.
	* Training avg performance was ~2070 steps/hr, utilizing 1 cpu and ~5% of the GPU. Final rewards
	  were rmin ~-2, rmean ~-0.4, rmax ~1.6.
	* Unable to run inference due to missing checkpoint content. It is looking for a file named
	  algorithm_state.pkl or checkpoint-[0-9]+, and none exists. The only file in the checkpoint dir
	  is the model*.pt. So it appears I am unable to work with Algorithm checkpoints from here on out.

* While C1 was running I cleaned up a bunch of TODOs in the source code, all except the graphics.py.

1/5/24

* Testing reading of the new checkpoints
	* Inference cannot read them.
	* Training program also cannot read them for the same reason: key files are missing. Tried the
	  .pt file itself as well as the two levels of directories above that.
	* Tried using the structure suggested in the error message: `trainer = TorchTrainer.restore(ckpt).
	  It loaded the checkpoint okay (top level dir), but assumed that the job had errored out, and
	  tried to pick it up from where it started. Since it had completed its specified number of iters,
	  it would not go any further.

	* Reverted back to the baseline (part2) method, not using the Train API.
		* All checkpoints are getting written on top of each other (same name).
		* Added logic to train.py to name the top level dir according to timestamp and put
		  multiple checkpoints underneath, all named according to iteration #.
		* This works for writing new checkpoints, for starting a new training session from one,
		  and for running inference.
	* Verified that inference on the saved part 1 checkpoint still doesn't work. It complains about
	  'is-atari' property in SACConfig.
		* The easily visible diff between these checkpoints is that the part 1 dir includes two
		  hidden files, .is_checkpoint (0 length) and .tune_metadata in the top level dir.
		* I removed these two files, but the error still occurs. Therefore, it is probably due to
		  some content in one of the .pkl files.
		* Still waiting on someone to respond to my post on Ray Discuss about this problem, so I'll
		  leave it alone for now and try to regenerate the model with a new run.
	* Applied a single-commit patch from the ray_train branch, that incorporated all the TODO cleanups,
	  to the algo_checkpoints branch. This is the one that works and can be carried forward.

* Run C2 training single agent BridgitNN with SAC for 30k iters from scratch using 16 bots.
	* Recent TODO cleanups have been applied.
	* New checkpointing with the old Algorithm approach (from pre-2.7 Ray).
	* Training completed with promising ending rmin ~-0.7, rmean ~0.5, rmax ~1.6 and mean ep len > 98.
	  Performance was 6200 ksteps/hr.
	* Inference on checkpoint 30000: Lane 0 starts won't change lanes. Others show pretty good speed
	  control, esp near the lane 0 merge. One start rear-ended a bot, not after a LC. It continues to
	  drive at 29 m/s on tail of lane 1 where limit is 33.5.
	* Inference on checkpoint 20000: Lane 0 starts don't change lanes. General tendency to drive a lot
	  slower than speed limits. This is definitely worse.

1/6/24

* Run C3 training single agent BridgitNN with SAC for additional 10k iters from C2 checkpoint 30000 using
  16 bots. Hoping that the previous model may overcome its single flaw of not changing lanes from lane 0.
	* Training completed, checkpoints are numbered for iterations in this run, not continuing on.
	* With mods to code for loading BridgitNN model, this becomes an old-style model, with the first
	  layer named "macro_encoder", whereas I prefer to name it "fc1" going forward. However, the code
	  cannot load models with different layer names embedded in them. 
	* Inference on checkpoint 10000: lane 0 start still doesn't change lanes! However,
	* A Lane 4 start chose to stay in lane 2 once achieved, rather than moving to lane 1, as all
	  previous models have done. In doing this, it sped up to 33.5 m/s on the tail, as I would have
	  wanted it to in lane 1. Log confirms the macro obs believes its SL here is 29.1, but it acts as
	  if these two lanes are reversed. Otherwise, its speed control is the best I've seen.

* Modified code to allow the Bridgit tactical guidance model (BridgitNN) to load pre-trained weights and
  run in inference mode to provide functionality for the BridgitGuidance.step() method. This will allow
  BridgitGuidance to be loaded into any of the non-ego vehicles.

* Confirmed that the accepted part 1 model (stored as cda1.1-B5-20000) still throws the "is_atari" error,
  and no word yet from the Ray team, so I may need to recreate it anyway.

* Debugged the problem noted above, where lane tail-end speed limits seem to be incorrect.
	* In addition to seeing the correct SL values in the inference logs for each time step, I re-ran
	  the embed_eval program over several snapshots from that region (none with ego in lane 2, as they
	  are apparently rare), and they all look correct - both the raw observation values and the
	  interpreted values coming out of the autoencoder.

* Run C4 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Revised the BridgitNN code so that its first layer is now named "fc1" instead of "macro_encoder",
	  which breaks compatibility with older checkpoints.
	* Modified reward function:
		1 Reduced the penalty for LC desirability poor choice; increased the factor from 0.3 to 0.4.
		2 Reduced normal lane change penalty mult from 0.002 to 0.0001. Both of these changes should
		  encourage the agent to take more risks in changing lanes.
		3 Reduced speed_mult from 0.06 to 0.05 for penalties for not following speed limit (to help
		  avoid episode rewards < -1).
		4 Increased mult for widely varying speed commands from 0.07 to 0.1 to encourage more
		  smoothness.
	* I killed the run after 5k iters since the speed variation penalty was still ridiculously low.

* Run C5 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Checkpoints are in dir 20240106-2006.
	* Revised reward penalty for speed variation - multiplier from 0.1 to 1.0.
	* Training progressed okay until 12000 iters, after which the rewards and episode length suddenly
	  tumbled and stayed at low values from then on.
	* Inference on one of the late iterations showed consistent desire to make immediate lane changes,
	  despite the situation. Inference on iter 12000 worked fairly well, but it had some speed control
	  problems and would change lanes out of lane 0.
	* I killed the run a little early.

1/7/24

* Run C6 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Checkpoints stored in 20240107-0534
	* Changed BotType1bGuidance so that its probability of lane change is reduced from 0.2 to 0.06,
	  in order to make it more lazy about the maneuver.
	* Made the bot guidance (both 1a & 1b) a little more conservative in their ACC logic.
	* Changed the global constants DISTRO_DIST_REAR, DISTRO_DIST_FRONT to be balanced at 200 m each.
	  They were previously 150 and 250, skewing the neighbor vehicles in front of ego. As ego often
	  starts at a low speed, most of the vehicles have passed it by by the time it reaches cruising
	  speed, so they are useless for the sensor training.
	* Training again failed to produce, so killed it after 19k iters. Progress was good through
	  ~14k iters, then the episode lengths dropped dramatically, and rmax went to ~-0.3.
	* Inference on checkpoint 13000 looks pretty decent, but still doesn't change lanes out of 0.

* Run C7 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Rolled back all the reward changes made for runs C4 and C5 above. Keeping the bot changes
	  made for run C6.
	* Checkpoints stored in 20240107-1231.
	* While this was running (at ~14k iters) the inference program crashed with a segfault several
	  times. Usually, this brings down the whole machine eventually, so it may start affecting
	  training results. However, training continued for some time afterwards...
	* Training continued through at least 21k iters, with rewards and mean episode lengths
	  gradually improving like they should. At this point rmin ~-1.5, rmean ~+0.1, rmax ~1.1
	  and mean ep len ~98.
	* Finally died of a segfault after 30200 iters, so the last checkpoint is 30000. At that time
	  rmin ~-1, rmean ~0.5, rmax ~1.5 and mean ep len ~99, training at 6100 ksteps/hr.
	* Inference on checkpoint 30000: still will not change lanes out of 0. Rear-ended bots in two
	  different tests. Otherwise, behavior is basically good.

1/8/24

* Run C8 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Reduced penalty for lane change - multiplier from 0.002 to 0.0002 (reintroducing C4 change
	  #2, but not quite as large a change.
	* Checkpoints stored in 20240108-0834.
	* Training completed in 14 hr (5900 ksteps/hr) with rmin ~-0.7, rmean ~0.5, rmax ~1.7, mean
	  episode len ~99.
	* Inference on checkpoint 40000: lane 0 start still refuses to change lanes. Other lanes
	  perform pretty well, however, twice it crashed while changing lanes coming up from behind
	  a slower neighbor in the left lane. In one episode, it rear-ended a bot (minimal speed diff).
	* Added logging to confirm that both the left & right center-lane boundary observations are
	  correct, as well as the lane change desirabilities, throughout an episode on lane 0. So it
	  simply is not learning how to handle these correct observations.

* Updated graphics to display a different icon for each type of vehicle (by guidance model), and to
  show the primary and secondary target destinations.

1/9/24

* Analysis of training lane assignments early in a training run. It includes 1800 iterations and 450
  episodes logged.
	* Initial lane distro is
		0: 106, 23.6%
		1:  26,  5.8%
		2:  19,  4.2%
		3:  70, 15.6%
		4: 107, 23.8%
		5: 122, 27.1%
	* Confirmed that the number of neighbors being used starts in the low range 0 to 3, and
	  gradually increases.
	* Confirmed that, for the lane 0 starts, speeds look evenly distributed across the full
	  range, and starting P locations also cover the full lane, realizing that the very early
	  ones are grouped closer to the end, and then gradually start backing up toward the beginning
	  of the lane.
	* In summary, all training conditions appear as designed, and nothing that should inhibit the
	  agent from having ample opportunity to learn the lane change required.
	* It could be that, since starts in lanes 3-5 all require left-hand lane changes, and lane 0
	  is the only one requiring a right-hand, the agent simply learns early that a left LC is
	  more important. Therefore, increasing the early opportunities to experience lane 0 might be
	  a benefit.

* Run C9 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Shifted the lane assignment probabilities for the early episodes so that it is now:
		0: 40%
		1:  5%
		2:  5%
		3: 15%
		4: 15%
		5: 20%
	* Increased penalty multiplier for speed variation from 0.07 to 0.1 (reintroducing C4
	  change #4).
	* Change the vehicle mix by adding a few Type1a bots to the mix.
	* Checkpoints stored in 20240109-1304.
	* Training completed in 13.5 hr with rmin ~-1.0, rmean ~0.5, rmax ~1.6, mean ep len ~99.
	* Inference on checkpoint 40000: rear-ended a bot with only a slightly slower speed.
	  Lane 0 starts still don't change lanes. Otherwise, similar behavior to before, not bad.
	  Occasionally, the speed variation penalty shows up at a significant size, but it is
	  rare enough that it probably doesn't have much impact on behavior; need it larger.

1/10/24

* Run C10 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Increased multiplier on speed variation penalty from 0.1 to 0.4.
	* Added flexible targeting, meaning that each episode reset has the opportunity to 
	  randomly select a different set of active targets from the list of valid targets given
	  by the user (in a config list). Valid targets can be a subset of the total list of
	  targets defined. These apply only to the ego vehicle; neighbor vehicle targets always
	  have the full list of defined targets available.
		* For this run, only target #2 is valid (the one in lane 2), thus forcing the
		  agent to learn more about right-hand LCs. Target randomization is off.
	* During early inference, discovered that the target mod didn't take effect, because
	  the reward function didn't get modified to only look at active targets. Therefore,
	  this training was using all 4 targets for the ego vehicle (different from all previous
	  runs, which only used targets 1 & 2).
	* Inference on checkpoint 29000: agent has learned to steer out of lane 0 sometimes.
	  However, since it trained with all 4 targets active, it sometimes steered _into_ lane
	  0 from lane 1 just to reach its endpoint! Giving it individual targets in inference
	  now is no good, since it hasn't learned too much about the meaning of the desirability
	  values, as they didn't change much in training, at least from episode to episode in
	  the same lane. Training with randomly chosen targets will provide it a much better
	  understanding of why those obs are valuable.
	* Speed changes were still erratic, so the penalty maybe needs to be bigger still.

* Run C11 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Fixed the reward function to only consider active targets.
	* Configured to randomize over all 4 targets for training.
	* Changed RoadwayB so that the ends of lanes 0 & 4 are marked as ASPHALT, which were
	  previously EXIT_RAMP. EXIT_RAMP is now not used.
	* Increased penalty for speed variation multiplier from 0.4 to 0.6, while reducing the
	  multiplier for speed limit deviation from 0.06 to 0.04 to keep from piling on large
	  penalties (C4 change #3).
	* Checkpoints are stored in 20240110-2217.
	* Run died early due to a segfault. Completed 15600 iters, at which point rmin ~-2.2,
	  rmean ~-0.2, rmax ~0.9, mean ep len ~91.
	* Inference on checkpoint 15000: does not change lanes out of lane 0. Speeds are a
	  little smoother, but still occasional large swings for no reason.

1/11/24

* Run C12 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Changed multiplier on speed variation penalty from 0.6 to 1.0.
	* Reduced penalty for making a poor choice on LC desirabilty from -factor to -0.3*factor.
***	* Updated RoadwayB to alter the track a bit:
		* Lengthened the exit ramps on lanes 0 and 4 to 200 m, so that they each have
		  100 m beyond their target destinations to provide conssitent sensor data
		  that gives the agent the impression of an on-going drive.
		* Changed the speed limits on the lane 3 segment 1 from 33.5 to 31.3 m/s and
		  on the lane 4 & 5 on-ramps from 20.1 to 22.4 m/s to give more variety of
		  training experiences.
	* Checkpoints stored in 20240111-0959.
	* After ~15k iters, rewards and episode lengths tanked. Inference on checkpoint 22000
	  shows rapid LC command, no matter where it is, until it runs off road. This is
	  apparently more desirable than the huge speed penalties it is receiving.

* Run C13 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Reduced speed limit penalty multiplier from 0.04 to 0.02.
	* Checkpoints stored in 20240111-1744.
	* Over the past several runs I have seen several crashes where agent changes lanes
	  just in front of a faster moving bot. Usually these occur late in the LC count,
	  indicating that when it committed to beginning the LC, the neighbor was probably
	  too far back to be seen. Since it has no way to abort an underway LC, the crash is
	  then inevitable. And the bot can't sense the ego vehicle's LC in progress until it
	  crosses the dividing line (step 8).
	* Training proceeded well through ~15k iters, at which point it showed rmin ~-2.1,
	  rmean ~-0.1, rmax ~1.2 and mean ep len ~89 steps. At that point performance began
	  to drop, oddly ending with similar rewards but mean ep len dropped to ~5 then
	  somewhat recovered to ~25 by the time it hit 40k iters.
	* Inference on checkpoint 15000: prefers to steer toward lane 1 even when there is
	  not an active target there. After several episodes, I saw one lane 0
	  start that changed lanes when no target exists in lane 0.
	* Inference on checkpoint 21000 (where mean ep len was worst): a lane 0 start
	  changed lanes into 1. Otherwise, every start did an immediate left turn off-road.
	* Inference on checkpoint 40000: not much different from 21000.
	* Inference on checkpoint 16000 with 100 step limit (like training episodes):
	  well-performing episodes only score -0.3 (no reward for completing the trajectory).
		* Lane 4 start changed to lane 3, as needed, and got a "poor" desirability
		  penalty of -0.9 for that!

1/12/24

* Run C14 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Eliminated reward bonus for complete episode (normally 100 steps) or for hitting
	  an active target, since those are so rare, and is outside the control of the
	  tactical guidance anyway.
	* Added an alert flag in the Vehicle class, which is now set if a poor LC desirability
	  choice is made (in the reward function), and displays on the inference graphics.
	* Changed LC desirability reward section to ony award for the single time step that
	  initiates the LC mvr (it was doing it for two consecutive steps), so that bonus
	  impact will be lower.
	* Halved the speed limit penalty multiplier from 0.02 to 0.01 to emphasize the value
	  of followiing lane desirability and speed smoothness.
	* Checkpoints stored in 20240112-1811.
	* I killed the run after 9k iters, in favor of a better reward scheme (next run).

* Run C15 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Changed reward design for lane change desirability, providing a keep-alive bonus for
	  every time step that is proportional to the desirability of that lateral decision.
	* Checkpoints stored in 20240112-2129.
	* Training completed with rmin ~-1 (many iters had rmin > -1), rmean ~0.6, rmax ~0.95
	  and mean ep len ~94 (never an iter with mean ep len > 98).
	* Inference on checkpoint 32000: way too many lane changes; several of them are
	  reckless, causing sideways crashes, even into bots it should see just off the front
	  bow. Speed selection is good, but still noisy.
	* Inference on checkpoint 40000: same as above with additional notes. Extraneous LCs
	  happen periodically back and forth between two of the long lanes, usually with 2-4
	  steps of rest in between mvrs. This induces a frequency penalty about half as large
	  as the max desirability bonus. Even egregious speed deviations are now only getting
	  penalties ~0.005, which is half the desirability bonus for a time step. The worst
	  penalties for speed variation are ~0.003, and usually a lot smaller.

1/13/24

* Run C16 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Increased reward penalty for frequent lane changes; mult from 0.0002 to 0.002 (as
	  it was prior to run C4).
	* Changed reward penalty for speed command variation from a square relationship to a
	  linear one, with multiplier reduced from 1.0 to 0.5, hoping to give more motivation
	  to tame the smaller variations, while not being severe in the occasional large ones,
	  which are needed occasionally.
	* Reduced speed deviation penalty mutliplier from 0.01 to 0.006 to keep from over-
	  burdening the agent with piles of penalties.
	* Checkpoints stored in 20240113-1638.
	* Training killed by OS at 16800 iters due to some file system fault.
		* Rebooted and resumed training from checkpoint 16000.
		* The resumed training got to 7000 additional iters, but by then mean ep len
		  had dropped to 4 steps. This appears unstable and not worth continuing.
		* Killed it.

1/14/24

* Run C17 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Reduced LR for policy & critic from 3e-5 to 2e-5.
	* Checkpoints stored in 20240114-0051.
	* After 29k iters, it has clearly not learned well. rmax ~-0.8 and mean ep len ~6.
	* Inference shows a strong desire to change lanes regardless of the circumstances. It
	  also seems to like very low speeds if it lasts long enough.
	* Killed the job at 31k iters due to poor learning.

* Run C18 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Reduced speed variation penalty multiplier from 0.5 to 0.2.
	* Changed reward bonus for LC maneuvers. Instead of a big lump at the beginning of the
	  mvr, which may be encouraging frequend mvr initiation, spread out the desirability-
	  based bonus on every time step in the maneuver.
	* Changed resource allocation: num_gpus from 0.2 to 0.5 (for the local worker doing the
	  learning algo) and num_gpus_per_worker from 0.2 to 0.1 (for 4 remote workers).
	* Checkpoints stored in 20240114-1129.
	* Killed it after 26k iters, as I discovered a defect in the reward code that
	  discouraged it from changing lanes.

* Run C19 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Fixed reward defect (wan't giving rewards during LC maneuvers).
	* Checkpoints stored in 20240114-2232.
	* Training progressed well through 33k iters, at which point I evaluated.
	* Inference on checkpoint 33000: it has little respect for speed limits, often going
	  much slower, probably for accumulating more LC des points. Another quirk is that it
	  tends to choose a poor LC that avoids a target, allowing the LC des bonus to diminish
	  for a while, but once no target is achievable, that bonus goes back to 0.01 per step
	  for the remainder.

1/15/24

* Run C20 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Changed LC des bonus so that it does not award a significant value when the agent is
	  on a no-win lane (target unreachable), to discourage taking this path just to collect
	  points.
	* Increased reward penalty for deviation from speed limit; multiplier from 0.006 to 0.008.
	* Modified ACC logic for both bot 1a and bot 1b guidance to be more conservative, to
	  avoid more rear-end collisions.
	* Checkpoints stored in 20240115-1017.
	* Training went well through 38k iters (where I evaluated), at that point with rmin ~-1,
	  rmean ~0.2, rmax ~0.5, mean ep len > 97.
	* Inference on checkpoint 38000: terrible speed, loving the very low values, esp before
	  doing a lane change. Prefers to change lanes from 4 to 3 even when there is a tgt in
	  lane 4 and the desirability vector prefers staying in lane 4. It refuses to change
	  lanes out of 0 when the nearest target is in 1 or 2. Twice it rear-ended a bot after
	  following it for some time (not large speed diffs).

* Run C21 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Changed speed penalty from a linear to a square relationship, with multiplier of 0.25.
	* Changed LC desirability reward from linear to a square relationship to help deter it
	  from choosing a non-optimum lane.
	* Added data element to obs vector for distance to forward vehicle in previous time step.
	  This should allow the agent to better judge the collision risk by sensing its relative
	  speed.
	* Checkpoints stored in 20240116-0006.
	* Training completed with questionable performance. rmax did okay, climbing to ~0.3, but
	  rmin and rmean didn't climb as in previous runs, ending ~-3, -0.8, respectively, with
	  mean ep len topping out ~80 at 11k iters, then gradually dropping to ~50 at the end.
	* Inference on checkpoint 33000: it somewhat attempts to follow speed limit (better than
	  C20), but still erratic. LC decisions are befuddling. Lane 0 starts won't change lanes
	  to get to a valid target, but lane 4 starts will change to lane 3 even when there's a
	  target in lane 4. It seems to have a heavy bias toward moving left whenever possible.
	  Speed penalties are pretty stiff (often 0.01 or more), and it doesn't seem to be
	  trying to avoid these. It still does random off-roads or crashes into neighbors, either
	  rear-end or adjacent lane.

1/16/24

* Run C22 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Added more explanation details to the LC des reward to show the full desirability vector
	  at each step, to help ensure there is not a defect somewhere.
	* Added modest reward (0.1) for completing a 100-step episode.
	* Changed all 3 learning rates to 1e-5 (was 2e-5 for actor & critic and 5e-5 for entropy).
	* Checkpoints stored in 20240116-1301.
	* Killed it early after finding a code defect in BridgitGuidance.

* Run C23 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots
	* Fixed defect in BridgitGuidance assigning unreasonably low lane desirabilities in some
	  cases.
	* Changed env _choose_active_targets() to generally lean toward a small number of targets
	  most of the time.
	* Checkpoints stored in 20240116-1457.
	* Training completed in 13.4 hr with ending rmin ~-3.4, rmean ~-0.6, rmax ~+0.5 and mean
	  ep len ~99.
	* Inference on checkpoint 40000: lane 0 start seems willing to change lanes for a target
	  to the right! However, starts in lane 4 & 5 don't want to change. It seems the LC des
	  bonus is usually being swamped by the speed penalties and sometimes the speed variation
	  penalties. I also see that an episode completion bonus (0.1) is awarded for a trajectory
	  in lane 1 that goes to the no-win zone (too far to reach the target in lane 4), thus its
	  max lane desirability is only 0.01. A trajectory that gets into this situation should be
	  heavily penalized.

1/17/24

* Run C24 training single BridgitNN agent from scratch with SAC over 50k iters using 16 bots.
	* Changing episode completion reward to only apply the bonus if it ends in a state that is
	  still viable to reach a target; otherwise it gets a big penalty.
	* Reduced speed deviation reward multiplier from 0.25 to 0.06 so that it is only a secondary
	  influence.
	* Removed penalty for speed variation, just to keep the picture clean and focused on lane
	  changes for now. May reintroduce this later.
	* Checkpoints stored in 20240117-1124.
	* Training completed in 16.9 hr (6.1 M steps/hr). Rewards and ep len plateaued early and
	  didn't change much after ~22k iters, where rmin ~-1, rmean ~0.3, rmax ~+1 and mean ep len
	  ~98. However, rmean seemed to improve a bit between there and the end, showing more
	  occurrences of ~0.5.
	* Inference on checkpoint 50000: it seems the agent is always aiming for an episode reward
	  (100 steps) close to 0, but afraid to go much above that. If LC des bonuses are good, it
	  chooses really low speeds to counteract the bonuses; if LC des is 0 because no target is
	  reachable, then it does really good speed control. It is willing to change lanes out of 0,
	  but once it did that despite the only target being in lane 0.
		* It has a strong desire to change lanes left regardless of the target location.
		* It also seems to not care about neighbor vehicles, at least in adjacent lanes,
		  because it often crashes during a lane change into a neighbor going roughly the
		  same speed, so blindness isn't the problem. Maybe the vehice sensor compression
		  was too much?
	* Inference on checkpoint 24000: performed a lot worse than the 50000 model. It really 
	  likes lower speeds, and I never saw a lane change.
	* Inference on checkpoint 37000: sometimes speeds are quite steady, but often way too low.
	  No lane changes observed.
	* Given the 3 checkpoint behaviors, maybe it just hasn't had enough training. It seems to
	  be moving in the right direction, so let's give it more time.

1/18/24


* Run C25 training single BridgitNN agent from run C24 checkpoint 50000, with SAC over additional
  50k iters using 16 bots.
	* Checkpoints stored in 20240118-1308.
	* Training completed in 16.9 hr. Rewards generally improved to rmin ~-0.9, rmean ~0.4,
	  rmax ~0.9, mean ep len ~99.
	* Inference on checkpoint 50000 (100k iters & 102M steps of total training): mostly not
	  interested in changing lanes, except near end when it does an illegal off-road. Speeds
	  are mostly well below speed limit. Nothing good here.  Run C24 was probably a little
	  better.

1/19/24

***
* Considerations for continued work, given the lack of success so far. I'm not even doing the
  intended part 2 work yet, still trying to reproduce the results of part 1 since that checkpoint
  can't be read with the new Ray library.
	1. Drop back to Ray 2.5 to work part 2.
		This feels short-sighted, because any new results will need to be migrated to
		2.9+ eventually.
	2. Rerun the final part 1 code under Ray 2.9 to generate a new checkpoint that is forward
	   compatible.
		May be worthwhile, just to prove that it wasn't a one-time fluke. It would also
		provide a baseline for comparison of all changes since then.
	3. Review how lane changes out of 0 were handled in part 1.
	4. Alter NN shape a bit.
		Easy to play with, but will take time for probably several trials, and no
		confidence that anything will improve.
	5. Add dropouts to the NN.
		Easy to do, one or two trials should give results.
	6. Re-architect the whole NN - add recursion and/or conv layers.
	7. Adjust noise params.
	8. Much longer training from scratch.
	9. Adjust learning rates (larger?).
	10. Change the sensor encoders for more accuracy or smaller size.
	11. Remove all the sensor observations and replace them with just a couple simple inputs.
	12. Abort the whole idea of using a NN for tactical guidance & replace with a pid. Then I
	   can focus on the meat of the bigger project, by building a NN for strategic guidance
	   and move into cda2 to add messaging to its actions.

* Run C26 training single BridgitNN agent from scratch, with SAC over 80k iters using 16 bots.
	* Expanded BridgitNN size (added layer 4, increased sizes of layers 1 and 2), and added
	  20% dropout to outputs of layers 1-3. This implements ideas 4, 5 & 8 from above.
	* Checkpoints written to 20240119-1637.
	* Died after 21k iters due to Ray error.
	* Restarted from checkpoint 21000 - completion job is at 20240120-0843.
	* Training ran to 77k iters after the restart (98k iters total), leveling off at
	  rmin ~-0.9, rmean ~0.4, rmax ~1.0, mean ep len ~100. This pattern started around iter
	  20000 of the restart (41k total). Still performing at 6.1M steps/hr.
	* Inference on -0843 checkpoint 77000: no lane changes ever. Speeds tend to hang close to
	  18-20 m/s in all situations. No good.
	* Inference on -0843 checkpoint 20000: same performance. Speeds are quite smooth and
	  steady, just at the wrong value. It seems the larger network and/or the dropouts have
	  made outputs a lot smoother, but maybe overly so (i.e. not interested in taking on new
	  values when warranted)?

1/21/24

* Run C27 training single BridgitNN agent from scratch, with SAC over 80k iters using 16 bots.
	* Changed reward for LC des to increase exponentially with each additional time step
	  (built for a limit of 100 time steps per episode).
	* Changed reset() to uniformly initialize agent's P coord along the lane's length, 
	  regardless of how many episodes have completed. It used to force all starts to beginning
	  of the lane after early episodes, so lots of the 100-step episodes never saw the full
	  LC experiences!
	* Changed reset() to not initialize speed < 10 m/s, since to many starts have been down
	  at those ridiculously low speeds, and cause unnecessary start-up crashes.
	* Checkpoints stored at 20240121-1323.
	* Training completed in ? hr. rewards didn't climb as much as before, and mean ep len
	  never reached 100. It hit low 90s after ~13k iters, then after 21k it started to drop
	  off a bit, ending ~81. rmean was rather noisy, but seems to be drifting upward from 12k
	  iters onward, ending ~-0.2, while rmax also climbed slightly, ending ~0.9.
	* Inference on checkpoint 30000 with target only in lane 2: starts in lanes 0 & 1 never
	  change lanes. Start in lane 3 changes lanes left eagerly, all the way to lane 1 (beyond
	  the target lane). Lane 4 & 5 starts change lanes nicely also, but all the way to lane 1.
	  Never saw a right lane change. In lane 0 the LC cmd never came close to the 0.5 thresh.

* Run C28 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots.
	* Changed BridgitGuidance so that LC desirability never exceeds 0.9 for a lane that doesn't
	  include a target, regardless of how far away it is.
	* Changed reward for LC desirability so that if desirability < 0.2 there is a penalty
	  rather than reducing the bonus to near zero.
	* Increased LC des bonus multiplier (A) from 0.003 to 0.004.
	* Checkpoints stored in 20240122-0020.
	* Training completed with results very similar to C27. Highest mean ep len was 95 steps.
	* Inference on checkpoint 20000: lane 4 & 5 starts do pretty well on lane changes, although
	  they tend to happen later than ideal. Lane 0 & 1 starts (with target in lane 2) never move
	  to the right. Not concerned with poor speed performance now.

1/22/24

* Run C29 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots.
	* Removed the tanh nonlinearity from BridgitNN layer 4 (left it linear).
	* Checkpoints stored in 20240122-1411.
	* Training completed with steady results since ~12k iters. rmin ~-2.5, rmean ~-0.7 noisy,
	  rmax ~1, mean ep len ~85.
	* Inference on checkpoint 30000: it is reluctant to change lanes. It does so sometimes from
	  lanes 3 & 4 (for lane 2 tgt), but never from lane 5, 0 or 1. Never saw it change lanes right.
	  Speeds are always way low, but not worried about them now.

1/23/24

* Run C30 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots.
	* Restoring the tanh nonlinearity back to layer 4 of the BridgitNN.
	* Changed NN dropout from 0.2 to 0.1.
	* Reordered completion logic in get_rewards() to deal with dones before dealing with inactive
	  vehicles, which had prevented it from seeing complete episode rewards.
	* Fixed defect in ACC logic for bots 1a & 1b to allow it to handle jittery forward vehicles
	  better, thus avoiding some rear-end crashes that may be providing artificially negative
	  training experiences (not the agent's fault).
	* Overrode step() to force speed command to always be 28 m/s, so that it will not have to
	  worry about learning that channel also, and can focus on learning proper LC commands.
	* Checkpoints stored in 20240123-1243.
	* Training completed in 10.1 hr. As before, it never reached desirable performance. rmin
	  ~-1.9, rmean ~-0.1, rmax ~1.4, mean ep len peaked at 85 then diminished to ~71 by the end.
	* Inference on 30000: forcing a constant speed sometimes causes agent to rear-end slower bots,
	  which will affect its training, giving it artificial penalties in some cases.
		* Lane 0 start changed lanes! then changed several more times between 1 & 2 (tgt only
		  in lane 2). In additional runs, it never changed again once hitting lane 1.
		* Lane 1 start stayed in lane 1. Starts in higher lanes all move over to get into
		  lane 1, despite negative LC des rewards.
		* The difference in LC des reward between a lane with des = 1 and a lane with
		  des = 0.67 is not that much (about 70% reduction), and probably should be more
		  drastic to help discourage stayiing in such a lane long term. 

1/24/24

* Ran the train program with extra logging to ensure that it is randomizing the initial conditions
  well and that reasonable trajectories are getting executed. All looks good. Initial lane choices
  are heavily weighted to lane 0, and otherwise spread according to the probability distribution
  coded. Target lists seem uniformly distributed, and mostly contain only 1 target. Step method
  shows many early cases of lane changes both right and left, although left is more commont
  (probably because there are more opportunities to do so).
	* May want to avoid lane 0 starts when only target 3 is active, since it is unreachable.

* Run C31 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots.
	* Removed the artificial holding of all speed commands; allow NN to learn them again.
	* Changed reward for LC des to be harsher on desirabilities < 1 by squaring it.
	* Also made the penalty for a terrible choice to grow exponentially. Changed both exponential
	  growths to start over with each lane change rather than counting steps since beginning of
	  episode.
	* Added reset() logic to prevent starting the ego vehicle in any location that is out of reach
	  of at least one target.
	* Checkpoints stored at 20240124-2038.
	* Training completed, but no good. Mean ep len touched 90, but settled in the mid-80s.
	  rmin ~-7, rmean ~-1.3, rmax ~0.9.
	* Inference on checkpoint 30000: speed is usually really low. Lane changes are good from lanes
	  3, 4, 5. Lane 0 starts will change lanes to 1 if in the merge zone before speed has dropped
	  a lot, but if it starts near beginning, speed drops before reaching the merge zone, and it
	  doesn't bother to change lanes. Even so, once in lane 1 it won't go to target in lane 2.
		* Noted that the LC des penalty for terrible choice does not increase exponentially,
		  as intended. Found that it is a function of steps since LC, which is capped at 60
		  steps; needs to grow unlimited.

1/27/24

* Created git branch update_roadway to generalize the Roadway class as a polymorphic interface.
	* Created class RoadwayB to represent the heretofor used roadway geometry. Ensured the train
	  and inference programs worked okay with this new refactoring.
	* Created class RoadwayC defining a brand-new track with 6 parallel lanes that each have several
	  speed limits along their length. It is symmetrical to avoid any preference for right- vs left-
	  turn learning. Adjusted the graphics somewhat to make a pleasing display of this one also.
	* Created class RoadwayD defining a brand-new track with 7 parallel lanes that each has a
	  different speed limit, but is constant through its length. Speed limits vary symmetrically
	  right and left.

1/28/24

* Completed enhancement to make roadway randomly selectable (between C and D) during setup of each
  episode.
* Fixed an apparent problem in the HpPrng where it tended to provide correlated sequences. Each time
  it was called by reset() to select a roadway model it alternated very regularly between the two
  choices. I added an occasional extra iteration of seed calculation within the Prng to avoid this.

* Run C32 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots.
	* Running 50/50 on roadways C and D only.
	* Includes updated PRNG, per previous bullet.
	* Checkpoints stored in 20240128-1315.
	* Inference on checkpoint 25000 for a couple episodes. Besides being really slow, it doesn't
	  seem to know how to steer toward a target.

* Fixed HighwayEnv reset() to properly manage Roadway objects when it is called multiple times.
  This was causing a graphic display problem when inference would make multiple reset() calls and it
  chose to select a different roadway type in each call (map coordinate frames are different).
	* This error possibly screws up results of C32 training, which was underway at the time.

* Run C33 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots.
	* Increased reward multiplier for speed deviation from 0.06 to 0.25, reversing the change
	  made in run C24 to allow focus on lane changes. Now that the two new straight roadways
	  are in place, and provide richer training ground for speed control, and a symmetrical
	  place to learn lane changes, there is no reason to ignore speed control.
	* Fixed reset() to properly handle memory of the roadway object when called multiple times
	  (see above bullet), which probably screwed up results for C32.
	* Checkpoints are stored at 20240128-2320.
	* Training completed. From ~10k iters onward mean ep len was in mid-90s, but tailed off to
	  ~90 at the very end. rmax rose quickly to ~1 and stayed there until ~10k iters, then
	  slowly wavered, but reached consistent ~1.4 around iter 27k, then dropped into negatives.
	  rmean wavered in [-3, -1] and rmin never exceeded -6.
	* Inference on checkpoint 16000: no lane changes, but speed control is quite steady, just
	  slow.

2/1/24

*** Realized that training has been discouraging a couple key behaviors, namely following the LC
  desirability inputs, and driving as fast as the speed limit suggests. For the lane changes,
  the reward is spread across the full 15 steps of the LC maneuver, using the reward value earned
  just before the LC, which is low enough to push the agent away from it; it has to look 15 steps
  into the future to realize the improved reward for initiating the maneuver, which is a lot to
  learn without a recurrent layer.  For the speed problem, the training episodes have been allowed
  to begin up to 150 m in front of the end of lanes, which means that many of the episodes will
  either run off the end of the lane (big penalty), or it learns to slow way down to get its 100
  steps in before hitting the end!

* Run C34 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Changed reset() to set ego initial speed as a Gaussian distro around where it is normally
	  expected to operate (near 29 m/s), in order to get it used to driving that fast right
	  away, giving it a better chance of success. Sometimes speed will be initialized lower,
	  to give it a full range of experiences, however.
	* Changed reset() to limit initial location to be no closer to the end of a lane than 700 m
	  (for lanes that are long enough), so that it isn't forced to run off the end early in
	  training, thus learning to simply slow down as a preventive measure.
	* Increased termiinal reward for completing episode with a decent LC desirability from
	  0.1 to 0.5.
	* LC desirability reward already seems to be assigning the to-be lane's bonus, so no
	  change for now.
	* Checkpoints are in 20240201-1955.
	* Run died after 11200 iters. Rebooted then started a new job from checkpoint 11000, which
	  gives an error about priority = 0 in the prioritized_replay_buffer, even though the
	  checkpoint was successfully loaded. Started a new job from checkpoint 10000.
		* Continuing checkpoints stored in 20240202-0103.
	* Training completed the new set of 30k iters. Typical rmax ~1.6, but rmean varied slowly
	  in [-1, +0.8] and rmin in [-12, -3]. Mean ep len consistently ~98.
	* Inference on checkpoint 9000 showed no lane changes, and speed mostly held ~20 m/s.
	* Inference on checkpoint 28000: speed control follows speed limits fairly well now, with
	  a bit of positive bias, but penatlies aren't very big. It slows to avoid rear-ending bots
	  in front. Lane changes happen sometime, and sometimes not good choices.

2/2/24

* Verified through added logging, that the replay buffer seems to be working as expected, taking in
  random steps until the Exploration object is done randomizing, then serving up experiences when
  asked.

* Run C35 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Fixed a defect in BridgitGuidance that was allowing possibly incorrect LC des calcuations
	  for the first 5 time steps of each episode.
	* Adjusted replay buffering: reduced capacity from 1M to 100k, and reduced exploration
	  initial random timesteps from 1M to 10k so that it can more quickly start learning from
	  some positive examples.
	* Checkpoints stored in 20240202-1331.
	* Training completed. Mean ep len quickly reached 98-100 and stayed there. Similarly, rmax
	  reached ~1.8 and stayed there. However, rmax grew to ~0.3, maxing out at 0.5 at 12k iters,
	  then dropped to < 0 for the remainder. rmin started ~-4, but gradually moved down to ~-10.
	* Inference on checkpoint 12000: never a lane change, regardless of desirability. Speeds
	  were mostly in [20, 22] regardless of speed limit, but it was very smooth.
	* Inference on checkpoint 30000: similar behavior, only speeds tended to be even slower,
	  sometimes < 9 m/s for a whole episode.

2/3/24

* Merged update_roadway branch back into the part2 branch. Then created new branch lat_motion
  to work on more sophisticated lateral movement logic.

* Run C36 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Changed exploration initial random timesteps from 10k to 100k so it fills the buffer.
	* Changed bot1b guidance to have slightly slower speed range (most of them will be
	  slower than the speed limit), to further stress the agent's ACC and encourage it to
	  do a passing maneuver if travel time becomes a goal.
	* Changed bot1, Bridgit & embed models to recognize neighbors changing lanes from an
	  adjacent lane into the bot's lane as candidates for nearest forward vehicle (input to
	  the ACC algo in the bots and embed guidance), which will help it avoid rear-ending
	  slower vehicles changing lanes in front of it.
	* Changed reward penalty for crashes to be more selective. It gives a -3 points for
	  rear-ending a neighbor or changing lanes into a neighbor who should be easily
	  visible, but no penalty for crashes caused by the neighbor or by moving into the
	  path of a fast-moving neighbor that ego couldn't see. It also gives -3 points for
	  any undetermined crash reason.
	* Checkpoints stored in 20240203-2301.
	* Training completed. Similar story as recent runs, with mean ep len ~98 early, rmax
	  ~1.7 early, but rmean barely reached > 0.
	* Inference on checkpoint 30000: speeds are typically very slow, and sometimes change
	  inexplicably. Never see a lane change.
	* Inference on checkpoint 10000: similar performance. Agent rear-ended a bot without
	  even attempting to slow down a bit.
	* Realized that LC des bonuses being keyed to steps since LC are encouraging agent to
	  stay in lane, since bonuses are bigger there than when a change occurs.

2/4/24

* Run C37 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Changed BridgitGuidance route planning to not normalize the LC desirabilities. In
	  cases where none of the 3 lanes hold a target, it needs to see desirabilities < 1
	  so it knows to get out of there.
	* Changed reward for LC desirability to grow exponentially from beginning of episode,
	  regardless of LC activity (was resetting the exponential growth aftere each LC).
	* Doubled speed deviation penalty multiplier from 0.25 to 0.5.
	* Checkpoints stored at 20240204-1822.
	* Training completed. Mean ep len reached 95 in 2600 iters, then stayed > 90, mostly
	  > 96, sometimes hitting 100. rmax reached 1.7 at 9600 iters, then stayed > 1.4,
	  mostly in 1.7 range. However, rmin started ~-8 and gradually got worse, ending in
	  the -20s. rmean then started ~-3, and after 8000 iters hung out in -1 to -2.
	* Inference on checkpoint 10000: no desire to change lanes. Speeds hang out ~19 m/s
	  regardless of speed limit, accepting pretty large penalties.

2/5/24

* Merged the lat_motion branch back into part2.

* Enhanced the inference program (& graphics) to allow manual override of the command vector
  (action output of ego's NN) for a single time step. This allows investigating behavior &
  rewards in lane change scenarios. Used this tool to see that LC rewards were working against
  my desires, so made some improvements.

* Run C38 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Tweaked reward penalty for LC terrible choices - multiplier from 0.02 to 0.004,
	  since it was producing crazy big values.
	* Changed LC des reward to accumulate it exponentially even during the LC
	  maneuver, so that cumulative rewards are bigger for making a good LC decision
	  than for staying in lane.
	* Tweaked BridgitGuidance route planning to start degrading desirability values
	  farther uptrack of where the LC is required to avoid two lanes showing 0.9 for
	  a long time, making the choice more difficult. It also forces a strategic replan
	  immediately after a LC completes so that the new desirability values are available.
	* Reduced multiplier for LC frequency penalty from 0.002 to 0.0002, since some of
	  the scenarios in the new roadways may require several rapid lane changes to reach
	  the target.
	* Enhanced inference a bit to allow manual override commands to apply for multiple
	  steps.
	* Checkpoints stored in 20240205-2217.
	* Training completed. Performance profile similar to before. Mean ep length stayed
	  high throughout, but rmax began to drop a lot after ~26k iters. rmean touched
	  above -1 at ~14k iters briefly, then more solidly ~24k iters (reaching -0.7).
	* Inference on checkpoint 24000: one run started near where it needed to LC soon,
	  and LC desirability values, even for the best lane, were already low (0.23), so
	  it immediately started getting penalized for all possible choices. No lane changes
	  happening. Speeds still hanging out ~18-20 m/s, regardless of speed limit. It
	  appears that it has not explored at all, or has somehow found any other actions
	  to be bad choices.

2/6/24

* Added logging to MultiAgentReplayBuffer to confirm that the pre-populated random experiences
  did have what appear to be a uniformly random distribution of both actions. So it seems that
  the agent is initially exposed to a wide range of experiences & rewards. I'm starting to
  wonder if there are too many neighbor vehicles that prevent it from taking big actions, for
  fear of inducing a crash.

* Run C39 training single BridgitNN agent from scratch, with SAC over 30k iters using 4 bots on
  roadways C & D.
	* Changed the vehicles config file to only include the ego and 4 bots.
	* Tweaked reward for LC des to apply a bonus if des (which is desirability^2) > 0.04,
	  where it was previously > 0.2.
	* Checkpoints stored in 20240206-2025.
	* Training completed with similar reward/ep len profile as before, but rmean got a
	  little higher for a little longer, around 26k iters, reached -0.3 for some time.
	* Inference on checkpoint 26000 shows desire to drive a little faster (~24 m/s), but
	  still no lane changes.

2/7/24

* Run C40 training single BridgitNN agent from scratch, with SAC over 30k iters using 1 bot on
  roadways C & D.
	* Replaced vehicle config file with the 1n file so it is training with only a single
	  neighbor vehicle.
	* Checkpoints stored 20240207-1100.
	* Training completed. Reward picture was somewhat better, with rmin ~-4 and rmean
	  ~-0.3 and normally mean ep len = 100 for the last 15k iters.
	* Inference on checkpoint 30000: speeds are definitely higher here, up to 27 m/s, but
	  still won't match the highest speed limits. And still no lane changes. Note that
	  there is no penalty for abrupt speed changes, but it does seem reluctant to change
	  when the speed limit hits a step change.

* Run C41 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Back to using the full 16 bot vehicle config, since that clearly isn't the only
	  thing preventing the agent from driving at full speeds.
	* Changed BridgitNN to use leaky_relu as the activation for all except layer 4,
	  which still uses tanh, since its output must be constrained to [-1, 1].
	* Checkpoints stored at 20240207-2149.
	* Training completed in 9.9 hr. Mean ep len reached 98 by 9k iters and stayed there
	  (often hitting 100) for the remainder. rmax ~1.4, rmean ended ~-1.5, but held
	  ~-0.7 for several hundred iters around 12k. At this point, rmin peaked ~-4 also.
	* Inference on checkpoint 12000: agent rear-ended a bot with 1.5 m/s speed diff in
	  same lane. No lane changes, and speeds are steady & slow (~22 m/s).
	* Ego was rear-ended by bot 5, with a speed diff of only 1.9 (bot was coming from
	  adjacent lane with no other bots in its forward view).

2/8/24

* Run C42 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Changed all 3 LRs from 1e-5 to 1e-4.
	* Changed replay buffer capacity from 100k to 1M.
	* Checkpoints stored in 20240208-1007.
	* Training completed. Mean ep len only maxed out at 98, but mostly stayed ~95, dropping
	  into the 70s near the end. rmax stayed ~1.6, and rmin ~-3. rmean peaked slowly ~+0.2
	  around 19k iters (stayed there for quite a while). Total time 10.4 hr.
	* Inference on checkpoint 19000: lane 4 start on roadway C shows rapid LC between 4 & 5
	  repeatedly, and each time it gets a new speed limit, the speed commands swing from one
	  extreme to the other in attempt to accelerate as rapidly as possible to the new desired
	  speed. Other lane starts acted similarly. On one it drove off the side of the road, too
	  eager to change lanes as often as possible. The LC frequency penalties were at most
	  equal to the speed penalties being levied every time step, so not really noticeable.

* Run C43 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Changed all LRs from 1e-4 to 7e-5.
	* Changed batch size from 1024 to 256 to capture more variance between batches; adjusted
	  rollout_fragment_length from 32 to 8 accordingly.
	* Slightly reduced the speed deviation penalty multiplier from 0.5 to 0.4 so it will
	  relax a little.
	* Increased penalty multiplier for LC frequency from 0.0002 to 0.0004.
	* Reduced LC desirability bonus by 0.3 if it is not choosing the best lane.
	* Checkpoints written to 20240208-2254.
	* Training completed in 9.2 hr. Mean ep len ~98, but never hit 100. rmax ~1.6, rmean
	  very slowly leveled out ~-1.1, and rmin ~-5.
	* Inference on checkpoint 30000: no lane changes, but new LC des rewards seem to be
	  working as intended. Speed control is not very good either - unresponsive. Playing
	  with injecting LC commands, I see rewards aren't always proportional to the best choice
	  available. When all 3 desirabilities are low (tgt is several lanes over), then rewards
	  are small, even though it is doing the best choice available (a single LC).
	* Realized there was a typo in 2 of the LRs, where they were e-7 instead of e-5!

2/9/24

* Run C44 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Changed LC desirability reward to be based only on the relative desirabilities of each
	  of the lanes available, rather than on the desirability value itself, so rewards now
	  grow consistently, independent of where we are on the track or where the target is, if
	  it is not in an immediately available lane.
	* Fixed the two erroneous LRs, so all 3 are now 7e-5.
	* Checkpoint stored in 20240209-1247.


NEXT:
- add evaulation steps to the training loop
- play with noise level, esp entropy coefficient
- play with LR
- try smaller batch size to get more variation between batches
- try initializing key weights to large values




TODO:
- retrace NN size expansion and addition of dropouts once success is found.
- reconsider turning noise off

- Fix defect (fault) when running inference on scenario 20 (can't find checkpoint file)
- consider adding logic to abort a LC mvr.
- make bots aware of vehicles in adjacent lane before initiating a LC mvr (esp at way different speeds).
- Add ability for agents to abort a LC mvr (both Bridgit and bots).
- add coreographed scenarios to routine training that force certain close call situations.
- consider using imitation learning. Create a bot (1c) that changes lanes to pass a slow forward vehicle. Then
  either:
	a) Pre-train the agent with supervised learning. Requires episode capture (like embed_collect) to capture a db
	   with full obs vector, actions and rewards. Then run training (like embedding) on this db comparing either full
	   episodes, short trajectory segments or single time steps.
	b) Intersperse imitation episodes with normal episodes in SAC RL training. ~10% of episodes would use the 1c bot
	   to produce the expert actions for a given time step, then adjust the full ego state to that result and repeat.
- consider rescaling obs element so that they all use the full range of [-1, 1] (e.g. speed limit).
- Convert Train checkpoints to msgpack format (see RLlib_notes.txt)
- Decouple BridgitGuidance.plan_route logic from reward structure (see handling of low max_prob near bottom). Env reward should be
  independent of model logic.
- Make guidance's plan_route() called from its step() method. No need for the env to worry about that separation of responsibilities.
- Refactor the vehicle configs to specify each vehicle type and the number of them.
- Try initializing NN weights for layer 1 where the most important inputs are to large values.

- Update plot for lane ID to show all lanes for roadway D
- update readme to reflect the multiple tracks
- Review TODOs in graphics.py
- graphics display the name of the roadway
- Display lane change as a gradual motion
- Investigate speed overshoot when speed limit changes (with bot1a controller)
- Investigate target markers initially having white border, then losing it after a car passes through.
- Add lane IDs to graphics
- Add checkpoint name to display.
- Review all TODOs
- Add ego sensor display to graphics showing pvmt type/speed colors, vehicle speeds
- Add graphic replay w/step fwd/back
- Greenfish icon editor Pro used for making the graphic icons.
