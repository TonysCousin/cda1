John's notes on development of the CDA1 project for the Part 2 goal: multiple agents using the same policy
----------------------------------------------------------------------------------------------------------

12/30/23

* Rearranged the project directory to make it more navigable.

1/1/24

* Performed routine system upgrade, including patches to the OS and an update of Ray from 2.5.1 to 2.9.0.
	* Ray changed their checkpoint API in v2.7, so now my saved checkpoints cannot be restored for
	  inference. Also, training doesn't save new checkpoints the same way, so I had to investigate
	  the new approach.

1/4/24

* Continued searching for a workaround to problems using the new checkpoint API. It seems to force me to
  use Ray's TorchTrainer, and its .fit() method, similar to Tune. This then requires resource allocation
  (num workers cannot be left to a default).
	* Specifying num_workers > 1 forces it to request more hardware than is available. It seems to 
	  want to assume 1 CPU and 1 GPU for each worker, but it's not clear where that is being
	  specified. My legacy resource specs in the AlgorithmConfig seem to be partially recognized.

* Run C1 training single agent BridgetNN with SAC from scratch for 30k iterations using 16 bots.
	* This should be identical config to run B5, which was run on 12/27/23 in part 1 of the project,
	  and resulted in the publication of the part 1 resulting model (however, that model cannot
	  currently be run because of Ray's problems reading the checkpoint). So this run should provide
	  a substitute model in case recovery is not possible.
	* The training still uses Algorithm checkpoints (although they seem a bit altered, as nowhere is
	  the checkpoint version ID stored), so hopefully restarting training from one of these will pick
	  up the optimizer state, etc, and allow smooth continuation of training.
	* Training avg performance was ~2070 steps/hr, utilizing 1 cpu and ~5% of the GPU. Final rewards
	  were rmin ~-2, rmean ~-0.4, rmax ~1.6.
	* Unable to run inference due to missing checkpoint content. It is looking for a file named
	  algorithm_state.pkl or checkpoint-[0-9]+, and none exists. The only file in the checkpoint dir
	  is the model*.pt. So it appears I am unable to work with Algorithm checkpoints from here on out.

* While C1 was running I cleaned up a bunch of TODOs in the source code, all except the graphics.py.

1/5/24

* Testing reading of the new checkpoints
	* Inference cannot read them.
	* Training program also cannot read them for the same reason: key files are missing. Tried the
	  .pt file itself as well as the two levels of directories above that.
	* Tried using the structure suggested in the error message: `trainer = TorchTrainer.restore(ckpt).
	  It loaded the checkpoint okay (top level dir), but assumed that the job had errored out, and
	  tried to pick it up from where it started. Since it had completed its specified number of iters,
	  it would not go any further.

	* Reverted back to the baseline (part2) method, not using the Train API.
		* All checkpoints are getting written on top of each other (same name).
		* Added logic to train.py to name the top level dir according to timestamp and put
		  multiple checkpoints underneath, all named according to iteration #.
		* This works for writing new checkpoints, for starting a new training session from one,
		  and for running inference.
	* Verified that inference on the saved part 1 checkpoint still doesn't work. It complains about
	  'is-atari' property in SACConfig.
		* The easily visible diff between these checkpoints is that the part 1 dir includes two
		  hidden files, .is_checkpoint (0 length) and .tune_metadata in the top level dir.
		* I removed these two files, but the error still occurs. Therefore, it is probably due to
		  some content in one of the .pkl files.
		* Still waiting on someone to respond to my post on Ray Discuss about this problem, so I'll
		  leave it alone for now and try to regenerate the model with a new run.
	* Applied a single-commit patch from the ray_train branch, that incorporated all the TODO cleanups,
	  to the algo_checkpoints branch. This is the one that works and can be carried forward.

* Run C2 training single agent BridgitNN with SAC for 30k iters from scratch using 16 bots.
	* Recent TODO cleanups have been applied.
	* New checkpointing with the old Algorithm approach (from pre-2.7 Ray).
	* Training completed with promising ending rmin ~-0.7, rmean ~0.5, rmax ~1.6 and mean ep len > 98.
	  Performance was 6200 ksteps/hr.
	* Inference on checkpoint 30000: Lane 0 starts won't change lanes. Others show pretty good speed
	  control, esp near the lane 0 merge. One start rear-ended a bot, not after a LC. It continues to
	  drive at 29 m/s on tail of lane 1 where limit is 33.5.
	* Inference on checkpoint 20000: Lane 0 starts don't change lanes. General tendency to drive a lot
	  slower than speed limits. This is definitely worse.

1/6/24

* Run C3 training single agent BridgitNN with SAC for additional 10k iters from C2 checkpoint 30000 using
  16 bots. Hoping that the previous model may overcome its single flaw of not changing lanes from lane 0.
	* Training completed, checkpoints are numbered for iterations in this run, not continuing on.
	* With mods to code for loading BridgitNN model, this becomes an old-style model, with the first
	  layer named "macro_encoder", whereas I prefer to name it "fc1" going forward. However, the code
	  cannot load models with different layer names embedded in them. 
	* Inference on checkpoint 10000: lane 0 start still doesn't change lanes! However,
	* A Lane 4 start chose to stay in lane 2 once achieved, rather than moving to lane 1, as all
	  previous models have done. In doing this, it sped up to 33.5 m/s on the tail, as I would have
	  wanted it to in lane 1. Log confirms the macro obs believes its SL here is 29.1, but it acts as
	  if these two lanes are reversed. Otherwise, its speed control is the best I've seen.

* Modified code to allow the Bridgit tactical guidance model (BridgitNN) to load pre-trained weights and
  run in inference mode to provide functionality for the BridgitGuidance.step() method. This will allow
  BridgitGuidance to be loaded into any of the non-ego vehicles.

* Confirmed that the accepted part 1 model (stored as cda1.1-B5-20000) still throws the "is_atari" error,
  and no word yet from the Ray team, so I may need to recreate it anyway.

* Debugged the problem noted above, where lane tail-end speed limits seem to be incorrect.
	* In addition to seeing the correct SL values in the inference logs for each time step, I re-ran
	  the embed_eval program over several snapshots from that region (none with ego in lane 2, as they
	  are apparently rare), and they all look correct - both the raw observation values and the
	  interpreted values coming out of the autoencoder.

* Run C4 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Revised the BridgitNN code so that its first layer is now named "fc1" instead of "macro_encoder",
	  which breaks compatibility with older checkpoints.
	* Modified reward function:
		1 Reduced the penalty for LC desirability poor choice; increased the factor from 0.3 to 0.4.
		2 Reduced normal lane change penalty mult from 0.002 to 0.0001. Both of these changes should
		  encourage the agent to take more risks in changing lanes.
		3 Reduced speed_mult from 0.06 to 0.05 for penalties for not following speed limit (to help
		  avoid episode rewards < -1).
		4 Increased mult for widely varying speed commands from 0.07 to 0.1 to encourage more
		  smoothness.
	* I killed the run after 5k iters since the speed variation penalty was still ridiculously low.

* Run C5 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Checkpoints are in dir 20240106-2006.
	* Revised reward penalty for speed variation - multiplier from 0.1 to 1.0.
	* Training progressed okay until 12000 iters, after which the rewards and episode length suddenly
	  tumbled and stayed at low values from then on.
	* Inference on one of the late iterations showed consistent desire to make immediate lane changes,
	  despite the situation. Inference on iter 12000 worked fairly well, but it had some speed control
	  problems and would change lanes out of lane 0.
	* I killed the run a little early.

1/7/24

* Run C6 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Checkpoints stored in 20240107-0534
	* Changed BotType1bGuidance so that its probability of lane change is reduced from 0.2 to 0.06,
	  in order to make it more lazy about the maneuver.
	* Made the bot guidance (both 1a & 1b) a little more conservative in their ACC logic.
	* Changed the global constants DISTRO_DIST_REAR, DISTRO_DIST_FRONT to be balanced at 200 m each.
	  They were previously 150 and 250, skewing the neighbor vehicles in front of ego. As ego often
	  starts at a low speed, most of the vehicles have passed it by by the time it reaches cruising
	  speed, so they are useless for the sensor training.
	* Training again failed to produce, so killed it after 19k iters. Progress was good through
	  ~14k iters, then the episode lengths dropped dramatically, and rmax went to ~-0.3.
	* Inference on checkpoint 13000 looks pretty decent, but still doesn't change lanes out of 0.

* Run C7 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Rolled back all the reward changes made for runs C4 and C5 above. Keeping the bot changes
	  made for run C6.
	* Checkpoints stored in 20240107-1231.
	* While this was running (at ~14k iters) the inference program crashed with a segfault several
	  times. Usually, this brings down the whole machine eventually, so it may start affecting
	  training results. However, training continued for some time afterwards...
	* Training continued through at least 21k iters, with rewards and mean episode lengths
	  gradually improving like they should. At this point rmin ~-1.5, rmean ~+0.1, rmax ~1.1
	  and mean ep len ~98.
	* Finally died of a segfault after 30200 iters, so the last checkpoint is 30000. At that time
	  rmin ~-1, rmean ~0.5, rmax ~1.5 and mean ep len ~99, training at 6100 ksteps/hr.
	* Inference on checkpoint 30000: still will not change lanes out of 0. Rear-ended bots in two
	  different tests. Otherwise, behavior is basically good.

1/8/24

* Run C8 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Reduced penalty for lane change - multiplier from 0.002 to 0.0002 (reintroducing C4 change
	  #2, but not quite as large a change.
	* Checkpoints stored in 20240108-0834.
	* Training completed in 14 hr (5900 ksteps/hr) with rmin ~-0.7, rmean ~0.5, rmax ~1.7, mean
	  episode len ~99.
	* Inference on checkpoint 40000: lane 0 start still refuses to change lanes. Other lanes
	  perform pretty well, however, twice it crashed while changing lanes coming up from behind
	  a slower neighbor in the left lane. In one episode, it rear-ended a bot (minimal speed diff).
	* Added logging to confirm that both the left & right center-lane boundary observations are
	  correct, as well as the lane change desirabilities, throughout an episode on lane 0. So it
	  simply is not learning how to handle these correct observations.

* Updated graphics to display a different icon for each type of vehicle (by guidance model), and to
  show the primary and secondary target destinations.

1/9/24

* Analysis of training lane assignments early in a training run. It includes 1800 iterations and 450
  episodes logged.
	* Initial lane distro is
		0: 106, 23.6%
		1:  26,  5.8%
		2:  19,  4.2%
		3:  70, 15.6%
		4: 107, 23.8%
		5: 122, 27.1%
	* Confirmed that the number of neighbors being used starts in the low range 0 to 3, and
	  gradually increases.
	* Confirmed that, for the lane 0 starts, speeds look evenly distributed across the full
	  range, and starting P locations also cover the full lane, realizing that the very early
	  ones are grouped closer to the end, and then gradually start backing up toward the beginning
	  of the lane.
	* In summary, all training conditions appear as designed, and nothing that should inhibit the
	  agent from having ample opportunity to learn the lane change required.
	* It could be that, since starts in lanes 3-5 all require left-hand lane changes, and lane 0
	  is the only one requiring a right-hand, the agent simply learns early that a left LC is
	  more important. Therefore, increasing the early opportunities to experience lane 0 might be
	  a benefit.

* Run C9 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Shifted the lane assignment probabilities for the early episodes so that it is now:
		0: 40%
		1:  5%
		2:  5%
		3: 15%
		4: 15%
		5: 20%
	* Increased penalty multiplier for speed variation from 0.07 to 0.1 (reintroducing C4
	  change #4).
	* Change the vehicle mix by adding a few Type1a bots to the mix.
	* Checkpoints stored in 20240109-1304.
	* Training completed in 13.5 hr with rmin ~-1.0, rmean ~0.5, rmax ~1.6, mean ep len ~99.
	* Inference on checkpoint 40000: rear-ended a bot with only a slightly slower speed.
	  Lane 0 starts still don't change lanes. Otherwise, similar behavior to before, not bad.
	  Occasionally, the speed variation penalty shows up at a significant size, but it is
	  rare enough that it probably doesn't have much impact on behavior; need it larger.

1/10/24

* Run C10 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Increased multiplier on speed variation penalty from 0.1 to 0.4.
	* Added flexible targeting, meaning that each episode reset has the opportunity to 
	  randomly select a different set of active targets from the list of valid targets given
	  by the user (in a config list). Valid targets can be a subset of the total list of
	  targets defined. These apply only to the ego vehicle; neighbor vehicle targets always
	  have the full list of defined targets available.
		* For this run, only target #2 is valid (the one in lane 2), thus forcing the
		  agent to learn more about right-hand LCs. Target randomization is off.
	* During early inference, discovered that the target mod didn't take effect, because
	  the reward function didn't get modified to only look at active targets. Therefore,
	  this training was using all 4 targets for the ego vehicle (different from all previous
	  runs, which only used targets 1 & 2).
	* Inference on checkpoint 29000: agent has learned to steer out of lane 0 sometimes.
	  However, since it trained with all 4 targets active, it sometimes steered _into_ lane
	  0 from lane 1 just to reach its endpoint! Giving it individual targets in inference
	  now is no good, since it hasn't learned too much about the meaning of the desirability
	  values, as they didn't change much in training, at least from episode to episode in
	  the same lane. Training with randomly chosen targets will provide it a much better
	  understanding of why those obs are valuable.
	* Speed changes were still erratic, so the penalty maybe needs to be bigger still.

* Run C11 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Fixed the reward function to only consider active targets.
	* Configured to randomize over all 4 targets for training.
	* Changed RoadwayB so that the ends of lanes 0 & 4 are marked as ASPHALT, which were
	  previously EXIT_RAMP. EXIT_RAMP is now not used.
	* Increased penalty for speed variation multiplier from 0.4 to 0.6, while reducing the
	  multiplier for speed limit deviation from 0.06 to 0.04 to keep from piling on large
	  penalties (C4 change #3).
	* Checkpoints are stored in 20240110-2217.
	* Run died early due to a segfault. Completed 15600 iters, at which point rmin ~-2.2,
	  rmean ~-0.2, rmax ~0.9, mean ep len ~91.
	* Inference on checkpoint 15000: does not change lanes out of lane 0. Speeds are a
	  little smoother, but still occasional large swings for no reason.

1/11/24

* Run C12 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Changed multiplier on speed variation penalty from 0.6 to 1.0.
	* Reduced penalty for making a poor choice on LC desirabilty from -factor to -0.3*factor.
***	* Updated RoadwayB to alter the track a bit:
		* Lengthened the exit ramps on lanes 0 and 4 to 200 m, so that they each have
		  100 m beyond their target destinations to provide conssitent sensor data
		  that gives the agent the impression of an on-going drive.
		* Changed the speed limits on the lane 3 segment 1 from 33.5 to 31.3 m/s and
		  on the lane 4 & 5 on-ramps from 20.1 to 22.4 m/s to give more variety of
		  training experiences.
	* Checkpoints stored in 20240111-0959.
	* After ~15k iters, rewards and episode lengths tanked. Inference on checkpoint 22000
	  shows rapid LC command, no matter where it is, until it runs off road. This is
	  apparently more desirable than the huge speed penalties it is receiving.

* Run C13 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Reduced speed limit penalty multiplier from 0.04 to 0.02.
	* Checkpoints stored in 20240111-1744.
	* Over the past several runs I have seen several crashes where agent changes lanes
	  just in front of a faster moving bot. Usually these occur late in the LC count,
	  indicating that when it committed to beginning the LC, the neighbor was probably
	  too far back to be seen. Since it has no way to abort an underway LC, the crash is
	  then inevitable. And the bot can't sense the ego vehicle's LC in progress until it
	  crosses the dividing line (step 8).
	* Training proceeded well through ~15k iters, at which point it showed rmin ~-2.1,
	  rmean ~-0.1, rmax ~1.2 and mean ep len ~89 steps. At that point performance began
	  to drop, oddly ending with similar rewards but mean ep len dropped to ~5 then
	  somewhat recovered to ~25 by the time it hit 40k iters.
	* Inference on checkpoint 15000: prefers to steer toward lane 1 even when there is
	  not an active target there. After several episodes, I saw one lane 0
	  start that changed lanes when no target exists in lane 0.
	* Inference on checkpoint 21000 (where mean ep len was worst): a lane 0 start
	  changed lanes into 1. Otherwise, every start did an immediate left turn off-road.
	* Inference on checkpoint 40000: not much different from 21000.
	* Inference on checkpoint 16000 with 100 step limit (like training episodes):
	  well-performing episodes only score -0.3 (no reward for completing the trajectory).
		* Lane 4 start changed to lane 3, as needed, and got a "poor" desirability
		  penalty of -0.9 for that!

1/12/24

* Run C14 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Eliminated reward bonus for complete episode (normally 100 steps) or for hitting
	  an active target, since those are so rare, and is outside the control of the
	  tactical guidance anyway.
	* Added an alert flag in the Vehicle class, which is now set if a poor LC desirability
	  choice is made (in the reward function), and displays on the inference graphics.
	* Changed LC desirability reward section to ony award for the single time step that
	  initiates the LC mvr (it was doing it for two consecutive steps), so that bonus
	  impact will be lower.
	* Halved the speed limit penalty multiplier from 0.02 to 0.01 to emphasize the value
	  of followiing lane desirability and speed smoothness.
	* Checkpoints stored in 20240112-1811.
	* I killed the run after 9k iters, in favor of a better reward scheme (next run).

* Run C15 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Changed reward design for lane change desirability, providing a keep-alive bonus for
	  every time step that is proportional to the desirability of that lateral decision.
	* Checkpoints stored in 20240112-2129.
	* Training completed with rmin ~-1 (many iters had rmin > -1), rmean ~0.6, rmax ~0.95
	  and mean ep len ~94 (never an iter with mean ep len > 98).
	* Inference on checkpoint 32000: way too many lane changes; several of them are
	  reckless, causing sideways crashes, even into bots it should see just off the front
	  bow. Speed selection is good, but still noisy.
	* Inference on checkpoint 40000: same as above with additional notes. Extraneous LCs
	  happen periodically back and forth between two of the long lanes, usually with 2-4
	  steps of rest in between mvrs. This induces a frequency penalty about half as large
	  as the max desirability bonus. Even egregious speed deviations are now only getting
	  penalties ~0.005, which is half the desirability bonus for a time step. The worst
	  penalties for speed variation are ~0.003, and usually a lot smaller.

1/13/24

* Run C16 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Increased reward penalty for frequent lane changes; mult from 0.0002 to 0.002 (as
	  it was prior to run C4).
	* Changed reward penalty for speed command variation from a square relationship to a
	  linear one, with multiplier reduced from 1.0 to 0.5, hoping to give more motivation
	  to tame the smaller variations, while not being severe in the occasional large ones,
	  which are needed occasionally.
	* Reduced speed deviation penalty mutliplier from 0.01 to 0.006 to keep from over-
	  burdening the agent with piles of penalties.
	* Checkpoints stored in 20240113-1638.
	* Training killed by OS at 16800 iters due to some file system fault.
		* Rebooted and resumed training from checkpoint 16000.
		* The resumed training got to 7000 additional iters, but by then mean ep len
		  had dropped to 4 steps. This appears unstable and not worth continuing.
		* Killed it.

1/14/24

* Run C17 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Reduced LR for policy & critic from 3e-5 to 2e-5.
	* Checkpoints stored in 20240114-0051.
	* After 29k iters, it has clearly not learned well. rmax ~-0.8 and mean ep len ~6.
	* Inference shows a strong desire to change lanes regardless of the circumstances. It
	  also seems to like very low speeds if it lasts long enough.
	* Killed the job at 31k iters due to poor learning.

* Run C18 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Reduced speed variation penalty multiplier from 0.5 to 0.2.
	* Changed reward bonus for LC maneuvers. Instead of a big lump at the beginning of the
	  mvr, which may be encouraging frequend mvr initiation, spread out the desirability-
	  based bonus on every time step in the maneuver.
	* Changed resource allocation: num_gpus from 0.2 to 0.5 (for the local worker doing the
	  learning algo) and num_gpus_per_worker from 0.2 to 0.1 (for 4 remote workers).
	* Checkpoints stored in 20240114-1129.
	* Killed it after 26k iters, as I discovered a defect in the reward code that
	  discouraged it from changing lanes.

* Run C19 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Fixed reward defect (wan't giving rewards during LC maneuvers).
	* Checkpoints stored in 20240114-2232.
	* Training progressed well through 33k iters, at which point I evaluated.
	* Inference on checkpoint 33000: it has little respect for speed limits, often going
	  much slower, probably for accumulating more LC des points. Another quirk is that it
	  tends to choose a poor LC that avoids a target, allowing the LC des bonus to diminish
	  for a while, but once no target is achievable, that bonus goes back to 0.01 per step
	  for the remainder.

1/15/24

* Run C20 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Changed LC des bonus so that it does not award a significant value when the agent is
	  on a no-win lane (target unreachable), to discourage taking this path just to collect
	  points.
	* Increased reward penalty for deviation from speed limit; multiplier from 0.006 to 0.008.
	* Modified ACC logic for both bot 1a and bot 1b guidance to be more conservative, to
	  avoid more rear-end collisions.
	* Checkpoints stored in 20240115-1017.
	* Training went well through 38k iters (where I evaluated), at that point with rmin ~-1,
	  rmean ~0.2, rmax ~0.5, mean ep len > 97.
	* Inference on checkpoint 38000: terrible speed, loving the very low values, esp before
	  doing a lane change. Prefers to change lanes from 4 to 3 even when there is a tgt in
	  lane 4 and the desirability vector prefers staying in lane 4. It refuses to change
	  lanes out of 0 when the nearest target is in 1 or 2. Twice it rear-ended a bot after
	  following it for some time (not large speed diffs).

* Run C21 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Changed speed penalty from a linear to a square relationship, with multiplier of 0.25.
	* Changed LC desirability reward from linear to a square relationship to help deter it
	  from choosing a non-optimum lane.
	* Added data element to obs vector for distance to forward vehicle in previous time step.
	  This should allow the agent to better judge the collision risk by sensing its relative
	  speed.
	* Checkpoints stored in 20240116-0006.
	* Training completed with questionable performance. rmax did okay, climbing to ~0.3, but
	  rmin and rmean didn't climb as in previous runs, ending ~-3, -0.8, respectively, with
	  mean ep len topping out ~80 at 11k iters, then gradually dropping to ~50 at the end.
	* Inference on checkpoint 33000: it somewhat attempts to follow speed limit (better than
	  C20), but still erratic. LC decisions are befuddling. Lane 0 starts won't change lanes
	  to get to a valid target, but lane 4 starts will change to lane 3 even when there's a
	  target in lane 4. It seems to have a heavy bias toward moving left whenever possible.
	  Speed penalties are pretty stiff (often 0.01 or more), and it doesn't seem to be
	  trying to avoid these. It still does random off-roads or crashes into neighbors, either
	  rear-end or adjacent lane.

1/16/24

* Run C22 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Added more explanation details to the LC des reward to show the full desirability vector
	  at each step, to help ensure there is not a defect somewhere.
	* Added modest reward (0.1) for completing a 100-step episode.
	* Changed all 3 learning rates to 1e-5 (was 2e-5 for actor & critic and 5e-5 for entropy).
	* Checkpoints stored in 20240116-1301.
	* Killed it early after finding a code defect in BridgitGuidance.

* Run C23 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots
	* Fixed defect in BridgitGuidance assigning unreasonably low lane desirabilities in some
	  cases.
	* Changed env _choose_active_targets() to generally lean toward a small number of targets
	  most of the time.
	* Checkpoints stored in 20240116-1457.
	* Training completed in 13.4 hr with ending rmin ~-3.4, rmean ~-0.6, rmax ~+0.5 and mean
	  ep len ~99.
	* Inference on checkpoint 40000: lane 0 start seems willing to change lanes for a target
	  to the right! However, starts in lane 4 & 5 don't want to change. It seems the LC des
	  bonus is usually being swamped by the speed penalties and sometimes the speed variation
	  penalties. I also see that an episode completion bonus (0.1) is awarded for a trajectory
	  in lane 1 that goes to the no-win zone (too far to reach the target in lane 4), thus its
	  max lane desirability is only 0.01. A trajectory that gets into this situation should be
	  heavily penalized.

1/17/24

* Run C24 training single BridgitNN agent from scratch with SAC over 50k iters using 16 bots.
	* Changing episode completion reward to only apply the bonus if it ends in a state that is
	  still viable to reach a target; otherwise it gets a big penalty.
	* Reduced speed deviation reward multiplier from 0.25 to 0.06 so that it is only a secondary
	  influence.
	* Removed penalty for speed variation, just to keep the picture clean and focused on lane
	  changes for now. May reintroduce this later.
	* Checkpoints stored in 20240117-1124.
	* Training completed in 16.9 hr (6.1 M steps/hr). Rewards and ep len plateaued early and
	  didn't change much after ~22k iters, where rmin ~-1, rmean ~0.3, rmax ~+1 and mean ep len
	  ~98. However, rmean seemed to improve a bit between there and the end, showing more
	  occurrences of ~0.5.
	* Inference on checkpoint 50000: it seems the agent is always aiming for an episode reward
	  (100 steps) close to 0, but afraid to go much above that. If LC des bonuses are good, it
	  chooses really low speeds to counteract the bonuses; if LC des is 0 because no target is
	  reachable, then it does really good speed control. It is willing to change lanes out of 0,
	  but once it did that despite the only target being in lane 0.
		* It has a strong desire to change lanes left regardless of the target location.
		* It also seems to not care about neighbor vehicles, at least in adjacent lanes,
		  because it often crashes during a lane change into a neighbor going roughly the
		  same speed, so blindness isn't the problem. Maybe the vehice sensor compression
		  was too much?
	* Inference on checkpoint 24000: performed a lot worse than the 50000 model. It really 
	  likes lower speeds, and I never saw a lane change.
	* Inference on checkpoint 37000: sometimes speeds are quite steady, but often way too low.
	  No lane changes observed.
	* Given the 3 checkpoint behaviors, maybe it just hasn't had enough training. It seems to
	  be moving in the right direction, so let's give it more time.

1/18/24


* Run C25 training single BridgitNN agent from run C24 checkpoint 50000, with SAC over additional
  50k iters using 16 bots.
	* Checkpoints stored in 20240118-1308.
	* Training completed in 16.9 hr. Rewards generally improved to rmin ~-0.9, rmean ~0.4,
	  rmax ~0.9, mean ep len ~99.
	* Inference on checkpoint 50000 (100k iters & 102M steps of total training): mostly not
	  interested in changing lanes, except near end when it does an illegal off-road. Speeds
	  are mostly well below speed limit. Nothing good here.  Run C24 was probably a little
	  better.

1/19/24

***
* Considerations for continued work, given the lack of success so far. I'm not even doing the
  intended part 2 work yet, still trying to reproduce the results of part 1 since that checkpoint
  can't be read with the new Ray library.
	1. Drop back to Ray 2.5 to work part 2.
		This feels short-sighted, because any new results will need to be migrated to
		2.9+ eventually.
	2. Rerun the final part 1 code under Ray 2.9 to generate a new checkpoint that is forward
	   compatible.
		May be worthwhile, just to prove that it wasn't a one-time fluke. It would also
		provide a baseline for comparison of all changes since then.
	3. Review how lane changes out of 0 were handled in part 1.
	4. Alter NN shape a bit.
		Easy to play with, but will take time for probably several trials, and no
		confidence that anything will improve.
	5. Add dropouts to the NN.
		Easy to do, one or two trials should give results.
	6. Re-architect the whole NN - add recursion and/or conv layers.
	7. Adjust noise params.
	8. Much longer training from scratch.
	9. Adjust learning rates (larger?).
	10. Change the sensor encoders for more accuracy or smaller size.
	11. Remove all the sensor observations and replace them with just a couple simple inputs.
	12. Abort the whole idea of using a NN for tactical guidance & replace with a pid. Then I
	   can focus on the meat of the bigger project, by building a NN for strategic guidance
	   and move into cda2 to add messaging to its actions.

* Run C26 training single BridgitNN agent from scratch, with SAC over 80k iters using 16 bots.
	* Expanded BridgitNN size (added layer 4, increased sizes of layers 1 and 2), and added
	  20% dropout to outputs of layers 1-3. This implements ideas 4, 5 & 8 from above.
	* Checkpoints written to 20240119-1637.
	* Died after 21k iters due to Ray error.
	* Restarted from checkpoint 21000 - completion job is at 20240120-0843.
	* Training ran to 77k iters after the restart (98k iters total), leveling off at
	  rmin ~-0.9, rmean ~0.4, rmax ~1.0, mean ep len ~100. This pattern started around iter
	  20000 of the restart (41k total). Still performing at 6.1M steps/hr.
	* Inference on -0843 checkpoint 77000: no lane changes ever. Speeds tend to hang close to
	  18-20 m/s in all situations. No good.
	* Inference on -0843 checkpoint 20000: same performance. Speeds are quite smooth and
	  steady, just at the wrong value. It seems the larger network and/or the dropouts have
	  made outputs a lot smoother, but maybe overly so (i.e. not interested in taking on new
	  values when warranted)?

1/21/24

* Run C27 training single BridgitNN agent from scratch, with SAC over 80k iters using 16 bots.
	* Changed reward for LC des to increase exponentially with each additional time step
	  (built for a limit of 100 time steps per episode).
	* Changed reset() to uniformly initialize agent's P coord along the lane's length, 
	  regardless of how many episodes have completed. It used to force all starts to beginning
	  of the lane after early episodes, so lots of the 100-step episodes never saw the full
	  LC experiences!
	* Changed reset() to not initialize speed < 10 m/s, since to many starts have been down
	  at those ridiculously low speeds, and cause unnecessary start-up crashes.
	* Checkpoints stored at 20240121-1323.
	* Training completed in ? hr. rewards didn't climb as much as before, and mean ep len
	  never reached 100. It hit low 90s after ~13k iters, then after 21k it started to drop
	  off a bit, ending ~81. rmean was rather noisy, but seems to be drifting upward from 12k
	  iters onward, ending ~-0.2, while rmax also climbed slightly, ending ~0.9.
	* Inference on checkpoint 30000 with target only in lane 2: starts in lanes 0 & 1 never
	  change lanes. Start in lane 3 changes lanes left eagerly, all the way to lane 1 (beyond
	  the target lane). Lane 4 & 5 starts change lanes nicely also, but all the way to lane 1.
	  Never saw a right lane change. In lane 0 the LC cmd never came close to the 0.5 thresh.

* Run C28 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots.
	* Changed BridgitGuidance so that LC desirability never exceeds 0.9 for a lane that doesn't
	  include a target, regardless of how far away it is.
	* Changed reward for LC desirability so that if desirability < 0.2 there is a penalty
	  rather than reducing the bonus to near zero.
	* Increased LC des bonus multiplier (A) from 0.003 to 0.004.
	* Checkpoints stored in 20240122-0020.
	* Training completed with results very similar to C27. Highest mean ep len was 95 steps.
	* Inference on checkpoint 20000: lane 4 & 5 starts do pretty well on lane changes, although
	  they tend to happen later than ideal. Lane 0 & 1 starts (with target in lane 2) never move
	  to the right. Not concerned with poor speed performance now.

1/22/24

* Run C29 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots.
	* Removed the tanh nonlinearity from BridgitNN layer 4 (left it linear).
	* Checkpoints stored in 20240122-1411.
	* Training completed with steady results since ~12k iters. rmin ~-2.5, rmean ~-0.7 noisy,
	  rmax ~1, mean ep len ~85.
	* Inference on checkpoint 30000: it is reluctant to change lanes. It does so sometimes from
	  lanes 3 & 4 (for lane 2 tgt), but never from lane 5, 0 or 1. Never saw it change lanes right.
	  Speeds are always way low, but not worried about them now.

1/23/24

* Run C30 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots.
	* Restoring the tanh nonlinearity back to layer 4 of the BridgitNN.
	* Changed NN dropout from 0.2 to 0.1.
	* Reordered completion logic in get_rewards() to deal with dones before dealing with inactive
	  vehicles, which had prevented it from seeing complete episode rewards.
	* Fixed defect in ACC logic for bots 1a & 1b to allow it to handle jittery forward vehicles
	  better, thus avoiding some rear-end crashes that may be providing artificially negative
	  training experiences (not the agent's fault).
	* Overrode step() to force speed command to always be 28 m/s, so that it will not have to
	  worry about learning that channel also, and can focus on learning proper LC commands.
	* Checkpoints stored in 20240123-1243.
	* Training completed in 10.1 hr. As before, it never reached desirable performance. rmin
	  ~-1.9, rmean ~-0.1, rmax ~1.4, mean ep len peaked at 85 then diminished to ~71 by the end.
	* Inference on 30000: forcing a constant speed sometimes causes agent to rear-end slower bots,
	  which will affect its training, giving it artificial penalties in some cases.
		* Lane 0 start changed lanes! then changed several more times between 1 & 2 (tgt only
		  in lane 2). In additional runs, it never changed again once hitting lane 1.
		* Lane 1 start stayed in lane 1. Starts in higher lanes all move over to get into
		  lane 1, despite negative LC des rewards.
		* The difference in LC des reward between a lane with des = 1 and a lane with
		  des = 0.67 is not that much (about 70% reduction), and probably should be more
		  drastic to help discourage stayiing in such a lane long term. 

1/24/24

* Ran the train program with extra logging to ensure that it is randomizing the initial conditions
  well and that reasonable trajectories are getting executed. All looks good. Initial lane choices
  are heavily weighted to lane 0, and otherwise spread according to the probability distribution
  coded. Target lists seem uniformly distributed, and mostly contain only 1 target. Step method
  shows many early cases of lane changes both right and left, although left is more commont
  (probably because there are more opportunities to do so).
	* May want to avoid lane 0 starts when only target 3 is active, since it is unreachable.

* Run C31 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots.
	* Removed the artificial holding of all speed commands; allow NN to learn them again.
	* Changed reward for LC des to be harsher on desirabilities < 1 by squaring it.
	* Also made the penalty for a terrible choice to grow exponentially. Changed both exponential
	  growths to start over with each lane change rather than counting steps since beginning of
	  episode.
	* Added reset() logic to prevent starting the ego vehicle in any location that is out of reach
	  of at least one target.
	* Checkpoints stored at 20240124-2038.
	* Training completed, but no good. Mean ep len touched 90, but settled in the mid-80s.
	  rmin ~-7, rmean ~-1.3, rmax ~0.9.
	* Inference on checkpoint 30000: speed is usually really low. Lane changes are good from lanes
	  3, 4, 5. Lane 0 starts will change lanes to 1 if in the merge zone before speed has dropped
	  a lot, but if it starts near beginning, speed drops before reaching the merge zone, and it
	  doesn't bother to change lanes. Even so, once in lane 1 it won't go to target in lane 2.
		* Noted that the LC des penalty for terrible choice does not increase exponentially,
		  as intended. Found that it is a function of steps since LC, which is capped at 60
		  steps; needs to grow unlimited.

1/27/24

* Created git branch update_roadway to generalize the Roadway class as a polymorphic interface.
	* Created class RoadwayB to represent the heretofor used roadway geometry. Ensured the train
	  and inference programs worked okay with this new refactoring.
	* Created class RoadwayC defining a brand-new track with 6 parallel lanes that each have several
	  speed limits along their length. It is symmetrical to avoid any preference for right- vs left-
	  turn learning. Adjusted the graphics somewhat to make a pleasing display of this one also.
	* Created class RoadwayD defining a brand-new track with 7 parallel lanes that each has a
	  different speed limit, but is constant through its length. Speed limits vary symmetrically
	  right and left.

1/28/24

* Completed enhancement to make roadway randomly selectable (between C and D) during setup of each
  episode.
* Fixed an apparent problem in the HpPrng where it tended to provide correlated sequences. Each time
  it was called by reset() to select a roadway model it alternated very regularly between the two
  choices. I added an occasional extra iteration of seed calculation within the Prng to avoid this.

* Run C32 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots.
	* Running 50/50 on roadways C and D only.
	* Includes updated PRNG, per previous bullet.
	* Checkpoints stored in 20240128-1315.
	* Inference on checkpoint 25000 for a couple episodes. Besides being really slow, it doesn't
	  seem to know how to steer toward a target.

* Fixed HighwayEnv reset() to properly manage Roadway objects when it is called multiple times.
  This was causing a graphic display problem when inference would make multiple reset() calls and it
  chose to select a different roadway type in each call (map coordinate frames are different).
	* This error possibly screws up results of C32 training, which was underway at the time.

* Run C33 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots.
	* Increased reward multiplier for speed deviation from 0.06 to 0.25, reversing the change
	  made in run C24 to allow focus on lane changes. Now that the two new straight roadways
	  are in place, and provide richer training ground for speed control, and a symmetrical
	  place to learn lane changes, there is no reason to ignore speed control.
	* Fixed reset() to properly handle memory of the roadway object when called multiple times
	  (see above bullet), which probably screwed up results for C32.
	* Checkpoints are stored at 20240128-2320.
	* Training completed. From ~10k iters onward mean ep len was in mid-90s, but tailed off to
	  ~90 at the very end. rmax rose quickly to ~1 and stayed there until ~10k iters, then
	  slowly wavered, but reached consistent ~1.4 around iter 27k, then dropped into negatives.
	  rmean wavered in [-3, -1] and rmin never exceeded -6.
	* Inference on checkpoint 16000: no lane changes, but speed control is quite steady, just
	  slow.

2/1/24

*** Realized that training has been discouraging a couple key behaviors, namely following the LC
  desirability inputs, and driving as fast as the speed limit suggests. For the lane changes,
  the reward is spread across the full 15 steps of the LC maneuver, using the reward value earned
  just before the LC, which is low enough to push the agent away from it; it has to look 15 steps
  into the future to realize the improved reward for initiating the maneuver, which is a lot to
  learn without a recurrent layer.  For the speed problem, the training episodes have been allowed
  to begin up to 150 m in front of the end of lanes, which means that many of the episodes will
  either run off the end of the lane (big penalty), or it learns to slow way down to get its 100
  steps in before hitting the end!

* Run C34 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Changed reset() to set ego initial speed as a Gaussian distro around where it is normally
	  expected to operate (near 29 m/s), in order to get it used to driving that fast right
	  away, giving it a better chance of success. Sometimes speed will be initialized lower,
	  to give it a full range of experiences, however.
	* Changed reset() to limit initial location to be no closer to the end of a lane than 700 m
	  (for lanes that are long enough), so that it isn't forced to run off the end early in
	  training, thus learning to simply slow down as a preventive measure.
	* Increased termiinal reward for completing episode with a decent LC desirability from
	  0.1 to 0.5.
	* LC desirability reward already seems to be assigning the to-be lane's bonus, so no
	  change for now.
	* Checkpoints are in 20240201-1955.
	* Run died after 11200 iters. Rebooted then started a new job from checkpoint 11000, which
	  gives an error about priority = 0 in the prioritized_replay_buffer, even though the
	  checkpoint was successfully loaded. Started a new job from checkpoint 10000.
		* Continuing checkpoints stored in 20240202-0103.
	* Training completed the new set of 30k iters. Typical rmax ~1.6, but rmean varied slowly
	  in [-1, +0.8] and rmin in [-12, -3]. Mean ep len consistently ~98.
	* Inference on checkpoint 9000 showed no lane changes, and speed mostly held ~20 m/s.
	* Inference on checkpoint 28000: speed control follows speed limits fairly well now, with
	  a bit of positive bias, but penatlies aren't very big. It slows to avoid rear-ending bots
	  in front. Lane changes happen sometime, and sometimes not good choices.

2/2/24

* Verified through added logging, that the replay buffer seems to be working as expected, taking in
  random steps until the Exploration object is done randomizing, then serving up experiences when
  asked.

* Run C35 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Fixed a defect in BridgitGuidance that was allowing possibly incorrect LC des calcuations
	  for the first 5 time steps of each episode.
	* Adjusted replay buffering: reduced capacity from 1M to 100k, and reduced exploration
	  initial random timesteps from 1M to 10k so that it can more quickly start learning from
	  some positive examples.
	* Checkpoints stored in 20240202-1331.
	* Training completed. Mean ep len quickly reached 98-100 and stayed there. Similarly, rmax
	  reached ~1.8 and stayed there. However, rmax grew to ~0.3, maxing out at 0.5 at 12k iters,
	  then dropped to < 0 for the remainder. rmin started ~-4, but gradually moved down to ~-10.
	* Inference on checkpoint 12000: never a lane change, regardless of desirability. Speeds
	  were mostly in [20, 22] regardless of speed limit, but it was very smooth.
	* Inference on checkpoint 30000: similar behavior, only speeds tended to be even slower,
	  sometimes < 9 m/s for a whole episode.

2/3/24

* Merged update_roadway branch back into the part2 branch. Then created new branch lat_motion
  to work on more sophisticated lateral movement logic.

* Run C36 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Changed exploration initial random timesteps from 10k to 100k so it fills the buffer.
	* Changed bot1b guidance to have slightly slower speed range (most of them will be
	  slower than the speed limit), to further stress the agent's ACC and encourage it to
	  do a passing maneuver if travel time becomes a goal.
	* Changed bot1, Bridgit & embed models to recognize neighbors changing lanes from an
	  adjacent lane into the bot's lane as candidates for nearest forward vehicle (input to
	  the ACC algo in the bots and embed guidance), which will help it avoid rear-ending
	  slower vehicles changing lanes in front of it.
	* Changed reward penalty for crashes to be more selective. It gives a -3 points for
	  rear-ending a neighbor or changing lanes into a neighbor who should be easily
	  visible, but no penalty for crashes caused by the neighbor or by moving into the
	  path of a fast-moving neighbor that ego couldn't see. It also gives -3 points for
	  any undetermined crash reason.
	* Checkpoints stored in 20240203-2301.
	* Training completed. Similar story as recent runs, with mean ep len ~98 early, rmax
	  ~1.7 early, but rmean barely reached > 0.
	* Inference on checkpoint 30000: speeds are typically very slow, and sometimes change
	  inexplicably. Never see a lane change.
	* Inference on checkpoint 10000: similar performance. Agent rear-ended a bot without
	  even attempting to slow down a bit.
	* Realized that LC des bonuses being keyed to steps since LC are encouraging agent to
	  stay in lane, since bonuses are bigger there than when a change occurs.

2/4/24

* Run C37 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Changed BridgitGuidance route planning to not normalize the LC desirabilities. In
	  cases where none of the 3 lanes hold a target, it needs to see desirabilities < 1
	  so it knows to get out of there.
	* Changed reward for LC desirability to grow exponentially from beginning of episode,
	  regardless of LC activity (was resetting the exponential growth aftere each LC).
	* Doubled speed deviation penalty multiplier from 0.25 to 0.5.
	* Checkpoints stored at 20240204-1822.
	* Training completed. Mean ep len reached 95 in 2600 iters, then stayed > 90, mostly
	  > 96, sometimes hitting 100. rmax reached 1.7 at 9600 iters, then stayed > 1.4,
	  mostly in 1.7 range. However, rmin started ~-8 and gradually got worse, ending in
	  the -20s. rmean then started ~-3, and after 8000 iters hung out in -1 to -2.
	* Inference on checkpoint 10000: no desire to change lanes. Speeds hang out ~19 m/s
	  regardless of speed limit, accepting pretty large penalties.

2/5/24

* Merged the lat_motion branch back into part2.

* Enhanced the inference program (& graphics) to allow manual override of the command vector
  (action output of ego's NN) for a single time step. This allows investigating behavior &
  rewards in lane change scenarios. Used this tool to see that LC rewards were working against
  my desires, so made some improvements.

* Run C38 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Tweaked reward penalty for LC terrible choices - multiplier from 0.02 to 0.004,
	  since it was producing crazy big values.
	* Changed LC des reward to accumulate it exponentially even during the LC
	  maneuver, so that cumulative rewards are bigger for making a good LC decision
	  than for staying in lane.
	* Tweaked BridgitGuidance route planning to start degrading desirability values
	  farther uptrack of where the LC is required to avoid two lanes showing 0.9 for
	  a long time, making the choice more difficult. It also forces a strategic replan
	  immediately after a LC completes so that the new desirability values are available.
	* Reduced multiplier for LC frequency penalty from 0.002 to 0.0002, since some of
	  the scenarios in the new roadways may require several rapid lane changes to reach
	  the target.
	* Enhanced inference a bit to allow manual override commands to apply for multiple
	  steps.
	* Checkpoints stored in 20240205-2217.
	* Training completed. Performance profile similar to before. Mean ep length stayed
	  high throughout, but rmax began to drop a lot after ~26k iters. rmean touched
	  above -1 at ~14k iters briefly, then more solidly ~24k iters (reaching -0.7).
	* Inference on checkpoint 24000: one run started near where it needed to LC soon,
	  and LC desirability values, even for the best lane, were already low (0.23), so
	  it immediately started getting penalized for all possible choices. No lane changes
	  happening. Speeds still hanging out ~18-20 m/s, regardless of speed limit. It
	  appears that it has not explored at all, or has somehow found any other actions
	  to be bad choices.

2/6/24

* Added logging to MultiAgentReplayBuffer to confirm that the pre-populated random experiences
  did have what appear to be a uniformly random distribution of both actions. So it seems that
  the agent is initially exposed to a wide range of experiences & rewards. I'm starting to
  wonder if there are too many neighbor vehicles that prevent it from taking big actions, for
  fear of inducing a crash.

* Run C39 training single BridgitNN agent from scratch, with SAC over 30k iters using 4 bots on
  roadways C & D.
	* Changed the vehicles config file to only include the ego and 4 bots.
	* Tweaked reward for LC des to apply a bonus if des (which is desirability^2) > 0.04,
	  where it was previously > 0.2.
	* Checkpoints stored in 20240206-2025.
	* Training completed with similar reward/ep len profile as before, but rmean got a
	  little higher for a little longer, around 26k iters, reached -0.3 for some time.
	* Inference on checkpoint 26000 shows desire to drive a little faster (~24 m/s), but
	  still no lane changes.

2/7/24

* Run C40 training single BridgitNN agent from scratch, with SAC over 30k iters using 1 bot on
  roadways C & D.
	* Replaced vehicle config file with the 1n file so it is training with only a single
	  neighbor vehicle.
	* Checkpoints stored 20240207-1100.
	* Training completed. Reward picture was somewhat better, with rmin ~-4 and rmean
	  ~-0.3 and normally mean ep len = 100 for the last 15k iters.
	* Inference on checkpoint 30000: speeds are definitely higher here, up to 27 m/s, but
	  still won't match the highest speed limits. And still no lane changes. Note that
	  there is no penalty for abrupt speed changes, but it does seem reluctant to change
	  when the speed limit hits a step change.

* Run C41 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Back to using the full 16 bot vehicle config, since that clearly isn't the only
	  thing preventing the agent from driving at full speeds.
	* Changed BridgitNN to use leaky_relu as the activation for all except layer 4,
	  which still uses tanh, since its output must be constrained to [-1, 1].
	* Checkpoints stored at 20240207-2149.
	* Training completed in 9.9 hr. Mean ep len reached 98 by 9k iters and stayed there
	  (often hitting 100) for the remainder. rmax ~1.4, rmean ended ~-1.5, but held
	  ~-0.7 for several hundred iters around 12k. At this point, rmin peaked ~-4 also.
	* Inference on checkpoint 12000: agent rear-ended a bot with 1.5 m/s speed diff in
	  same lane. No lane changes, and speeds are steady & slow (~22 m/s).
	* Ego was rear-ended by bot 5, with a speed diff of only 1.9 (bot was coming from
	  adjacent lane with no other bots in its forward view).

2/8/24

* Run C42 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Changed all 3 LRs from 1e-5 to 1e-4.
	* Changed replay buffer capacity from 100k to 1M.
	* Checkpoints stored in 20240208-1007.
	* Training completed. Mean ep len only maxed out at 98, but mostly stayed ~95, dropping
	  into the 70s near the end. rmax stayed ~1.6, and rmin ~-3. rmean peaked slowly ~+0.2
	  around 19k iters (stayed there for quite a while). Total time 10.4 hr.
	* Inference on checkpoint 19000: lane 4 start on roadway C shows rapid LC between 4 & 5
	  repeatedly, and each time it gets a new speed limit, the speed commands swing from one
	  extreme to the other in attempt to accelerate as rapidly as possible to the new desired
	  speed. Other lane starts acted similarly. On one it drove off the side of the road, too
	  eager to change lanes as often as possible. The LC frequency penalties were at most
	  equal to the speed penalties being levied every time step, so not really noticeable.

* Run C43 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Changed all LRs from 1e-4 to 7e-5.
	* Changed batch size from 1024 to 256 to capture more variance between batches; adjusted
	  rollout_fragment_length from 32 to 8 accordingly.
	* Slightly reduced the speed deviation penalty multiplier from 0.5 to 0.4 so it will
	  relax a little.
	* Increased penalty multiplier for LC frequency from 0.0002 to 0.0004.
	* Reduced LC desirability bonus by 0.3 if it is not choosing the best lane.
	* Checkpoints written to 20240208-2254.
	* Training completed in 9.2 hr. Mean ep len ~98, but never hit 100. rmax ~1.6, rmean
	  very slowly leveled out ~-1.1, and rmin ~-5.
	* Inference on checkpoint 30000: no lane changes, but new LC des rewards seem to be
	  working as intended. Speed control is not very good either - unresponsive. Playing
	  with injecting LC commands, I see rewards aren't always proportional to the best choice
	  available. When all 3 desirabilities are low (tgt is several lanes over), then rewards
	  are small, even though it is doing the best choice available (a single LC).
	* Realized there was a typo in 2 of the LRs, where they were e-7 instead of e-5!

2/9/24

* Run C44 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Changed LC desirability reward to be based only on the relative desirabilities of each
	  of the lanes available, rather than on the desirability value itself, so rewards now
	  grow consistently, independent of where we are on the track or where the target is, if
	  it is not in an immediately available lane.
	* Fixed the two erroneous LRs, so all 3 are now 7e-5.
	* Checkpoint stored in 20240209-1247.
	* Training completed in 9.4 hr. Performed much better than previous. Mean ep len never
	  reached 100, but stayed in mid-90s the whole time. rmax ~1.7 consistently. rmean
	  gradually increaased to as high as 0.97, but mostly hovered ~0.8. rmin touched -1
	  a couple times, but mostly ~-3.
	* Inference on checkpoint 29000: lane changes do quite well, even multiple back-to-back
	  to reach a target lane. I saw one extraneous LC, but it was immediately corrected.
	  Speed control needs work. It's not bad in the mid range, but can't reach the 33.5
	  speed limit, although in other times it readily commands speeds of 35+ in order to
	  accelerate rapidly. speed cmds tend to fluctuate greatly. It does well at avoiding a
	  rear-end collision.

* Run C45 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Increased LC frequency penalty multiplier from 0.0004 to 0.0005.
	* Increased speed deviation penalty multiplier from 0.4 to 0.5.
	* Reintroduced the speed variance penalty with multiplier of 0.005 so that it will
	  start out with a minor impact.
	* Checkpoints stored in 20240209-2225.
	* Run died after 18200 iters. At that point, we had mean ep len ~94, rmax 1.6, rmean
	  ~0.3, rmin ~-3. Apparently, checkpoint 18000 included replay buffer content with 0
	  priorities, so can't be used. Restarted from checkpoint 17000.
		* Continuing checkpoints are stored in 20240210-0855.
	* Training completed in 9.4 hr. Mean ep len was in high 90s early, but gradually
	  diminished o mid-80s by the end. rmax ~1.7 pretty consistently. rmin stayed ~-3 with
	  occasional excursions to -1. rmean consistently in [0.4, 0.7] after 10k iters.
	* Inference on checkpoint 10000: lots of extraneous LCs, but hit target. Speed are a
	  little noisy, but follow speed limits well. Penalties for speed cmd variation are
	  small & need to be ~4x larger to be noticed. Start in lane 5 with tgt in lane 0 it
	  quickly moved over to 0 and stayed there! It consistently hits the targets.
	* Inference on checkpoint 30000: a little more focus on correct lane choice, but still
	  sometimes moves around too much. Speeds are more consistent, but a ~2 m/s off the
	  target speed. It appears that speed deviation penalties (and others) are getting
	  swamped by the LC desirability reward after 30-40 steps since LC des reward is
	  growing and the others are not. This seems to be contributing to a bit of a noisy
	  speed profile. This checkpoint definitely performs better than the 10000 one.

2/11/24

* Run C46 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Tweaked speed reward to not have a dead zone, which should help determine gradients.
	* Increased penalty multiplier for speed variation from 0.005 to 0.01.
	* Changed LC desirability bonus from exponential growth to straight value to try to
	  better balance it with other rewards. Left the poor choice penalty to grow, however.
	* Checkpoints stored at 20240211-1257.
	* Killed after 8k iters becasue I forgot to change one of the rewards.

* Run C47 training single BridgitNN agent from run C46, checkpoint 08000, with SAC over 30k
  iters using 16 bots on roadways C & D.
	* Increased the penalty for lane change initiation, both multiplier & additive constant
	  to help dissuade extraneous maneuvers. Max penalty will now be -0.0600.
	* Continued from previous run.
	* Checkpoints stored at 20240211-1525.
	* Inference on checkpoint 29000: Found that there was a defect in the LC desirability
	  reward calcs. I still see some noise in the speed profile, although it follows
	  speed limit pretty well. Still too many extraneous lane changes happening.

2/12/24

* Refacored the environment to use a different format vehicle config file, which now only
  specifies each vehicle type (and its attributes) and the max number of them to be used. The
  reset() method then chooses which of these to use in each scenario, randomizing the selection
  so that the same types of vehicles aren't always positioned closest or farthest from ego.

* Run C48 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Fixed LC desirability reward defect.
	* Increased multiplier for LC frequency penalty from 0.001 to 0.002.
	* Increased multiplier for speed cmd variance from 0.01 to 0.02.
	* Checkpoints stored at 20240212-0118.
	* Training completed. Mean ep len peaked in low 90s. rmax ~1.4, rmean ~0.4, rmin ~-3
	  with some excursions up to -1. All reward metrics seem to be slowly increasing at the
	  end, indicating further training may be useful.
	* Inference on checkpoint 30000: speed plot it still noisy, with commands moving quite a
	  bit. Some extraneous LCs, but not many. One run slowly crept up on a slow bot and rear
	  ended it, seemingly due to over-controlling the speed. Still seem to be getting speed
	  penalties when following a slow bot, which should not happen. On one start it immediately
	  drove into the grass; this was where initial speed and speed limit were way different,
	  and it seemed to over-react by setting both commands to near 1 for a quick adjustment.
	* Overall, this is pretty good!

* Run C49 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways C & D.
	* Enhanced get_fwd_vehicle_speed() to look at all sensor zones forward of the ego vehicle
	  instead of just the nearest 6 zones.
	* Increased penalty for speed variation multiplier from 0.02 to 0.05.
	* Increased lane change penalty contant term from 0.005 to 0.01 and multiplier decreased
	  from 0.002 to 0.001.
	* Checkpoints stored in 20240212-1446.
	* Training completed 29k iters then died. Mean ep len peaked in high 90s early,
	  then gradually dropped to mid-80s. rmax gradually increased to 1.2 at end. rmin stayed
	  around -3 to -4 with a few up to -1. rmean gradually increased to +0.2. It appears all
	  rewards may improve a bit with more training.
	* Inference on checkpoint 29000: speed control is quite good, if still a bit noisy. It
	  follows speed limits quickly, and slows for slower vehicles in front, never rear-ending
	  one. Lateral control is pretty good; an occasional extraneous LC, but usually heads
	  to the target lane rapidly, while being mostly cognizant of vehicles to the side. One
	  run it chose to drive off side of the road in first 6 time steps. One run it cut off a
	  bot coming up in the next lane somewhat faster, but it may not have been able to see,
	  and the bot could have been more aware. It seems to anticipate upcoming speed limit
	  changes and adjust to them before they apply.
		* Penalties for speed & speed variation are usually O(0.001) compared to lane
		  desirability bonus of O(0.01), so they are about as small as it can be expected
		  to see.
		* Note that this evaluation was only done on roadways C & D.
	* A few tests on roadway B also shows that it handles it well! even though it has never
	  seen this one.
***	* THIS IS THE ONE! It finally performs well enough to be used for the baseline Bridgit
	  model in the MARL training. Stored the checkpoint in models/cda1.1-C49-29000.

2/13/24

*** Evaluation of C49 model over episodes of 100 steps and unlimited duration, and all 3
  roadways. Very good!
* Merged the part2 branch into develop (but kept this branch for continuing work on the real
  part 2 stuff). Updated the README for github and merged develop into master, tagging it as
  "cda1.1.1". Merged updated develop back into part2 branch and continuing to work there.

* Updating the BridgitGuidance model to work with its NN.
	* Extracted the scaling & unscaling functions into standalone functions so that they
	  can be used in multiple places that may not have access to the env or its wrapper.
	* Reworked BridgitGuidance.step() to properly handle the inputs & outputs in the
	  formats needed.
	* Ran a few quick inferences with one Bridgit agent as a neighbor. It doesn't perform
	  nearly as well in this role as it does as the ego vehicle.  Need to investigate.

2/14/24

* Used inference to investigate problems with the Bridgit agent as vehicle 1.
	* Changed BridgitGuidance.step() to use deterministic actions instead of stochastic
	  distribution from the NN. It still changes lanes left as quickly as possible.
	* I suspect I'm not interpreting the NN outputs correctly. Need to investigate the
	  Algorithm.compute_single_action() logic.
	* Inference highlighted two anomalies that may not be related to the command issue:
		1. step() shows action[0] = 36 m/s, but inference reports that
		   raw_obs[ObsVec.SPEED_CMD] = 18.00.
		2. Ego started in lane 2 of roadway D. In first time step it initiated a
		   left lane change, and immediately got flagged as off-road, even though
		   lane 1 exists there.

2/17/24

* Lots of investigation of Ray code for executing a NN in inference mode.
	* Need to set the model to eval mode when inference_only.
	* Need to "squash" the NN output actions by passing the mean values through F.tanh(),
	  and not using the stddev values (no exploration).
		* Note that the NN output vector contains all the mean values first followed
		  by all the stddev values.
	* Found that the first time step (or two?) usually produces action outputs with
	  extreme values. Maybe this indicates poor selection of initial obs values?
	* Even after applying these changes to computation of actions, the BridgitGuidance
	  still very much likes lane changes. In fact, the raw NN output for LC cmd is
	  usually around +/-2.7 or so, so it doesn't even seem to be trying to stay in lane.
	  This makes me suspect the inputs need to be somehow manipulated as well. However,
	  some env code mods allowed direct comparison between the Ray inference results on
	  the ego vehicle and what BridgitGuidance would have produced for that same vehicle
	  in the same state. Results are always identical, which says there has been no
	  input manipulation in Ray that is not going on in BridgitGuidance.

* Experiments running inference on roadway D. In all cases, still have code change in env
  to force BridgitGuidance to compute ego's actions (which happen to match those coming from
  the Ray Algorithm object.
	* With v1 as a normal bot, the ego vehicle performs as I had seen in training evals
	  a few days ago. However, I do now notice that it usually initiates an LC in the
	  first time step, with vengeance (scaled cmd near +/-0.99). This happens even when
	  there is an active target in the same lane (it then does an immediate correction
	  LC and stays put).
		* It seems that training was disadvantaged by having the env ignore the
		  LC cmd in the first time step; it didn't learn to keep it under control.
		* Debugging shows that v1's LC desirability is 0 for all lanes always! It
		  has seen this in training, but only when it is in a no-win situation, so
		  it is guaranteed to get a terrible reward, so action choice doesn't
		  matter.
		* Actually, LC desirabilities are all 0s even for ego on step 1.
		* Guidance plan_route is only run for the ego vehicle, since it is
		  kludged in env.step(). Need to refactor this.
	* With v1 set as BridgitGuidance also, ego still performs pretty well, but seems
	  more eager to drive off-road in first time step.
		* Since neighbor vehicles can go to any target, v1 may be getting
		  overwhelmed with the LC desirabilities pointing in all directions many
		  times, so it constantly changes what lane it wants to try.

2/18/24

* Run D1 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways B, C & D.

	* Refactored how plan_route is called & how its output is communicated. It
	  outputs LC desirabilities into the obs vector. Env.step() now invokes
	  guidance.step() on ego vehicle as well as on bot vehicles of that type.
	  When guidance.step() invokes plan_route(), the return values are inserted
	  into the obs vector that was provided. Experiment confirms this will get
	  communicated back to the caller (env.step()) so that it will propagate to the rest
	  of the code.
	* Made Bridgit neighbor vehicles go to any target, depending on is_learning.
	* Changed step() to not constrain the commands on first step (to avoid immediate LC).
	  Also changed reset() to initialize the 4 historical obs elements with the same
	  value that their current elements hold.
	* Fixed defect in RoadwayD specification of lane 2 left separation distance.
	* Reduced random threshold in _choose_active_targets() from 0.75 to 0.6 to
	  encourage more episodes with more active targets. This should help when Bridgit
	  agents are operated as bots that can see all targets.
	* Increased LRs slightly from 7e-5 to 8e-5.
	* Increased penalty for frequent lane changes; constant term from 0.01 to 0.02.
	* Increased multipler for speed variation penalty from 0.05 to 0.07.
	* Checkpoints stored at 20240218-1305.
	* The run died after 8600 iters, with rmin ~-3.6, rmean ~-0.8, rmax ~+0.6 and mean
	  ep len peaking at 92 then dropping to 75.
	* Restarted from checkpoint 08000 - continued checkpoints in 20240218-1655.
	* Inference on new checkpoint 17000: way too eager to change lanes arbitrarily.
	* Training completed additional 30k iters, or 47k total iters. Ending performance
	  was rmin ~-3, rmean ~-0.1, rmax ~1.2, mean ep len ~70. Not impressive, and it
	  hadn't changed notably in the last 7k iters.
	* Inference on checkpoint 30000: still eager to do several extraneous LCs, even
	  immediately before the target at a penalty of -0.0630 points. Still a few runs
	  with large LC cmd values in first couple steps. Speed penalties can be much
	  larger than the LC des bonuses, so it works harder to maintain speed limit, which
	  it does well.
		* The big revelation is that, by having guidance.step() invoke the route
		  planning instead of doing it at the bottom of the env loop, it is now
		  making replan decisions based on 1 step old value of the LC counter, so
		  it waits an extra step before refreshing the LC desirabilities. This
		  is causing the NN to still want to continue the LC in the step where
		  the LC completes, thus initiating a new one immediately. Also, the
		  reward completely ignores the LC cmd while a mvr is underway, allowing
		  it to stay high, even on the final step of the mvr. This unconstrained
		  behavior may be encouraging it to learn that large values are a good
		  habit.
		* Probably time to add an obs element that indicates the vehicle's LC
		  state. The existing obs element STEPS_SINCE_LN_CHG is a continuous
		  value that scales [0..60] -> [0..1], so doesn't provide enough
		  precision to clearly see when the counter has reached the total number
		  of steps in a vehicle's maneuver (which could even be different for
		  different agents), so an additional element would be useful.

* Planning for part 2 multi-agent training, with Bridgit vehicle(s) as some of the
  neighbors.
	* Neighbor Bridgits will always run as non-learning, using the static NN model.
	* Vehicle 0 can be configured to be Bridgit, in which case it can be trained. In
	  the inference program, if it is an inference-only scenario (20s) it will load
	  the canned NN model like neighbor vehicles do. For any other scenarios, the
	  program will look for the CLI-specified checkpoint file and run the vehicle in
	  inference mode with that model.
	* If Vehicle 0 is configured as a bot, then it will be marked as non-learning and
	  no checkpoint loading will be attempted.
	* Vehicle 0 is always indicated with the special yellow car + red flag.
	* Measuring success in part 2:
		* The baseline Bridgit model will be subjected to additional training
		  where some of the neighbors are running copies of the previous baseline
		  Bridgit NN model.
		* Training success will be judged as part 1 training was, with reward
		  profiles, episode lengths, and manual evaluation runs performing as
		  expected.
		* As each round succeeds, the number of neighbors operated by Bridgit will
		  be increased. While it is not reasonable for every vehicle to be operated
		  this way (need a mix of personalities), it will be useful to know that
		  the model can function in such an environment.
		* Additional measures would involve traffic flow metrics. Establish a
		  baseline case with N bots (mix of both types), and run a sequence of M
		  evaluation episodes (unconstrained length). New scenarios would need to
		  be defined that place the ego vehicle at the same location near the
		  beginning of each lane (randomize starting lane selection) so that we
		  have consistent expectations of drive time for a given lane. Then
		  the neighbors can be randomly placed around the ego, like for training.
			* Bot vehicles will need to be reprogrammed to make a better
			  attempt at following speed limits, rather than a bias below it.
			* Only use the RoadwayB roadway model, since it better represents
			  real-world geometry.
			* Use a fixed set of active targets, such as originally envisioned
			  in lanes 1 & 2.
		* Accumulate stats on:
			* Crashes per episode (all vehicles, as this will be an indication
			  of unnecessary congestion).
			* Ego vehicle travel time from each lane starting point to each
			  target.
			* For each neighbor vehicle, track its "speed disadvantage", which
			  can be defined as the integral of its target_speed - actual_speed
			  over the course of its drive. Target_speed for a bot is defined
			  when it is created. For a Bridgit neighbor, it will be the posted
			  speed limit. In order to compare performance between two vehicles
			  with differing route lengths, this integral must then be
			  normalized to route length, so units would be delta-m/s per km.
			  This will then be a measure of congestion.

2/19/24

* Run D2 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways B, C & D.
	* Changed BridgitGuidance.plan_route() so that it looks one step ahead for the end
	  of an in-progress LC maneuver to force the replan, since it is now being execcuted
	  at the top of the loop, where it used to be at the bottom and the counter was
	  already updated.
	* Added obs vector element LC_UNDERWAY so that the agent unambiguously knows when
	  such a maneuver is in progress.
	* Fixed a defect in scale_obs() where it was not copying the LEFT_OCCUPIED or
	  RIGHT_OCCUPIED elements. This explains all the lane changes into neighbors where
	  it seemed blind to them.
	* Reduced penalty for speed deviation multiplier from 0.5 to 0.3 so that it doesn't
	  dominate the LC rewards.
	* Checkpoints are stored at 20240219-2137.
	* Early inference on checkpoint 11000 because I felt there is still a mistake:
	  still showing a desire to do a lot of lane changes. Yes, there is still a delay in
	  computing the new LC desirabilities, even though plan_route is now anticipating LC
	  completion and running one time step earlier; because it is run before dynamics
	  update, it still sees the vehicle in the origin lane, so LC desirabilities are the
	  old values. Speeds are a little less consistent with speed limits, and are noisier.
	  About half the time the LCs are going in the undesirable direction, so it is not
	  just chasing more rewards.
	* I killed the run based on known new defect in plan_route().

2/20/24

* Run D3 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways B, C & D.
	* Changed BridgitGuidance so its LC desirability calculations anticipate the new
	  target lane on the last time step of an LC mvr.
	* Fixed a new defect in BridgitGuidance.plan_route() that was forcing it to run on
	  every time step.
	* Changed definition of obs STEPS_SINCE_LN_CHG to start counting from 1 as a mvr is
	  initiated, instead of only being reset when the mvr is complete. This only affects
	  input to the NN, it is not used in env calcs.
	* Increased penalty for speed variations multiplier from 0.07 to 0.15.
	* Checkpoints stored at 20240220-0250.
	* Early inference on checkpoint 20000: lots of off-roading, esp in first time step. I
	  wonder if this is in response to the normal distro of episodes with rmin < 3, since
	  going off road early ends your life with only -1. Also, it is still common to see
	  agent changing into the side of a neighbor, or into the path of a faster moving
	  neighbor; looks like neighbors aren't watching out for this.
	* Killed it early after finding a couple defects (see below).

* Run D4 training single BridgitNN agent from scratch, with SAC over 30k iters using 16 bots on
  roadways B, C & D.
	* Fixed defect in BridgitModel that was restarting the STEPS_SINCE_LN_CHG early, thus
	  causing a much larger than deserved penalty for beginning a LC.
	* Fixed a defect in _get_rewards() that often didn't penalize an ego-fault crash.
	* Changed completion penalties for crash from -3 to -5, for off-road from -1 to -3,
	  and for stopping from -1 to -3, to encourage agent to find a driving solution.
	* Checkpoints stored in 20240220-1045.
		* Code was throwing lots of divide-by-zero errors & restarting workers, which
		  will screw up the noise schedule. So I fixed the defect in rewards and
		  restarted from checkpoint 06000.
	* Continuation run checkpoints stored in 20240220-1304.
	* Training completed with rmin ~-5, rmean ~0, rmax ~1.1 and mean ep len ~98, all
	  steady for the last 6k iters.
	* Inference on checkpoint 29000: it mostly performs very well. 
		* Ran ~30-40 runs spread across all 3 roadways.
		* Speeds are a bit off of the speed limit (~2-5 m/s), but it is responsive to
		  speed limit changes. 
		* It hits targets in all but one run, but tends to procrastinate initiating the
		  lane changes, then stacks all of them together, gaining max penalties,
		  instead of spreading them out to reduce the frequency penalty. 
		* Saw only one dumb rear-end crash, and no side crashes. Many times it appeared
		  to be actively adjusting speed to avoid lateral crashes or rear-endings.
		* No extraneous LCs and no off-road, except  when it missed the one target.
***	* USE THIS as baseline for going forward with multi-agent training.

* Smoke testing of using the new agent (cda1.1-D4-29000) as a neighbor. See exception raised
  about finding speed limit on NoneType in reset() only when using scenario 1. This only happens
  occasionally. I added 2 assertions in reset & verified that self.roadway is being defined as
  expected. Maybe some method call is corrupting it?

2/21/24

* Run D5 training BridgitNN from the cda1.1-D4-29000 baseline, with 1 Bridgit (D4) neighbor +
  15 bots on all 3 roadways for 10k iters.
	* Running with 1 Bridgit neighbor and 15 others.
	* Reduced LR from 8e-5 to 1e-5.
	* Reduced Gaussian noise initial magnitude from 0.25 to 0.1; final magnitude from 0.1
	  to 0.01 over 1M steps (was 50M); random timesteps from 100k to 0; 
	* Fixed defect in reward calc of whether agent should've seen upcoming neighbor that
	  it crashed.
	* Fixed minor defect where _place_neighbor_vehicles() was always selecting the first
	  vehicle types in the config file, and not fully randomizing them.
	* Fixed defect in BridgitModel where it allowed a full reset even for an inactive
	  vehicle, which terminated the run.
	* Checkpoints stored at 20240221-2129.
	* Training completed in 3.2 hr with rmin ~-5, rmean ~0.4, rmax 1.3 and mean ep len ~98.
	  Total iters trained now 40k (70M steps).
	* Inference on checkpoint 10000: smooth performance all around. Speed control is a bit
	  lacking; often high or low by up to 6 m/s. Sometimes waits quite long before doing
	  necessary LCs. Seems to anticipate speed limit changes, which is good. Following
	  behavior is consistently conservative. Never saw an ego crash in ~30 runs.

2/22/24

* Run D6 training MA Bridgit from the D5 checkpoint 10000, with 4 Bridgit neighbors (also
  running the D5 checkpoint 10000) and 12 bots for 10k additional iters on all 3 roadways.
  Corresponds to part2 commit 1a44d5b.
	* Note that the checkpoint policy is not permanently saved in the repo.
	* Increased penalty for speed deviation; multiplier from 0.3 to 0.5.
	* Increased penalty for speed variation; multiplier from 0.15 to 0.2.
	* Checkpoint stored at 20240222-0951.
	* Training completed in 4.3 hr with rmin ~-5, but frequent excursions above -5,
	  rmean ~0, rmax ~1.3, mean ep len ~99.
	  Total training now 50k iters (83M steps).
	* Inference on checkpoint 10000 (30 runs on all 3 roadways):
		* Speed ctrl - maybe a little tighter than before.
		* LCs - a couple extraneous moves. Still doing multiples as rapidly as
		  possible, gaining bigger penalties.
		* Crashes - rear-ended a Bridgit neighbor that was somewhat below speed
		  limit. I suspect the neighbor had the Bridgit wavering speed ctrl, 
		  and ego was too used to the steady motion of the bots, so didn't leave
		  enough room to accommodate. Rear-ended a bot where spd lim dropped.
		  Rear-ended a much slower moving Bridgit immediately after doing 2 LCs
		  to a lane with lower speed limit; so it didn't see this coming, despite
		  sensors to tell it so. Lateral crash into a bot while doing an extraneous
		  LC; bot was slower, so ego didn't recognize sensor data to predict the
		  problem.
		* Off-road once, on first time step.

* Run D7 training MA Bridgit from the D4 baseline, with 8 Bridgit neighbors (also
  running the D4 baseline) and 8 bots for 10k additional iters on all 3 roadways.
	* Doubled the speed variation penalty multiplier from 0.2 to 0.4.
	* Increased LC frequency penalty linear multiplier from 0.001 to 0.004, which will 
	  make the max penalty (for immediate LC) larger than the accumulated LC des bonus
	  earned during the maneuver.
	* Checkpoints stored at 20240222-1359.
	* Training completed in 3.2 hr with rmin ~-6, rmean ~-1.3, rmax ~1.2 and recent mean
	  ep len ~79. Ray result showed the final batch had several episodes of length 1 and
	  very few with lengths between 3 and 100.
	  Total iters trained is 50k (67M steps).
	* Inference on checkpoint 10000: lots of off-roads with illegal LCs. I suspect this
	  has been learned as, now that LCs are so expensive, it's cheaper to just die early.
	  Had a crash where a Bridgit neighbor changed lanes immediately in front of ego,
	  but was going slower, so ego rear-ended it.

* Run D8 training MA Bridgit from the D7 checkpoint 10000, with 8 Bridgit neighbors
  (running D4 baseline) and 8 bots for additional 10k iters on all 3 roadways.
	* Reduced the LC frequency penalty linear multiplier from 0.004 to 0.002 to avoid
	  LCs being too expensive compared to a crash (since it has baked in a lot of
	  personality for doing these LCs anyway, and additional 10k iters probably won't
	  change that much).
	* Training completed in 3.2 hr with rmin ~-5, rmean ~-1.1, rmax 1.3, mean ep len
	  ~70. Final batch showed lots of episodes with length 1.

* Run D9 training MA Bridgit from scratch for 30k iters with 8 Bridgit neighbors (running
  the D4 baseline) and 8 bots on all 3 roadways.
	* Changed LR back to 8e-5 and noise initial magnitude back to 0.25, but left the
	  other HPs the same. No changes to rewards.
	* Hoping to see the bigger emphasis on speed and speed variation penalties, and
	  LC frequency penalties being applied uniformly from the beginning to make for
	  a smoother agent.
	* Checkpoints stored at 20240222-2349.
	* Training completed. Performance was disappointing, with rmin steady ~-6, but
	  rmean peaking ~-0.2 @6k iters then gradually dropping to -2.3. rmax steady
	  ~1.0, but mean ep len peaked at 100 @5k iters, then gradually dropped into
	  the 30s by the end.
	* Quick inference on checkpoint 30000: loves to dive into the grass immediately.
	* Inference on checkpoint 10000: seems to have learned the basics quickly, but
	  speed control is pretty poor where SL changes are occurring, earning huge
	  penalties often. Reluctant to change lanes, often missing targets.
	* Noise was essentially non-existent, as the schedule had it down to 0.01 in 1M
	  steps, which occurred by iter 800. I am starting to believe that added noise
	  is essential to this learning.

2/23/24

* Run D10 training MA Bridgit from scratch for 30k iters with 8 Bridgit neighbors (running
  the D4 baseline) and 8 bots on all 3 roadways.
	* Reinstated the initial training noise HPs: initial random timesteps 100k;
	  final scale from 0.01 to 0.1; scale timesteps from 1M to 50M. (Kept initial
	  scale at 1.0 and stddev magnitude at 0.25.)
	* Checkpoints stored at 20240223-1141.
	* Training almost completed (27k iters) before I killed it for poor performance.
	  mean ep len hovered in the 20s for most of the training (started dropping after
	  5k iters), and rmax never broke above 0. rmin never broke above -5, indicating
	  that the agent learned to prefer suicide to running a full episode.
	* Quick inference on checkpoint 26000: most of the time agent chooses immediate LC
	  so lots of off-roads. Indeed, even in easy cruising situations, penalties for
	  speed and speed variation are similar magnitude to the bonus for good lane choice
	  so lane choice is not getting much attention.

* Run D11 training MA Bridgit from scratch for 30k iters with 8 Bridgit neighbors (running
  the D4 baseline) and 8 bots on all 3 roadways.
	* Reinstated the speed & speed variation penalties used in run D4 (speed mult 0.3,
	  speed variation mult = 0.15).
	* Added reward clipping for underway penalties, which guarantees that an episode
	  won't experience worse penalties than an undesirable termination.
	* Checkpoints stored at 20240223-2115.
	* Training completed. After a dip, mean ep len came back to 96. rmin finally got
	  positive, up to 0.6, but rmean ended a -0.8.
	* Inference on checkpoint 30000 on roadway D: never wants to change lanes. Clipping
	  has a major impact here - the first time step that LC desirability is poor, its
	  penalty is well larger than the clip limit, and grows from there. So agent never
	  learns how bad it is to stay in lane. It does learn to slow way down (just above
	  the threshold) when approaching end of lane, so it never goes off road.

2/24/24

* Run D12 training MA Bridgit from scratch for 30k iters with 8 Bridgit neighbors (running
  the D4 baseline) and 8 bots on all 3 roadways.
	* Removed reward clipping, but scaled all penalties by 0.1 to mostly avoid them 
	  accumulating too much. Left all terminating penalties at -5.
	* Modified Vehicle's definition of stopped to 5 m/s instead of 0.5 (for 4 steps).
	* Checkpoints stored in 20240224-0958.
	* Killed training early - no good. Mean ep len settled ~80, rmean ~-1.0, rmin ~-5.
	* Quick inference on checkpoint 18000 shows that it rarely changes lanes, preferring
	  to slow to a stop if target is not easy to reach. It does like to LC left, but
	  not right. Speed control is almost non-existent, loving to go fast always.
	  Penalties shown just don't compete with the LC desirability rewards and episode
	  end bonuses. Not sure they need to be 10x more, but they need more.

* Run D13 training MA Bridgit from scratch for 30k iters with 8 Bridgit neighbors (running
  the D4 baseline) and 8 bots on all 3 roadways.
	* Increased LC des penalty multiplier from 0.0004 to 0.004 to match that used in
	  baseline D4 run.
	* Increased penalty for LC frequency: constant from 0.002 to 0.02, linear multiplier
	  from 0.0002 to 0.001 to match run D4.
	* Increased speed mult from 0.03 to 0.3 to match run D4.
	* Increased speed variation multiplier from 0.015 to 0.15 to match run D4.
	* This is now like run D4 except a couple defect fixes and now using 8 Bridgit
	  neighbors.
	* Checkpoints stored at 20240224-1629.
	* Training complete in 9.9 hr. rmin ended ~-6.2, rmean grew slowly to -1.1,
	  rmax 0.5, mean ep len dropped to mid-70s then recovered to mid-90s.
	* Inference on checkpoint 30000: 
		* Speed control - okay, not great. It takes a couple hundred meters to
		  lock into a target, with sometimes large swings at the beginning, as
		  if it has some memory. Maybe 1 step history is somehow amplifying
		  this feedback loop? Sometimes with constant SL it still stabilizes
		  way off the mark.
		* Lane change - several extraneous one, but it seems to be trying to
		  spread out the necessary ones to minimize penalty. It often wants to
		  do a LC early, seemingly just to do one.
		* Crashes - 1 rear-end of a bot right after ego LC. 2 lateral into a
		  Bridgit going slightly slower and slightly in front going from lane
		  5 where there is much urgency; should've been avoided. All of these
		  were late in the maneuver, so it had to do a lot of prediction.
		* Targets - never missed a target. It likes to slow substantially just
		  before a target sometimes.
	* WORTH KEEPING around for possible consideration as the next iteration.

* Began modifying code to handle formal traffic performance evaluations.
	* Completed all of the HighwayEnv code mods to define the new scenarios, per
	  the updated software rqmts doc.

2/25/24

* Run D14 training MA Bridgit from the D13 checkpoint 30000 for an additional 15k iters
  with 8 Bridgit neighbors (running the D4 baseline) and 8 bots on all 3 roadways.
	* Increased lane change penalty constant term fro 0.02 to 0.03 to discourage
	  extraneous mvrs.
	* Adjusted speed deviation penalty slightly, multiplier from 0.3 to 0.35.
	* Reduced LR from 8e-5 to 4e-5.
	* Checkpoints stored at 20240225-0952.
	* Training completed in 5.2 hr. Rewards increased smoothly to rmin ~-5.3,
	  rmean ~-0.3, rmax 0.9, mean ep len ~99.
	* Total 45k iters (42M steps).
	* Inference on checkpoint 15000: 
		* Speed control: seems a bit tighter than before.
		* Lane change: seems more reluctant to do these, but still an occasional
		  extraneous mvr.
		* Crash: 1 rear-end of a slower Bridgit beside; tried to slow, but not
		  enough; step 16 from lane 5. 1 lateral trying to cut in front of
		  another Bridgit.
		* Targets: missed 1 on roadway C, too slow to LC, then was blocked and
		  refused to crash into the neighbor.
	* I believe this is the best agent I've seen yet. It's a KEEPER! Storing it as
	  models/cda1.2-D14-15000 and set it as the new default model for BridgitNN.
		* Ran more inference runs with this new agent in all Bridgits and it
		  works well.

* Started building the traffic_eval program
	* Enhanced BridgitNN to allow injection of a main model file if desired.
	* Updated specification of targets for evaluation scenarios.
	* Created partially functiona traffic_eval code that can run one episode.

2/27/24

* Completed buildiing the traffic_eval program and ran it against the D14 baseline
  Bridgit model. Evaluation over 250 episodes. Report stored as
	docs/D14_eval_report.txt

	* It appears that the biggest problem with the Bridgit model right now is
	  the tendency to crash. It is not clear from this evaluation how many crashes
	  are the fault of any given vehicle type, but my graphical inference runs
	  show that Bridgit could definitely do better. The biggest approach to
	  solving this, I think, is to give Bridgit (and the bots) visibility into
	  neighbor vehicles' LC status while it is underway (a new sensor layer for
	  Bridgit), and the ability to abort a LC once begun.
	* I also want to create an evaluation of an all-bot traffic to serve as a
	  baseline to compare to.





TODO:
- Make a toggle in train prgm to switch all HPs from new training to continuation learning.
- Consider a train prgm HP to switch the Bridgit NN model more easily.
- Ensure speed variation penalty is larger than 2-3 consecutive speed penalties in order to make it worth slowing the accel.
- Add 2nd prev speed command to allow penalizing of jerky commands
- retrace NN size expansion and addition of dropouts once success is found.
- Add ability for agents to abort a LC mvr (both Bridgit and bots). Requires knowledge of neighbor LC activity.
- Decouple BridgitGuidance.plan_route logic from reward structure (see handling of low max_prob near bottom). Env reward should be
  independent of model logic.
- Ensure EmbedGuidance matchess BridgitGuidance.
- add coreographed scenarios to routine training that force certain close call situations.
- consider using imitation learning. Create a bot (1c) that changes lanes to pass a slow forward vehicle. Then
  either:
	a) Pre-train the agent with supervised learning. Requires episode capture (like embed_collect) to capture a db
	   with full obs vector, actions and rewards. Then run training (like embedding) on this db comparing either full
	   episodes, short trajectory segments or single time steps.
	b) Intersperse imitation episodes with normal episodes in SAC RL training. ~10% of episodes would use the 1c bot
	   to produce the expert actions for a given time step, then adjust the full ego state to that result and repeat.
- Convert Train checkpoints to msgpack format (see RLlib_notes.txt)
- Enhance Bridgit to use the FUTURE obs for performance config params. This could dial in aggressiveness for meeting the
  speed limit, or even reaching the target asap. Increase reward for speed compliance and decrease it for lane stability,
  and vice versa.

- Review TODOs in graphics.py
- Add ego's fwd distance plot
- Display lane change as a gradual motion
- Investigate speed overshoot when speed limit changes (with bot1a controller)
- Add lane IDs to graphics
- Add checkpoint name to display.
- Add legend for car colors.
- Review all TODOs
- Add ego sensor display to graphics showing pvmt type/speed colors, vehicle speeds
- Add graphic replay w/step fwd/back
- Greenfish icon editor Pro used for making the graphic icons.
