John's notes on development of the CDA1 project for the Part 2 goal: multiple agents using the same policy
----------------------------------------------------------------------------------------------------------

12/30/23

* Rearranged the project directory to make it more navigable.

1/1/24

* Performed routine system upgrade, including patches to the OS and an update of Ray from 2.5.1 to 2.9.0.
	* Ray changed their checkpoint API in v2.7, so now my saved checkpoints cannot be restored for
	  inference. Also, training doesn't save new checkpoints the same way, so I had to investigate
	  the new approach.

1/4/24

* Continued searching for a workaround to problems using the new checkpoint API. It seems to force me to
  use Ray's TorchTrainer, and its .fit() method, similar to Tune. This then requires resource allocation
  (num workers cannot be left to a default).
	* Specifying num_workers > 1 forces it to request more hardware than is available. It seems to 
	  want to assume 1 CPU and 1 GPU for each worker, but it's not clear where that is being
	  specified. My legacy resource specs in the AlgorithmConfig seem to be partially recognized.

* Run C1 training single agent BridgetNN with SAC from scratch for 30k iterations using 16 bots.
	* This should be identical config to run B5, which was run on 12/27/23 in part 1 of the project,
	  and resulted in the publication of the part 1 resulting model (however, that model cannot
	  currently be run because of Ray's problems reading the checkpoint). So this run should provide
	  a substitute model in case recovery is not possible.
	* The training still uses Algorithm checkpoints (although they seem a bit altered, as nowhere is
	  the checkpoint version ID stored), so hopefully restarting training from one of these will pick
	  up the optimizer state, etc, and allow smooth continuation of training.
	* Training avg performance was ~2070 steps/hr, utilizing 1 cpu and ~5% of the GPU. Final rewards
	  were rmin ~-2, rmean ~-0.4, rmax ~1.6.
	* Unable to run inference due to missing checkpoint content. It is looking for a file named
	  algorithm_state.pkl or checkpoint-[0-9]+, and none exists. The only file in the checkpoint dir
	  is the model*.pt. So it appears I am unable to work with Algorithm checkpoints from here on out.

* While C1 was running I cleaned up a bunch of TODOs in the source code, all except the graphics.py.

1/5/24

* Testing reading of the new checkpoints
	* Inference cannot read them.
	* Training program also cannot read them for the same reason: key files are missing. Tried the
	  .pt file itself as well as the two levels of directories above that.
	* Tried using the structure suggested in the error message: `trainer = TorchTrainer.restore(ckpt).
	  It loaded the checkpoint okay (top level dir), but assumed that the job had errored out, and
	  tried to pick it up from where it started. Since it had completed its specified number of iters,
	  it would not go any further.

	* Reverted back to the baseline (part2) method, not using the Train API.
		* All checkpoints are getting written on top of each other (same name).
		* Added logic to train.py to name the top level dir according to timestamp and put
		  multiple checkpoints underneath, all named according to iteration #.
		* This works for writing new checkpoints, for starting a new training session from one,
		  and for running inference.
	* Verified that inference on the saved part 1 checkpoint still doesn't work. It complains about
	  'is-atari' property in SACConfig.
		* The easily visible diff between these checkpoints is that the part 1 dir includes two
		  hidden files, .is_checkpoint (0 length) and .tune_metadata in the top level dir.
		* I removed these two files, but the error still occurs. Therefore, it is probably due to
		  some content in one of the .pkl files.
		* Still waiting on someone to respond to my post on Ray Discuss about this problem, so I'll
		  leave it alone for now and try to regenerate the model with a new run.
	* Applied a single-commit patch from the ray_train branch, that incorporated all the TODO cleanups,
	  to the algo_checkpoints branch. This is the one that works and can be carried forward.

* Run C2 training single agent BridgitNN with SAC for 30k iters from scratch using 16 bots.
	* Recent TODO cleanups have been applied.
	* New checkpointing with the old Algorithm approach (from pre-2.7 Ray).
	* Training completed with promising ending rmin ~-0.7, rmean ~0.5, rmax ~1.6 and mean ep len > 98.
	  Performance was 6200 ksteps/hr.
	* Inference on checkpoint 30000: Lane 0 starts won't change lanes. Others show pretty good speed
	  control, esp near the lane 0 merge. One start rear-ended a bot, not after a LC. It continues to
	  drive at 29 m/s on tail of lane 1 where limit is 33.5.
	* Inference on checkpoint 20000: Lane 0 starts don't change lanes. General tendency to drive a lot
	  slower than speed limits. This is definitely worse.

1/6/24

* Run C3 training single agent BridgitNN with SAC for additional 10k iters from C2 checkpoint 30000 using
  16 bots. Hoping that the previous model may overcome its single flaw of not changing lanes from lane 0.
	* Training completed, checkpoints are numbered for iterations in this run, not continuing on.
	* With mods to code for loading BridgitNN model, this becomes an old-style model, with the first
	  layer named "macro_encoder", whereas I prefer to name it "fc1" going forward. However, the code
	  cannot load models with different layer names embedded in them. 
	* Inference on checkpoint 10000: lane 0 start still doesn't change lanes! However,
	* A Lane 4 start chose to stay in lane 2 once achieved, rather than moving to lane 1, as all
	  previous models have done. In doing this, it sped up to 33.5 m/s on the tail, as I would have
	  wanted it to in lane 1. Log confirms the macro obs believes its SL here is 29.1, but it acts as
	  if these two lanes are reversed. Otherwise, its speed control is the best I've seen.

* Modified code to allow the Bridgit tactical guidance model (BridgitNN) to load pre-trained weights and
  run in inference mode to provide functionality for the BridgitGuidance.step() method. This will allow
  BridgitGuidance to be loaded into any of the non-ego vehicles.

* Confirmed that the accepted part 1 model (stored as cda1.1-B5-20000) still throws the "is_atari" error,
  and no word yet from the Ray team, so I may need to recreate it anyway.

* Debugged the problem noted above, where lane tail-end speed limits seem to be incorrect.
	* In addition to seeing the correct SL values in the inference logs for each time step, I re-ran
	  the embed_eval program over several snapshots from that region (none with ego in lane 2, as they
	  are apparently rare), and they all look correct - both the raw observation values and the
	  interpreted values coming out of the autoencoder.

* Run C4 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Revised the BridgitNN code so that its first layer is now named "fc1" instead of "macro_encoder",
	  which breaks compatibility with older checkpoints.
	* Modified reward function:
		* Reduced the penalty for LC desirability poor choice; increased the factor from 0.3 to 0.4.
		* Reduced normal lane change penalty mult from 0.002 to 0.0001. Both of these changes should
		  encourage the agent to take more risks in changing lanes.
		* Reduced speed_mult from 0.06 to 0.05 for penalties for not following speed limit (to help
		  avoid episode rewards < -1).
		* Increased mult for widely varying speed commands from 0.07 to 0.1 to encourage more
		  smoothness.
	* I killed the run after 5k iters since the speed variation penalty was still ridiculously low.

* Run C5 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Checkpoints are in dir 20240106-2006.
	* Revised reward penalty for speed variation - multiplier from 0.1 to 1.0.
	* Training progressed okay until 12000 iters, after which the rewards and episode length suddenly
	  tumbled and stayed at low values from then on.
	* Inference on one of the late iterations showed consistent desire to make immediate lane changes,
	  despite the situation. Inference on iter 12000 worked fairly well, but it had some speed control
	  problems and would change lanes out of lane 0.
	* I killed the run a little early.

1/7/24

* Run C6 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Checkpoints stored in 20240107-0534
	* Changed BotType1bGuidance so that its probability of lane change is reduced from 0.2 to 0.06,
	  in order to make it more lazy about the maneuver.
	* Made the bot guidance (both 1a & 1b) a little more conservative in their ACC logic.
	* Changed the global constants DISTRO_DIST_REAR, DISTRO_DIST_FRONT to be balanced at 200 m each.
	  They were previously 150 and 250, skewing the neighbor vehicles in front of ego. As ego often
	  starts at a low speed, most of the vehicles have passed it by by the time it reaches cruising
	  speed, so they are useless for the sensor training.
	* Training again failed to produce, so killed it after 19k iters. Progress was good through
	  ~14k iters, then the episode lengths dropped dramatically, and rmax went to ~-0.3.
	* Inference on checkpoint 13000 looks pretty decent, but still doesn't change lanes out of 0.

* Run C7 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Rolled back all the reward changes made for runs C4 and C5 above. Keeping the bot changes
	  made for run C6.
	* Checkpoints stored in 20240107-1231.
	* While this was running (at ~14k iters) the inference program crashed with a segfault several
	  times. Usually, this brings down the whole machine eventually, so it may start affecting
	  training results. However, training continued for some time afterwards...




TODO:
- Reintroduce reward changes
- Reduce speed penalty even more, to be on line with speed var penalty.

- Review TODOs in graphics.py
- add coreographed scenarios to routine training that force certain close call situations.
- consider adding dropouts to BridgitNN
- consider using imitation learning. Create a bot (1c) that changes lanes to pass a slow forward vehicle. Then
  either:
	a) Pre-train the agent with supervised learning. Requires episode capture (like embed_collect) to capture a db
	   with full obs vector, actions and rewards. Then run training (like embedding) on this db comparing either full
	   episodes, short trajectory segments or single time steps.
	b) Intersperse imitation episodes with normal episodes in SAC RL training. ~10% of episodes would use the 1c bot
	   to produce the expert actions for a given time step, then adjust the full ego state to that result and repeat.
- consider rescaling obs element so that they all use the full range of [-1, 1] (e.g. speed limit).
- make Roadway an abstract interface, and allow multiple realizations of it to be chosen during training; add a 5-lane highwaay.
- Convert Train checkpoints to msgpack format (see RLlib_notes.txt)
- Convert Stopper class to a Checkpointing class to manage the number of checkpoints, and also the reporting of results based on running averages.

- Investigate speed overshoot when speed limit changes (with bot1a controller)
- Add lane IDs to graphics
- Add vehicle icons to graphics
- Review all TODOs
- Add ego sensor display to graphics showing pvmt type/speed colors, vehicle speeds
- Add graphic replay w/step fwd/back
- Display lane change as a gradual motion
