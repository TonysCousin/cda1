John's notes on development of the CDA1 project for the Part 2 goal: multiple agents using the same policy
----------------------------------------------------------------------------------------------------------

12/30/23

* Rearranged the project directory to make it more navigable.

1/1/24

* Performed routine system upgrade, including patches to the OS and an update of Ray from 2.5.1 to 2.9.0.
	* Ray changed their checkpoint API in v2.7, so now my saved checkpoints cannot be restored for
	  inference. Also, training doesn't save new checkpoints the same way, so I had to investigate
	  the new approach.

1/4/24

* Continued searching for a workaround to problems using the new checkpoint API. It seems to force me to
  use Ray's TorchTrainer, and its .fit() method, similar to Tune. This then requires resource allocation
  (num workers cannot be left to a default).
	* Specifying num_workers > 1 forces it to request more hardware than is available. It seems to 
	  want to assume 1 CPU and 1 GPU for each worker, but it's not clear where that is being
	  specified. My legacy resource specs in the AlgorithmConfig seem to be partially recognized.

* Run C1 training single agent BridgetNN with SAC from scratch for 30k iterations using 16 bots.
	* This should be identical config to run B5, which was run on 12/27/23 in part 1 of the project,
	  and resulted in the publication of the part 1 resulting model (however, that model cannot
	  currently be run because of Ray's problems reading the checkpoint). So this run should provide
	  a substitute model in case recovery is not possible.
	* The training still uses Algorithm checkpoints (although they seem a bit altered, as nowhere is
	  the checkpoint version ID stored), so hopefully restarting training from one of these will pick
	  up the optimizer state, etc, and allow smooth continuation of training.
	* Training avg performance was ~2070 steps/hr, utilizing 1 cpu and ~5% of the GPU. Final rewards
	  were rmin ~-2, rmean ~-0.4, rmax ~1.6.
	* Unable to run inference due to missing checkpoint content. It is looking for a file named
	  algorithm_state.pkl or checkpoint-[0-9]+, and none exists. The only file in the checkpoint dir
	  is the model*.pt. So it appears I am unable to work with Algorithm checkpoints from here on out.

* While C1 was running I cleaned up a bunch of TODOs in the source code, all except the graphics.py.

1/5/24

* Testing reading of the new checkpoints
	* Inference cannot read them.
	* Training program also cannot read them for the same reason: key files are missing. Tried the
	  .pt file itself as well as the two levels of directories above that.
	* Tried using the structure suggested in the error message: `trainer = TorchTrainer.restore(ckpt).
	  It loaded the checkpoint okay (top level dir), but assumed that the job had errored out, and
	  tried to pick it up from where it started. Since it had completed its specified number of iters,
	  it would not go any further.

	* Reverted back to the baseline (part2) method, not using the Train API.
		* All checkpoints are getting written on top of each other (same name).
		* Added logic to train.py to name the top level dir according to timestamp and put
		  multiple checkpoints underneath, all named according to iteration #.
		* This works for writing new checkpoints, for starting a new training session from one,
		  and for running inference.
	* Verified that inference on the saved part 1 checkpoint still doesn't work. It complains about
	  'is-atari' property in SACConfig.
		* The easily visible diff between these checkpoints is that the part 1 dir includes two
		  hidden files, .is_checkpoint (0 length) and .tune_metadata in the top level dir.
		* I removed these two files, but the error still occurs. Therefore, it is probably due to
		  some content in one of the .pkl files.
		* Still waiting on someone to respond to my post on Ray Discuss about this problem, so I'll
		  leave it alone for now and try to regenerate the model with a new run.
	* Applied a single-commit patch from the ray_train branch, that incorporated all the TODO cleanups,
	  to the algo_checkpoints branch. This is the one that works and can be carried forward.

* Run C2 training single agent BridgitNN with SAC for 30k iters from scratch using 16 bots.
	* Recent TODO cleanups have been applied.
	* New checkpointing with the old Algorithm approach (from pre-2.7 Ray).
	* Training completed with promising ending rmin ~-0.7, rmean ~0.5, rmax ~1.6 and mean ep len > 98.
	  Performance was 6200 ksteps/hr.
	* Inference on checkpoint 30000: Lane 0 starts won't change lanes. Others show pretty good speed
	  control, esp near the lane 0 merge. One start rear-ended a bot, not after a LC. It continues to
	  drive at 29 m/s on tail of lane 1 where limit is 33.5.
	* Inference on checkpoint 20000: Lane 0 starts don't change lanes. General tendency to drive a lot
	  slower than speed limits. This is definitely worse.

1/6/24

* Run C3 training single agent BridgitNN with SAC for additional 10k iters from C2 checkpoint 30000 using
  16 bots. Hoping that the previous model may overcome its single flaw of not changing lanes from lane 0.
	* Training completed, checkpoints are numbered for iterations in this run, not continuing on.
	* With mods to code for loading BridgitNN model, this becomes an old-style model, with the first
	  layer named "macro_encoder", whereas I prefer to name it "fc1" going forward. However, the code
	  cannot load models with different layer names embedded in them. 
	* Inference on checkpoint 10000: lane 0 start still doesn't change lanes! However,
	* A Lane 4 start chose to stay in lane 2 once achieved, rather than moving to lane 1, as all
	  previous models have done. In doing this, it sped up to 33.5 m/s on the tail, as I would have
	  wanted it to in lane 1. Log confirms the macro obs believes its SL here is 29.1, but it acts as
	  if these two lanes are reversed. Otherwise, its speed control is the best I've seen.

* Modified code to allow the Bridgit tactical guidance model (BridgitNN) to load pre-trained weights and
  run in inference mode to provide functionality for the BridgitGuidance.step() method. This will allow
  BridgitGuidance to be loaded into any of the non-ego vehicles.

* Confirmed that the accepted part 1 model (stored as cda1.1-B5-20000) still throws the "is_atari" error,
  and no word yet from the Ray team, so I may need to recreate it anyway.

* Debugged the problem noted above, where lane tail-end speed limits seem to be incorrect.
	* In addition to seeing the correct SL values in the inference logs for each time step, I re-ran
	  the embed_eval program over several snapshots from that region (none with ego in lane 2, as they
	  are apparently rare), and they all look correct - both the raw observation values and the
	  interpreted values coming out of the autoencoder.

* Run C4 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Revised the BridgitNN code so that its first layer is now named "fc1" instead of "macro_encoder",
	  which breaks compatibility with older checkpoints.
	* Modified reward function:
		1 Reduced the penalty for LC desirability poor choice; increased the factor from 0.3 to 0.4.
		2 Reduced normal lane change penalty mult from 0.002 to 0.0001. Both of these changes should
		  encourage the agent to take more risks in changing lanes.
		3 Reduced speed_mult from 0.06 to 0.05 for penalties for not following speed limit (to help
		  avoid episode rewards < -1).
		4 Increased mult for widely varying speed commands from 0.07 to 0.1 to encourage more
		  smoothness.
	* I killed the run after 5k iters since the speed variation penalty was still ridiculously low.

* Run C5 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Checkpoints are in dir 20240106-2006.
	* Revised reward penalty for speed variation - multiplier from 0.1 to 1.0.
	* Training progressed okay until 12000 iters, after which the rewards and episode length suddenly
	  tumbled and stayed at low values from then on.
	* Inference on one of the late iterations showed consistent desire to make immediate lane changes,
	  despite the situation. Inference on iter 12000 worked fairly well, but it had some speed control
	  problems and would change lanes out of lane 0.
	* I killed the run a little early.

1/7/24

* Run C6 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Checkpoints stored in 20240107-0534
	* Changed BotType1bGuidance so that its probability of lane change is reduced from 0.2 to 0.06,
	  in order to make it more lazy about the maneuver.
	* Made the bot guidance (both 1a & 1b) a little more conservative in their ACC logic.
	* Changed the global constants DISTRO_DIST_REAR, DISTRO_DIST_FRONT to be balanced at 200 m each.
	  They were previously 150 and 250, skewing the neighbor vehicles in front of ego. As ego often
	  starts at a low speed, most of the vehicles have passed it by by the time it reaches cruising
	  speed, so they are useless for the sensor training.
	* Training again failed to produce, so killed it after 19k iters. Progress was good through
	  ~14k iters, then the episode lengths dropped dramatically, and rmax went to ~-0.3.
	* Inference on checkpoint 13000 looks pretty decent, but still doesn't change lanes out of 0.

* Run C7 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Rolled back all the reward changes made for runs C4 and C5 above. Keeping the bot changes
	  made for run C6.
	* Checkpoints stored in 20240107-1231.
	* While this was running (at ~14k iters) the inference program crashed with a segfault several
	  times. Usually, this brings down the whole machine eventually, so it may start affecting
	  training results. However, training continued for some time afterwards...
	* Training continued through at least 21k iters, with rewards and mean episode lengths
	  gradually improving like they should. At this point rmin ~-1.5, rmean ~+0.1, rmax ~1.1
	  and mean ep len ~98.
	* Finally died of a segfault after 30200 iters, so the last checkpoint is 30000. At that time
	  rmin ~-1, rmean ~0.5, rmax ~1.5 and mean ep len ~99, training at 6100 ksteps/hr.
	* Inference on checkpoint 30000: still will not change lanes out of 0. Rear-ended bots in two
	  different tests. Otherwise, behavior is basically good.

1/8/24

* Run C8 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Reduced penalty for lane change - multiplier from 0.002 to 0.0002 (reintroducing C4 change
	  #2, but not quite as large a change.
	* Checkpoints stored in 20240108-0834.
	* Training completed in 14 hr (5900 ksteps/hr) with rmin ~-0.7, rmean ~0.5, rmax ~1.7, mean
	  episode len ~99.
	* Inference on checkpoint 40000: lane 0 start still refuses to change lanes. Other lanes
	  perform pretty well, however, twice it crashed while changing lanes coming up from behind
	  a slower neighbor in the left lane. In one episode, it rear-ended a bot (minimal speed diff).
	* Added logging to confirm that both the left & right center-lane boundary observations are
	  correct, as well as the lane change desirabilities, throughout an episode on lane 0. So it
	  simply is not learning how to handle these correct observations.

* Updated graphics to display a different icon for each type of vehicle (by guidance model), and to
  show the primary and secondary target destinations.

1/9/24

* Analysis of training lane assignments early in a training run. It includes 1800 iterations and 450
  episodes logged.
	* Initial lane distro is
		0: 106, 23.6%
		1:  26,  5.8%
		2:  19,  4.2%
		3:  70, 15.6%
		4: 107, 23.8%
		5: 122, 27.1%
	* Confirmed that the number of neighbors being used starts in the low range 0 to 3, and
	  gradually increases.
	* Confirmed that, for the lane 0 starts, speeds look evenly distributed across the full
	  range, and starting P locations also cover the full lane, realizing that the very early
	  ones are grouped closer to the end, and then gradually start backing up toward the beginning
	  of the lane.
	* In summary, all training conditions appear as designed, and nothing that should inhibit the
	  agent from having ample opportunity to learn the lane change required.
	* It could be that, since starts in lanes 3-5 all require left-hand lane changes, and lane 0
	  is the only one requiring a right-hand, the agent simply learns early that a left LC is
	  more important. Therefore, increasing the early opportunities to experience lane 0 might be
	  a benefit.

* Run C9 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Shifted the lane assignment probabilities for the early episodes so that it is now:
		0: 40%
		1:  5%
		2:  5%
		3: 15%
		4: 15%
		5: 20%
	* Increased penalty multiplier for speed variation from 0.07 to 0.1 (reintroducing C4
	  change #4).
	* Change the vehicle mix by adding a few Type1a bots to the mix.
	* Checkpoints stored in 20240109-1304.
	* Training completed in 13.5 hr with rmin ~-1.0, rmean ~0.5, rmax ~1.6, mean ep len ~99.
	* Inference on checkpoint 40000: rear-ended a bot with only a slightly slower speed.
	  Lane 0 starts still don't change lanes. Otherwise, similar behavior to before, not bad.
	  Occasionally, the speed variation penalty shows up at a significant size, but it is
	  rare enough that it probably doesn't have much impact on behavior; need it larger.

1/10/24

* Run C10 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Increased multiplier on speed variation penalty from 0.1 to 0.4.
	* Added flexible targeting, meaning that each episode reset has the opportunity to 
	  randomly select a different set of active targets from the list of valid targets given
	  by the user (in a config list). Valid targets can be a subset of the total list of
	  targets defined. These apply only to the ego vehicle; neighbor vehicle targets always
	  have the full list of defined targets available.
		* For this run, only target #2 is valid (the one in lane 2), thus forcing the
		  agent to learn more about right-hand LCs. Target randomization is off.
	* During early inference, discovered that the target mod didn't take effect, because
	  the reward function didn't get modified to only look at active targets. Therefore,
	  this training was using all 4 targets for the ego vehicle (different from all previous
	  runs, which only used targets 1 & 2).
	* Inference on checkpoint 29000: agent has learned to steer out of lane 0 sometimes.
	  However, since it trained with all 4 targets active, it sometimes steered _into_ lane
	  0 from lane 1 just to reach its endpoint! Giving it individual targets in inference
	  now is no good, since it hasn't learned too much about the meaning of the desirability
	  values, as they didn't change much in training, at least from episode to episode in
	  the same lane. Training with randomly chosen targets will provide it a much better
	  understanding of why those obs are valuable.
	* Speed changes were still erratic, so the penalty maybe needs to be bigger still.

* Run C11 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Fixed the reward function to only consider active targets.
	* Configured to randomize over all 4 targets for training.
	* Changed RoadwayB so that the ends of lanes 0 & 4 are marked as ASPHALT, which were
	  previously EXIT_RAMP. EXIT_RAMP is now not used.
	* Increased penalty for speed variation multiplier from 0.4 to 0.6, while reducing the
	  multiplier for speed limit deviation from 0.06 to 0.04 to keep from piling on large
	  penalties (C4 change #3).
	* Checkpoints are stored in 20240110-2217.
	* Run died early due to a segfault. Completed 15600 iters, at which point rmin ~-2.2,
	  rmean ~-0.2, rmax ~0.9, mean ep len ~91.
	* Inference on checkpoint 15000: does not change lanes out of lane 0. Speeds are a
	  little smoother, but still occasional large swings for no reason.

1/11/24

* Run C12 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Changed multiplier on speed variation penalty from 0.6 to 1.0.
	* Reduced penalty for making a poor choice on LC desirabilty from -factor to -0.3*factor.
***	* Updated RoadwayB to alter the track a bit:
		* Lengthened the exit ramps on lanes 0 and 4 to 200 m, so that they each have
		  100 m beyond their target destinations to provide conssitent sensor data
		  that gives the agent the impression of an on-going drive.
		* Changed the speed limits on the lane 3 segment 1 from 33.5 to 31.3 m/s and
		  on the lane 4 & 5 on-ramps from 20.1 to 22.4 m/s to give more variety of
		  training experiences.
	* Checkpoints stored in 20240111-0959.
	* After ~15k iters, rewards and episode lengths tanked. Inference on checkpoint 22000
	  shows rapid LC command, no matter where it is, until it runs off road. This is
	  apparently more desirable than the huge speed penalties it is receiving.

* Run C13 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Reduced speed limit penalty multiplier from 0.04 to 0.02.
	* Checkpoints stored in 20240111-1744.
	* Over the past several runs I have seen several crashes where agent changes lanes
	  just in front of a faster moving bot. Usually these occur late in the LC count,
	  indicating that when it committed to beginning the LC, the neighbor was probably
	  too far back to be seen. Since it has no way to abort an underway LC, the crash is
	  then inevitable. And the bot can't sense the ego vehicle's LC in progress until it
	  crosses the dividing line (step 8).
	* Training proceeded well through ~15k iters, at which point it showed rmin ~-2.1,
	  rmean ~-0.1, rmax ~1.2 and mean ep len ~89 steps. At that point performance began
	  to drop, oddly ending with similar rewards but mean ep len dropped to ~5 then
	  somewhat recovered to ~25 by the time it hit 40k iters.
	* Inference on checkpoint 15000: prefers to steer toward lane 1 even when there is
	  not an active target there. After several episodes, I saw one lane 0
	  start that changed lanes when no target exists in lane 0.
	* Inference on checkpoint 21000 (where mean ep len was worst): a lane 0 start
	  changed lanes into 1. Otherwise, every start did an immediate left turn off-road.
	* Inference on checkpoint 40000: not much different from 21000.
	* Inference on checkpoint 16000 with 100 step limit (like training episodes):
	  well-performing episodes only score -0.3 (no reward for completing the trajectory).
		* Lane 4 start changed to lane 3, as needed, and got a "poor" desirability
		  penalty of -0.9 for that!

1/12/24

* Run C14 training single BridgitNN agent from scratch with SAC over 40k iters using 16 bots.
	* Eliminated reward bonus for complete episode (normally 100 steps) or for hitting
	  an active target, since those are so rare, and is outside the control of the
	  tactical guidance anyway.
	* Added an alert flag in the Vehicle class, which is now set if a poor LC desirability
	  choice is made (in the reward function), and displays on the inference graphics.
	* Changed LC desirability reward section to ony award for the single time step that
	  initiates the LC mvr (it was doing it for two consecutive steps), so that bonus
	  impact will be lower.
	* Halved the speed limit penalty multiplier from 0.02 to 0.01 to emphasize the value
	  of followiing lane desirability and speed smoothness.
	* Checkpoints stored in 20240112-1811.




TODO:
- Reintroduce reward changes from runs C4, C5
- consider adding dropouts to BridgitNN
- need prev time step distance to fwd vehicle to help prevent rear-endings
- consider adding logic to abort a LC mvr.

- update roadway diagram in readme
- add coreographed scenarios to routine training that force certain close call situations.
- consider using imitation learning. Create a bot (1c) that changes lanes to pass a slow forward vehicle. Then
  either:
	a) Pre-train the agent with supervised learning. Requires episode capture (like embed_collect) to capture a db
	   with full obs vector, actions and rewards. Then run training (like embedding) on this db comparing either full
	   episodes, short trajectory segments or single time steps.
	b) Intersperse imitation episodes with normal episodes in SAC RL training. ~10% of episodes would use the 1c bot
	   to produce the expert actions for a given time step, then adjust the full ego state to that result and repeat.
- consider rescaling obs element so that they all use the full range of [-1, 1] (e.g. speed limit).
- make Roadway an abstract interface, and allow multiple realizations of it to be chosen during training; add a 5-lane highwaay.
- Convert Train checkpoints to msgpack format (see RLlib_notes.txt)
- Convert Stopper class to a Checkpointing class to manage the number of checkpoints, and also the reporting of results based on running averages.

- Review TODOs in graphics.py
- Display lane change as a gradual motion
- Investigate speed overshoot when speed limit changes (with bot1a controller)
- Investigate target markers initially having white border, then losing it after a car passes through.
- Add lane IDs to graphics
- Review all TODOs
- Add ego sensor display to graphics showing pvmt type/speed colors, vehicle speeds
- Add graphic replay w/step fwd/back
- Greenfish icon editor Pro used for making the graphic icons.
